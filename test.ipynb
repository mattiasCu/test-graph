{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdrug import core\n",
    "from torchdrug import datasets, transforms,layers\n",
    "from torchdrug.core import Registry as R\n",
    "from torchdrug.layers import geometry\n",
    "\n",
    "import torch\n",
    "import torchdrug\n",
    "from torchdrug import data\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.use('Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:10:23   Extracting /home/xiaotong/scratch/protein-datasets/EnzymeCommission.zip to /home/xiaotong/scratch/protein-datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/xiaotong/scratch/protein-datasets/EnzymeCommission/enzyme_commission.pkl.gz: 100%|██████████| 18716/18716 [00:47<00:00, 391.38it/s]\n"
     ]
    }
   ],
   "source": [
    "EnzymeCommission = R.search(\"datasets.EnzymeCommission\")\n",
    "PV = R.search(\"transforms.ProteinView\")\n",
    "trans = PV(view = \"residue\")\n",
    "dataset = EnzymeCommission(\"~/scratch/protein-datasets/\", test_cutoff=0.95, \n",
    "                           atom_feature=\"full\", bond_feature=\"full\", verbose=1, transform = trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "展示第一个样本的前两个残基的原子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein(num_atom=1596, num_bond=2920, num_residue=349)\n"
     ]
    }
   ],
   "source": [
    "# 数据集第一个样本，前两个残基的原子\n",
    "protein = dataset[0][\"graph\"]\n",
    "print(protein)\n",
    "is_first_two = (protein.residue_number == 1) | (protein.residue_number == 2) | (protein.residue_number == 3)\n",
    "first_two = protein.residue_mask(is_first_two, compact=True)\n",
    "\n",
    "first_two.visualize()\n",
    "#plt.savefig(\"fig/first_two_three.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedProtein(batch_size=2, num_atoms=[1596, 3761], num_bonds=[2920, 6468], num_residues=[349, 997])\n"
     ]
    }
   ],
   "source": [
    "first_two_elements = dataset[:2]\n",
    "graphs = [element[\"graph\"] for element in first_two_elements]\n",
    "protein2 = data.Protein.pack(graphs)\n",
    "print(protein2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试edge_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein(num_atom=1596, num_bond=2920, num_residue=349)\n",
      "tensor([[   1,    0,    0],\n",
      "        [   0,    1,    0],\n",
      "        [   2,    1,    0],\n",
      "        ...,\n",
      "        [1429, 1430,    0],\n",
      "        [1431, 1421,    0],\n",
      "        [1421, 1431,    0]])\n",
      "node_in:  tensor([   1,    0,    2,  ..., 1429, 1431, 1421])\n",
      "node_out:  tensor([   0,    1,    1,  ..., 1430, 1421, 1431])\n",
      "\n",
      "\n",
      "atom2residue: tensor([  0,   0,   0,  ..., 346, 347, 348])\n",
      "atom2residue.shape: torch.Size([1596])\n",
      "\n",
      "\n",
      "residue_in:  tensor([  0,   0,   0,  ..., 184, 184, 184])\n",
      "residue_out:  tensor([  0,   0,   0,  ..., 184, 184, 184])\n",
      "residue_in.shape:  torch.Size([2920])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph = dataset[0][\"graph\"]\n",
    "print(graph)\n",
    "\n",
    "edge_list = graph.edge_list\n",
    "print(edge_list)\n",
    "\n",
    "num_relations = 7\n",
    "\n",
    "node_in, node_out, _ = edge_list.t()\n",
    "print(\"node_in: \", node_in)\n",
    "print(\"node_out: \", node_out)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"atom2residue:\", graph.atom2residue)\n",
    "print(\"atom2residue.shape:\", graph.atom2residue.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "residue_in, residue_out = graph.atom2residue[node_in], graph.atom2residue[node_out]\n",
    "print(\"residue_in: \", residue_in)\n",
    "print(\"residue_out: \", residue_out)\n",
    "print(\"residue_in.shape: \", residue_in.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只保留alpha碳，以及按照gearnet格式简化图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph before:  PackedProtein(batch_size=1, num_atoms=[1596], num_bonds=[2920], num_residues=[349])\n",
      "Graph after:  PackedProtein(batch_size=1, num_atoms=[185], num_bonds=[3754], num_residues=[185])\n",
      "node_feature:  torch.Size([185, 21])\n",
      "edge_feature:  torch.Size([3754, 59])\n",
      "edge_weight:  torch.Size([3754])\n",
      "node_position:  torch.Size([185, 3])\n",
      "new_edge_weight:  torch.Size([3754, 1]) tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        ...,\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()], \n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(k=10, min_distance=5),\n",
    "                                                                 geometry.SequentialEdge(max_distance=2)\n",
    "                                                                 ],\n",
    "                                                    edge_feature=\"gearnet\"\n",
    "                                                    )\n",
    "\n",
    "_protein = data.Protein.pack([protein])\n",
    "protein_ = graph_construction_model(_protein)\n",
    "print(\"Graph before: \", _protein)\n",
    "print(\"Graph after: \", protein_)\n",
    "\n",
    "print(\"node_feature: \", protein_.node_feature.shape)\n",
    "print(\"edge_feature: \", protein_.edge_feature.shape)\n",
    "\n",
    "print(\"edge_weight: \", protein_.edge_weight.shape)\n",
    "\n",
    "print(\"node_position: \", protein_.node_position.shape)\n",
    "\n",
    "# 测试unsqueeze\n",
    "edge_weight = protein_.edge_weight.unsqueeze(-1)\n",
    "print(\"new_edge_weight: \", edge_weight.shape, edge_weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gearnet 流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成稀疏的邻接矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([90, 85, 83,  ..., 75, 50, 51]) tensor([575, 547, 547,  ..., 749, 756, 756])\n",
      "torch.Size([3754]) torch.Size([3754])\n",
      "tensor(indices=tensor([[ 90,  85,  83,  ...,  75,  50,  51],\n",
      "                       [575, 547, 547,  ..., 749, 756, 756]]),\n",
      "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
      "       size=(185, 1295), nnz=3754, layout=torch.sparse_coo)\n",
      "torch.Size([185, 1295])\n"
     ]
    }
   ],
   "source": [
    "node_in, node_out, relation = protein_.edge_list.t()\n",
    "node_out = node_out * protein_.num_relation + relation\n",
    "print(node_in, node_out)\n",
    "print(node_in.shape, node_out.shape)\n",
    "\n",
    "import torchdrug.utils as utils\n",
    "adjacency = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), protein_.edge_weight,\n",
    "                                    (protein_.num_node, protein_.num_node * protein_.num_relation))\n",
    "\n",
    "print(adjacency)\n",
    "print(adjacency.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGM模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedProtein(batch_size=2, num_atoms=[1596, 3761], num_bonds=[2920, 6468], num_residues=[349, 997])\n",
      "Graph before:  PackedProtein(batch_size=2, num_atoms=[1596, 3761], num_bonds=[2920, 6468], num_residues=[349, 997])\n",
      "Graph after:  PackedProtein(batch_size=2, num_atoms=[185, 415], num_bonds=[3754, 8999], num_residues=[185, 415])\n",
      "\n",
      "\n",
      "graph_node_feature : torch.Size([600, 21])\n",
      "graph_edge_feature : torch.Size([12753, 59])\n",
      "\n",
      "\n",
      "graph_node _feature_type : torch.int64\n",
      "\n",
      "\n",
      "graph.edge_list:  tensor([[ 95,  96,   5],\n",
      "        [109, 110,   5],\n",
      "        [108, 109,   5],\n",
      "        ...,\n",
      "        [438, 470,   0],\n",
      "        [489, 470,   0],\n",
      "        [493, 470,   0]])\n",
      "graph.edge_list.shape:  torch.Size([12753, 3])\n",
      "\n",
      "\n",
      "graph.edge_weight:  tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "graph.edge_weight.shape:  torch.Size([12753])\n"
     ]
    }
   ],
   "source": [
    "graphs = dataset[:2]\n",
    "graphs = [element[\"graph\"] for element in graphs]\n",
    "graphs = data.Protein.pack(graphs)\n",
    "print(graphs)\n",
    "protein2_ = graph_construction_model(graphs)\n",
    "graph = protein2_\n",
    "#graph = protein_\n",
    "print(\"Graph before: \", protein2)\n",
    "print(\"Graph after: \", graph)\n",
    "print(\"\\n\")\n",
    "print(\"graph_node_feature :\",graph.node_feature.shape)\n",
    "print(\"graph_edge_feature :\",graph.edge_feature.shape)\n",
    "print(\"\\n\")\n",
    "print(\"graph_node _feature_type :\",graph.node_feature.dtype)\n",
    "\n",
    "len(graph.node_feature)\n",
    "#print(graph.node_feature.max())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"graph.edge_list: \", graph.edge_list)\n",
    "print(\"graph.edge_list.shape: \", graph.edge_list.shape)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"graph.edge_weight: \", graph.edge_weight)\n",
    "print(\"graph.edge_weight.shape: \", graph.edge_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader = data.DataLoader(graph, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关系神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_mean, scatter_add, scatter_max\n",
    "import torch.nn as nn\n",
    "from torchdrug import utils\n",
    "\n",
    "class relationalGraph(layers. MessagePassingBase):\n",
    "    eps = 1e-10\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(relationalGraph, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_relation = num_relation\n",
    "        self.edge_input_dim = edge_input_dim\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "        #self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        if edge_input_dim:\n",
    "            self.edge_linear = nn.Linear(edge_input_dim, input_dim)\n",
    "        else:\n",
    "            self.edge_linear = None\n",
    "\n",
    "    def message_and_aggregate(self, graph, input):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "\n",
    "        node_in, node_out, relation = graph.edge_list.t()\n",
    "        node_out = node_out * self.num_relation + relation\n",
    "        degree_out = scatter_add(graph.edge_weight, node_out, dim_size=graph.num_node * graph.num_relation)\n",
    "        edge_weight = graph.edge_weight / degree_out[node_out]\n",
    "        adjacency = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), edge_weight,\n",
    "                                            (graph.num_node, graph.num_node * graph.num_relation))\n",
    "        update = torch.sparse.mm(adjacency.t(), input)\n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float()\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = edge_weight.unsqueeze(-1)\n",
    "            edge_update = scatter_add(edge_input * edge_weight, node_out, dim=0,\n",
    "                                      dim_size=graph.num_node * graph.num_relation)\n",
    "            update += edge_update\n",
    "\n",
    "        return update\n",
    "\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        output = self.linear(update)\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  torch.Size([4200, 512])\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 512\n",
    "num_relations = graph.num_relation\n",
    "\n",
    "output = relationalGraph(input_dim, output_dim, num_relations)(graph, graph.node_feature.float())\n",
    "print(\"output: \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation_output:  torch.Size([600, 512])\n"
     ]
    }
   ],
   "source": [
    "def split_matrix(matrix, num_relation):\n",
    "    # 计算每份的行数\n",
    "    rows_per_split = matrix.shape[0] // num_relation\n",
    "\n",
    "    # 分割矩阵\n",
    "    split_matrices = []\n",
    "    for i in range(num_relation):\n",
    "        start_idx = i * rows_per_split\n",
    "        end_idx = start_idx + rows_per_split\n",
    "        split_matrices.append(matrix[start_idx:end_idx])\n",
    "\n",
    "    return split_matrices\n",
    "\n",
    "relation_output = split_matrix(output, graph.num_relation)\n",
    "\n",
    "print(\"relation_output: \", relation_output[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdrug.layers import functional\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, temperature=0.5, dropout=0.1):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_features = out_features\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.query = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.key = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.value = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([out_features]))\n",
    "        \n",
    "\n",
    "    def forward(self, node_features, graph):\n",
    "        num_nodes = node_features.size(0)\n",
    "\n",
    "        # 计算查询、键和值，并为多头自注意力进行变形\n",
    "        Q = self.query(node_features).view(num_nodes, self.num_heads, self.out_features)  # [num_nodes, num_heads, out_features]\n",
    "        K = self.key(node_features).view(num_nodes, self.num_heads, self.out_features)    # [num_nodes, num_heads, out_features]\n",
    "        #V = self.value(node_features).view(num_nodes, self.num_heads, self.out_features)  # [num_nodes, num_heads, out_features]\n",
    "\n",
    "        # 计算相似性分数\n",
    "        scores = torch.einsum(\"nhd,mhd->nhm\", Q, K) / self.scale  # [num_heads, num_nodes, num_nodes]\n",
    "        \n",
    "        scores = scores / self.temperature\n",
    "\n",
    "        # 计算注意力权重\n",
    "        attention_weights = F.softmax(scores, dim=-1)  # [num_heads, num_nodes, num_nodes]\n",
    "        \n",
    "        # 应用 dropout\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # 多头结果合并\n",
    "        attention_weights = attention_weights.mean(dim=-2)  # [num_nodes, num_nodes]\n",
    "\n",
    "        return attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  tensor([[0.0018, 0.0016, 0.0019,  ..., 0.0019, 0.0016, 0.0016],\n",
      "        [0.0014, 0.0018, 0.0019,  ..., 0.0019, 0.0011, 0.0016],\n",
      "        [0.0016, 0.0016, 0.0012,  ..., 0.0016, 0.0018, 0.0019],\n",
      "        ...,\n",
      "        [0.0016, 0.0016, 0.0019,  ..., 0.0018, 0.0018, 0.0019],\n",
      "        [0.0014, 0.0018, 0.0018,  ..., 0.0019, 0.0018, 0.0016],\n",
      "        [0.0016, 0.0016, 0.0016,  ..., 0.0018, 0.0012, 0.0019]],\n",
      "       grad_fn=<MeanBackward1>) tensor([[0.0016, 0.0014, 0.0016,  ..., 0.0018, 0.0018, 0.0019],\n",
      "        [0.0016, 0.0019, 0.0016,  ..., 0.0018, 0.0019, 0.0016],\n",
      "        [0.0016, 0.0016, 0.0016,  ..., 0.0016, 0.0014, 0.0014],\n",
      "        ...,\n",
      "        [0.0019, 0.0016, 0.0019,  ..., 0.0016, 0.0019, 0.0016],\n",
      "        [0.0019, 0.0016, 0.0016,  ..., 0.0016, 0.0016, 0.0014],\n",
      "        [0.0016, 0.0016, 0.0016,  ..., 0.0018, 0.0019, 0.0016]],\n",
      "       grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchdrug import layers, utils\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim, \n",
    "                 num_heads, attention_out_features, temperature=0.5, dropout=0.1):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        \n",
    "        self.relational_graph = relationalGraph(input_dim, output_dim, num_relation, edge_input_dim)\n",
    "        self.attention = MultiHeadSelfAttention(output_dim, attention_out_features, num_heads, temperature, dropout)\n",
    "    \n",
    "    def split_matrix(self,matrix, num_relation):\n",
    "        # 计算每份的行数\n",
    "        rows_per_split = matrix.shape[0] // num_relation\n",
    "\n",
    "        # 分割矩阵\n",
    "        split_matrices = []\n",
    "        for i in range(num_relation):\n",
    "            start_idx = i * rows_per_split\n",
    "            end_idx = start_idx + rows_per_split\n",
    "            split_matrices.append(matrix[start_idx:end_idx])\n",
    "\n",
    "        return split_matrices\n",
    "    \n",
    "    \n",
    "    def merge_relation_matrices(self,matrix_list):\n",
    "        merged_list = []\n",
    "\n",
    "        # 遍历每个矩阵\n",
    "        for idx, matrix in enumerate(matrix_list):\n",
    "            num_rows = matrix.size(0)\n",
    "            # 创建一个形状为 [num_rows, 1] 的张量，记录矩阵索引\n",
    "            relation_idx = torch.full((num_rows, 1), idx, dtype=torch.long)\n",
    "            # 将矩阵和索引列拼接在一起\n",
    "            extended_matrix = torch.cat((matrix, relation_idx), dim=1)\n",
    "            merged_list.append(extended_matrix)\n",
    "        \n",
    "        # 将所有扩展后的矩阵合并成一个矩阵\n",
    "        merged_matrix = torch.cat(merged_list, dim=0)\n",
    "        \n",
    "        return merged_matrix\n",
    "\n",
    "    def process_list(self,tensor_list):\n",
    "        result_list = []\n",
    "        \n",
    "        for matrix in tensor_list:\n",
    "            num_nodes = matrix.size(0)\n",
    "            indices_list = []\n",
    "\n",
    "            for i in range(num_nodes):\n",
    "                row = matrix[i]\n",
    "                # 找到前5个最大的数及其索引\n",
    "                sorted_indices = torch.argsort(row, descending=True)  # 从大到小排序\n",
    "                max_values = row[sorted_indices][:5]\n",
    "                \n",
    "                # 检查是否有超过5个相同的最大数\n",
    "                count = (row == max_values[-1]).sum().item()\n",
    "                \n",
    "                if count > 5:\n",
    "                    # 选择与当前行序号距离最近的列索引\n",
    "                    relevant_indices = sorted_indices[:count]\n",
    "                    distances = torch.abs(relevant_indices - i)\n",
    "                    closest_indices = relevant_indices[torch.argsort(distances)][:5]\n",
    "                    selected_indices = closest_indices\n",
    "                else:\n",
    "                    selected_indices = sorted_indices[:5]\n",
    "                \n",
    "                # 记录当前行号和所选列号\n",
    "                for j in selected_indices:\n",
    "                    indices_list.append([i, j.item()])\n",
    "            \n",
    "            result_matrix = torch.tensor(indices_list, dtype=torch.long)\n",
    "            result_list.append(result_matrix)\n",
    "            \n",
    "        \n",
    "        return merge_relation_matrices(result_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, node_features, graph):\n",
    "        # Apply relational graph layer\n",
    "        relational_output = self.relational_graph(graph, node_features)\n",
    "        \n",
    "        relational_output = split_matrix(relational_output, graph.num_relation)\n",
    "        #print(\"relational_output: \", len(relational_output))\n",
    "        \n",
    "        attention_output = []\n",
    "        for i in range(len(relational_output)):\n",
    "            # Apply multi-head self attention\n",
    "            output = self.attention(relational_output[i], graph)\n",
    "            attention_output.append(output)\n",
    "        \n",
    "        \n",
    "        return attention_output\n",
    "    \n",
    "\n",
    "# Example of how to initialize and use the model\n",
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 512\n",
    "num_relation = graph.num_relation\n",
    "edge_input_dim = graph.edge_feature.shape[-1]\n",
    "num_heads = 8\n",
    "attention_out_features = 512\n",
    "\n",
    "model = CombinedModel(input_dim, output_dim, num_relation, edge_input_dim, \n",
    "                      num_heads, attention_out_features)\n",
    "\n",
    "# Example data\n",
    "\n",
    "\n",
    "output = model(graph.node_feature.float(), graph)\n",
    "print(\"output: \", output[0],output[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  tensor([[  0, 585,   0],\n",
      "        [  0,  32,   0],\n",
      "        [  0, 333,   0],\n",
      "        ...,\n",
      "        [599, 450,   6],\n",
      "        [599, 296,   6],\n",
      "        [599, 240,   6]])\n"
     ]
    }
   ],
   "source": [
    "def merge_relation_matrices(matrix_list):\n",
    "    merged_list = []\n",
    "\n",
    "    # 遍历每个矩阵\n",
    "    for idx, matrix in enumerate(matrix_list):\n",
    "        num_rows = matrix.size(0)\n",
    "        # 创建一个形状为 [num_rows, 1] 的张量，记录矩阵索引\n",
    "        relation_idx = torch.full((num_rows, 1), idx, dtype=torch.long)\n",
    "        # 将矩阵和索引列拼接在一起\n",
    "        extended_matrix = torch.cat((matrix, relation_idx), dim=1)\n",
    "        merged_list.append(extended_matrix)\n",
    "    \n",
    "    # 将所有扩展后的矩阵合并成一个矩阵\n",
    "    merged_matrix = torch.cat(merged_list, dim=0)\n",
    "    \n",
    "    return merged_matrix\n",
    "\n",
    "def process_list(tensor_list):\n",
    "    result_list = []\n",
    "    \n",
    "    for matrix in tensor_list:\n",
    "        num_nodes = matrix.size(0)\n",
    "        indices_list = []\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "            row = matrix[i]\n",
    "            # 找到前5个最大的数及其索引\n",
    "            sorted_indices = torch.argsort(row, descending=True)  # 从大到小排序\n",
    "            max_values = row[sorted_indices][:5]\n",
    "            \n",
    "            # 检查是否有超过5个相同的最大数\n",
    "            count = (row == max_values[-1]).sum().item()\n",
    "            \n",
    "            if count > 5:\n",
    "                # 选择与当前行序号距离最近的列索引\n",
    "                relevant_indices = sorted_indices[:count]\n",
    "                distances = torch.abs(relevant_indices - i)\n",
    "                closest_indices = relevant_indices[torch.argsort(distances)][:5]\n",
    "                selected_indices = closest_indices\n",
    "            else:\n",
    "                selected_indices = sorted_indices[:5]\n",
    "            \n",
    "            # 记录当前行号和所选列号\n",
    "            for j in selected_indices:\n",
    "                indices_list.append([i, j.item()])\n",
    "        \n",
    "        result_matrix = torch.tensor(indices_list, dtype=torch.long)\n",
    "        result_list.append(result_matrix)\n",
    "        \n",
    "    \n",
    "    return merge_relation_matrices(result_list)\n",
    "\n",
    "output = process_list(output)\n",
    "print(\"output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedProtein(batch_size=2, num_atoms=[1596, 3761], num_bonds=[2920, 6468], num_residues=[349, 997])\n"
     ]
    }
   ],
   "source": [
    "graphs = dataset[:2]\n",
    "graphs = [element[\"graph\"] for element in graphs]\n",
    "graphs = data.Protein.pack(graphs)\n",
    "print(graphs)\n",
    "protein2_ = graph_construction_model(graphs)\n",
    "\n",
    "\n",
    "@R.register(\"models.DGMGearNet\")\n",
    "class DGMGeometryAwareRelationalGraphNeuralNetwork(nn.Module, core.Configurable):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims, num_relation, edge_input_dim=None, num_angle_bin=None,\n",
    "                 short_cut=False, batch_norm=False, activation=\"relu\", concat_hidden=False, readout=\"sum\"):\n",
    "        super(DGMGeometryAwareRelationalGraphNeuralNetwork, self).__init__()\n",
    "\n",
    "        if not isinstance(hidden_dims, Sequence):\n",
    "            hidden_dims = [hidden_dims]\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = sum(hidden_dims) if concat_hidden else hidden_dims[-1]\n",
    "        self.dims = [input_dim] + list(hidden_dims)\n",
    "        self.edge_dims = [edge_input_dim] + self.dims[:-1]\n",
    "        self.num_relation = num_relation\n",
    "        self.num_angle_bin = num_angle_bin\n",
    "        self.short_cut = short_cut\n",
    "        self.concat_hidden = concat_hidden\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(self.dims) - 1):\n",
    "            \n",
    "            self.layers.append(layers.GeometricRelationalGraphConv(self.dims[i], self.dims[i + 1], num_relation,\n",
    "                                                                   None, batch_norm, activation))\n",
    "        if num_angle_bin:\n",
    "            self.spatial_line_graph = layers.SpatialLineGraph(num_angle_bin)\n",
    "            self.edge_layers = nn.ModuleList()\n",
    "            for i in range(len(self.edge_dims) - 1):\n",
    "                self.edge_layers.append(layers.GeometricRelationalGraphConv(\n",
    "                    self.edge_dims[i], self.edge_dims[i + 1], num_angle_bin, None, batch_norm, activation))\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norms = nn.ModuleList()\n",
    "            for i in range(len(self.dims) - 1):\n",
    "                self.batch_norms.append(nn.BatchNorm1d(self.dims[i + 1]))\n",
    "\n",
    "        if readout == \"sum\":\n",
    "            self.readout = layers.SumReadout()\n",
    "        elif readout == \"mean\":\n",
    "            self.readout = layers.MeanReadout()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown readout `%s`\" % readout)\n",
    "\n",
    "    def forward(self, graph, input, all_loss=None, metric=None):\n",
    "        \"\"\"\n",
    "        Compute the node representations and the graph representation(s).\n",
    "\n",
    "        Parameters:\n",
    "            graph (Graph): :math:`n` graph(s)\n",
    "            input (Tensor): input node representations\n",
    "            all_loss (Tensor, optional): if specified, add loss to this tensor\n",
    "            metric (dict, optional): if specified, output metrics to this dict\n",
    "\n",
    "        Returns:\n",
    "            dict with ``node_feature`` and ``graph_feature`` fields:\n",
    "                node representations of shape :math:`(|V|, d)`, graph representations of shape :math:`(n, d)`\n",
    "        \"\"\"\n",
    "        hiddens = []\n",
    "        layer_input = input\n",
    "        if self.num_angle_bin:\n",
    "            line_graph = self.spatial_line_graph(graph)\n",
    "            edge_input = line_graph.node_feature.float()\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden = self.layers[i](graph, layer_input)\n",
    "            if self.short_cut and hidden.shape == layer_input.shape:\n",
    "                hidden = hidden + layer_input\n",
    "            if self.num_angle_bin:\n",
    "                edge_hidden = self.edge_layers[i](line_graph, edge_input)\n",
    "                edge_weight = graph.edge_weight.unsqueeze(-1)\n",
    "                node_out = graph.edge_list[:, 1] * self.num_relation + graph.edge_list[:, 2]\n",
    "                update = scatter_add(edge_hidden * edge_weight, node_out, dim=0,\n",
    "                                     dim_size=graph.num_node * self.num_relation)\n",
    "                update = update.view(graph.num_node, self.num_relation * edge_hidden.shape[1])\n",
    "                update = self.layers[i].linear(update)\n",
    "                update = self.layers[i].activation(update)\n",
    "                hidden = hidden + update\n",
    "                edge_input = edge_hidden\n",
    "            if self.batch_norm:\n",
    "                hidden = self.batch_norms[i](hidden)\n",
    "            hiddens.append(hidden)\n",
    "            layer_input = hidden\n",
    "\n",
    "        if self.concat_hidden:\n",
    "            node_feature = torch.cat(hiddens, dim=-1)\n",
    "        else:\n",
    "            node_feature = hiddens[-1]\n",
    "        graph_feature = self.readout(graph, node_feature)\n",
    "\n",
    "        return {\n",
    "            \"graph_feature\": graph_feature,\n",
    "            \"node_feature\": node_feature\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gearnet测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdrug import models\n",
    "\n",
    "dataloader = data.DataLoader(graph, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiddden_node_feature: torch.Size([600, 512])\n"
     ]
    }
   ],
   "source": [
    "GN = models.GearNet(input_dim=21,\n",
    "                    hidden_dims=512,\n",
    "                    batch_norm=True,\n",
    "                    concat_hidden=True,\n",
    "                    short_cut=True,\n",
    "                    readout=\"sum\",\n",
    "                    num_relation=7,\n",
    "                    edge_input_dim=59\n",
    "                    )\n",
    "\n",
    "# 获取一个批次的数据并传递给模型\n",
    "for batch in dataloader:\n",
    "    #graph = batch['graph']\n",
    "    gearnet_output= GN(graph, graph.node_feature.to(torch.float32))\n",
    "    node_feature1 = gearnet_output[\"node_feature\"]\n",
    "    print(\"hiddden_node_feature:\", node_feature1.shape)\n",
    "\n",
    "    break  # 这里只查看一个批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_list:  tensor([[ 95,  96,   5],\n",
      "        [109, 110,   5],\n",
      "        [108, 109,   5],\n",
      "        ...,\n",
      "        [438, 470,   0],\n",
      "        [489, 470,   0],\n",
      "        [493, 470,   0]])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "tensor(5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m edge_matrices\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_list: \u001b[39m\u001b[38;5;124m\"\u001b[39m, graph\u001b[38;5;241m.\u001b[39medge_list)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mextract_edge_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_relation\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[110], line 10\u001b[0m, in \u001b[0;36mextract_edge_matrices\u001b[0;34m(edge_list, num_relations)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m edge \u001b[38;5;129;01min\u001b[39;00m edge_list:\n\u001b[1;32m      9\u001b[0m     in_node, out_node, relation \u001b[38;5;241m=\u001b[39m edge\n\u001b[0;32m---> 10\u001b[0m     \u001b[43medge_matrices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrelation\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mappend((in_node, out_node))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 将列表转换为numpy数组\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m relation \u001b[38;5;129;01min\u001b[39;00m edge_matrices:\n",
      "\u001b[0;31mKeyError\u001b[0m: tensor(5)"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gearnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
