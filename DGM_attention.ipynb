{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdrug import  layers, datasets,transforms,core\n",
    "from torchdrug.core import Registry as R\n",
    "from torchdrug.layers import geometry\n",
    "\n",
    "import torch\n",
    "from torchdrug import data\n",
    "\n",
    "from torch_scatter import scatter_add\n",
    "import torch.nn as nn\n",
    "from torchdrug import utils\n",
    "from torch.utils import checkpoint\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:12:18   Extracting /home/xiaotong/scratch/protein-datasets/EnzymeCommission.zip to /home/xiaotong/scratch/protein-datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/xiaotong/scratch/protein-datasets/EnzymeCommission/enzyme_commission.pkl.gz: 100%|██████████| 18716/18716 [00:45<00:00, 412.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "PackedProtein(batch_size=2, num_atoms=[185, 415], num_bonds=[3754, 8999], num_residues=[185, 415])\n"
     ]
    }
   ],
   "source": [
    "EnzymeCommission = R.search(\"datasets.EnzymeCommission\")\n",
    "PV = R.search(\"transforms.ProteinView\")\n",
    "trans = PV(view = \"residue\")\n",
    "dataset = EnzymeCommission(\"~/scratch/protein-datasets/\", test_cutoff=0.95, \n",
    "                           atom_feature=\"full\", bond_feature=\"full\", verbose=1, transform = trans)\n",
    "\n",
    "# 只保留alpha碳的简化格式\n",
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()], \n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(k=10, min_distance=5),\n",
    "                                                                 geometry.SequentialEdge(max_distance=2)\n",
    "                                                                 ],\n",
    "                                                    edge_feature=\"gearnet\"\n",
    "                                                    )\n",
    "\n",
    "\n",
    "graphs = dataset[:2]\n",
    "graphs = [element[\"graph\"] for element in graphs]\n",
    "graphs = data.Protein.pack(graphs)\n",
    "print(\"\\n\\n\")\n",
    "graph = graph_construction_model(graphs)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([185, 415])\n",
      "2\n",
      "tensor([[ 95,  96,   5],\n",
      "        [109, 110,   5],\n",
      "        [108, 109,   5],\n",
      "        ...,\n",
      "        [438, 470,   0],\n",
      "        [489, 470,   0],\n",
      "        [493, 470,   0]])\n"
     ]
    }
   ],
   "source": [
    "#graph = graph.to(device)\n",
    "\n",
    "print(graph.num_nodes)\n",
    "print(graph.batch_size)\n",
    "print(graph.edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关系卷积神经网络，获取多个不同的嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationalGraph(layers.MessagePassingBase):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(relationalGraph, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_relation = num_relation\n",
    "        self.edge_input_dim = edge_input_dim\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        if edge_input_dim:\n",
    "            self.edge_linear = nn.Linear(edge_input_dim, input_dim)\n",
    "        else:\n",
    "            self.edge_linear = None\n",
    "            \n",
    "    def trans(self, A, graph):\n",
    "    \n",
    "        Degree_inv_sqrt = torch.diag(torch.pow(torch.sum(A, dim=1), -0.5))\n",
    "        A_norm = torch.mm(torch.mm(Degree_inv_sqrt, A), Degree_inv_sqrt)\n",
    "        \n",
    "        n_rel = graph.num_relation\n",
    "        n = A_norm.size(0)\n",
    "        n_rel = n_rel.item()  # 将 n_rel 从 Tensor 转换为 int\n",
    "        assert n % n_rel == 0, \"n must be divisible by n_rel\"\n",
    "        \n",
    "        block_size = n // n_rel\n",
    "        \n",
    "        # 初始化一个张量来存储累加结果\n",
    "        accumulated = torch.zeros_like(A_norm[:block_size])\n",
    "        \n",
    "        # 将后面的所有块累加到第一块\n",
    "        for i in range(n_rel):\n",
    "            accumulated += A_norm[i * block_size: (i + 1) * block_size]\n",
    "        \n",
    "        # 用累加后的第一块替换原始矩阵的第一块\n",
    "        A_trans = accumulated\n",
    "    \n",
    "        return A_trans\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "        device = input.device  # Ensure device consistency\n",
    "        \n",
    "        if new_edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "            node_out = node_out * self.num_relation + relation\n",
    "        \n",
    "            edge_weight = torch.ones_like(node_out)\n",
    "            degree_out = scatter_add(edge_weight, node_out, dim_size=graph.num_node * graph.num_relation)\n",
    "            degree_out = degree_out\n",
    "            edge_weight = edge_weight / degree_out[node_out]\n",
    "            adjacency = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), edge_weight,\n",
    "                                                (graph.num_node, graph.num_node * graph.num_relation))\n",
    "            update = torch.sparse.mm(adjacency.t(), input)\n",
    "        \n",
    "        else:\n",
    "            adjacency = self.trans(new_edge_list, graph).to(device)\n",
    "            update = torch.mm(adjacency.t().to(device), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(edge_input * edge_weight, node_out, dim=0,\n",
    "                                      dim_size=graph.num_node * graph.num_relation)\n",
    "            update += edge_update\n",
    "\n",
    "        return update\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        # 自环特征\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        input = input.repeat(self.num_relation, 1).to(device)\n",
    "        loop_update = self.self_loop(input).to(device)\n",
    "        \n",
    "        output = self.linear(update)+loop_update\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        \n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self._message_and_aggregate, *graph.to_tensors(), input)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph, input, new_edge_list)\n",
    "        output = self.combine(input, update)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.1482, 0.0119,  ..., 0.1540, 0.0000, 0.3477],\n",
      "        [0.1279, 0.0225, 0.0904,  ..., 0.2627, 0.0000, 0.2235],\n",
      "        [0.1890, 0.2838, 0.0000,  ..., 0.4515, 0.0000, 0.1637],\n",
      "        ...,\n",
      "        [0.0632, 0.0000, 0.1168,  ..., 0.2792, 0.0000, 0.0075],\n",
      "        [0.1890, 0.2838, 0.0000,  ..., 0.4515, 0.0000, 0.1637],\n",
      "        [0.0000, 0.2916, 0.0000,  ..., 0.6574, 0.2065, 0.1689]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "output:  torch.Size([4200, 64])\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 64\n",
    "num_relations = graph.num_relation\n",
    "\n",
    "relational_output = relationalGraph(input_dim, output_dim, num_relations)(graph, graph.node_feature.float(), new_edge_list = None)\n",
    "print(relational_output)\n",
    "print(\"output: \", relational_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重连接模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 只从点所在的图进行reconnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 185\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_start_end(current, graph):\n",
    "    \"\"\"\n",
    "    根据一维张量 a 生成新的张量 b。\n",
    "    \n",
    "    :param a: 输入的一维张量\n",
    "    :return: 输出的一维张量 b\n",
    "    \"\"\"\n",
    "    # 初始化 b，第一个元素是 0\n",
    "    segment = graph.num_nodes.repeat(graph.num_relation)\n",
    "    index = torch.zeros(segment.size(0) + 1, dtype=segment.dtype)\n",
    "    \n",
    "    # 计算 b 的每个元素\n",
    "    for i in range(1, len(index)):\n",
    "        index[i] = index[i - 1] + segment[i - 1]\n",
    "    \n",
    "    # 遍历张量以找到索引值的位置\n",
    "    for i in range(len(index) - 1):\n",
    "        if index[i] <= current < index[i + 1]:\n",
    "            return (index[i].item(), index[i + 1].item())\n",
    "        elif index[i] == current:\n",
    "            return (index[i].item(), index[i + 1].item())\n",
    "    \n",
    "    # 如果索引值恰好等于张量的最后一个元素\n",
    "    if current == index[-1]:\n",
    "        return (index[-1].item(), index[-1].item())\n",
    "    \n",
    "    \n",
    "\n",
    "# 示例使用\n",
    "start, end = get_start_end(184, graph)\n",
    "print(start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 785\n"
     ]
    }
   ],
   "source": [
    "def get_start_end2( current, graph):\n",
    "    segment = graph.num_nodes.repeat(graph.num_relation)\n",
    "    index = torch.cumsum(segment, dim=0)\n",
    "    \n",
    "    # Use torch.searchsorted to find the appropriate segment\n",
    "    pos = torch.searchsorted(index, current, right=True)\n",
    "\n",
    "    if pos == 0:\n",
    "        return (0, index[0].item())\n",
    "    elif pos >= len(index):\n",
    "        return (index[-1].item(), index[-1].item())\n",
    "    else:\n",
    "        return (index[pos-1].item(), index[pos].item())\n",
    "    \n",
    "start, end = get_start_end2(600, graph)\n",
    "print(start, end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gumble-softmax采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 1., 1.]])\n",
      "tensor([[4.8048e-01, 3.7910e-04, 2.6873e-04, 4.0213e-03, 1.1619e-01, 5.7953e-04,\n",
      "         3.6010e-01, 6.4904e-04, 1.2300e-02, 2.5035e-02],\n",
      "        [5.5033e-04, 6.5851e-05, 1.9387e-05, 5.5963e-05, 9.5151e-02, 1.5899e-01,\n",
      "         5.4683e-05, 1.9378e-01, 5.4991e-01, 1.4249e-03],\n",
      "        [9.8238e-01, 2.3563e-05, 5.6371e-04, 5.5736e-03, 8.7534e-05, 5.6787e-05,\n",
      "         8.2759e-03, 2.6395e-03, 7.3919e-05, 3.2825e-04],\n",
      "        [6.5204e-05, 4.4989e-04, 9.1505e-03, 3.3549e-02, 8.4767e-03, 9.2767e-05,\n",
      "         4.9709e-03, 1.1033e-03, 9.4183e-01, 3.1603e-04],\n",
      "        [2.2411e-04, 3.5781e-06, 1.3032e-03, 9.5245e-07, 5.9685e-05, 1.1207e-04,\n",
      "         5.7596e-05, 5.7783e-03, 9.1864e-01, 7.3822e-02]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gumbel_softmax_top_k(logits, tau=0.5, k=1, hard=False):\n",
    "    \"\"\"\n",
    "    Gumbel-Softmax采样方法，每一步都是可微分的，并选择最大的k个元素。\n",
    "    \n",
    "    参数:\n",
    "        logits (torch.Tensor): 输入logits张量，维度为 (batch_size, num_classes)。\n",
    "        tau (float): Gumbel-Softmax的温度参数，控制平滑程度。\n",
    "        k (int): 选择最大的k个元素。\n",
    "        hard (bool): 是否返回硬分类结果。\n",
    "    \n",
    "    返回:\n",
    "        torch.Tensor: Gumbel-Softmax采样结果，维度为 (batch_size, num_classes)。\n",
    "    \"\"\"\n",
    "    # 获取Gumbel分布噪声\n",
    "    gumbels = -torch.empty_like(logits).exponential_().log()  # 生成Gumbel(0,1)噪声\n",
    "    gumbels = (logits + gumbels) / tau  # 添加噪声并除以温度参数\n",
    "\n",
    "    # 计算softmax\n",
    "    y_soft = F.softmax(gumbels, dim=-1)  # 维度为 (batch_size, num_classes)\n",
    "\n",
    "    if hard:\n",
    "        # 硬分类结果：选取原始logits最大的k个位置\n",
    "        topk_indices = logits.topk(k, dim=-1)[1]  # 获取前k个元素的索引\n",
    "        y_hard = torch.zeros_like(logits).scatter_(-1, topk_indices, 1.0)  # 生成one-hot向量\n",
    "        # 使用直通估计器\n",
    "        y = (y_hard - y_soft).detach() + y_soft\n",
    "    else:\n",
    "        y = y_soft\n",
    "\n",
    "    return y\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == \"__main__\":\n",
    "    logits = torch.randn(5, 10)  # 维度为 (batch_size=5, num_classes=10)\n",
    "    tau = 0.5\n",
    "    k = 3\n",
    "    hard = True\n",
    "    samples = gumbel_softmax_top_k(logits, tau, k, hard)\n",
    "    print(samples)\n",
    "    samples = gumbel_softmax_top_k(logits, tau, k)\n",
    "    print(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### window self attention + gumble softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Rewirescorelayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, window_size, k, temperature=0.5, dropout=0.1):\n",
    "        super(Rewirescorelayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_features = out_features\n",
    "        self.window_size = window_size\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.k = k\n",
    "        \n",
    "        self.query = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.key = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.value = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([out_features])).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    # get the start and end indices for each window of nodes\n",
    "    def get_start_end(self, current, graph):\n",
    "        segment = graph.num_nodes.repeat(graph.num_relation)\n",
    "        index = torch.cumsum(segment, dim=0)\n",
    "        \n",
    "        # Use torch.searchsorted to find the appropriate segment\n",
    "        pos = torch.searchsorted(index, current, right=True)\n",
    "\n",
    "        if pos == 0:\n",
    "            return (0, index[0].item())\n",
    "        elif pos >= len(index):\n",
    "            return (index[-1].item(), index[-1].item())\n",
    "        else:\n",
    "            return (index[pos-1].item(), index[pos].item())\n",
    "\n",
    "    def gumbel_softmax_top_k(self, logits, tau=1.0, hard=False):\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "        gumbels = (logits + gumbels) / tau\n",
    "\n",
    "        y_soft = F.softmax(gumbels, dim=-1)\n",
    "\n",
    "        if hard:\n",
    "            topk_indices = logits.topk(self.k, dim=-1)[1]\n",
    "            y_hard = torch.zeros_like(logits).scatter_(-1, topk_indices, 1.0)\n",
    "            y = (y_hard - y_soft).detach() + y_soft\n",
    "        else:\n",
    "            y = y_soft\n",
    "\n",
    "        return y\n",
    "\n",
    "    def forward(self, graph, node_features):\n",
    "        device = node_features.device\n",
    "        num_nodes = node_features.size(0)\n",
    "    \n",
    "        \n",
    "         # calculate the start and end indices for each node\n",
    "        half_window = self.window_size // 2\n",
    "        start_end_indices = [self.get_start_end(i, graph) for i in range(num_nodes)]\n",
    "        start_indices = torch.tensor([max(start_end_indices[i][0], i - half_window) for i in range(num_nodes)], device=device)\n",
    "        end_indices = torch.tensor([min(start_end_indices[i][1], i + half_window) for i in range(num_nodes)], device=device)\n",
    "        window_sizes = end_indices - start_indices\n",
    "\n",
    "        Q = self.query(node_features).view(num_nodes, self.num_heads, self.out_features)     # [num_nodes, num_heads, out_features]\n",
    "        K = self.key(node_features).view(num_nodes, self.num_heads, self.out_features)       # [num_nodes, num_heads, out_features]\n",
    "        \n",
    "        output = torch.zeros(num_nodes, num_nodes, device=device)\n",
    "        all_scores = torch.zeros(num_nodes, self.num_heads, self.window_size, device=device)\n",
    "        \n",
    "        # construct the windowed K matrix\n",
    "        K_windows = torch.zeros((num_nodes, self.window_size, self.num_heads, self.out_features), device=device)\n",
    "        for i in range(num_nodes):\n",
    "            start = start_indices[i]\n",
    "            end = end_indices[i]\n",
    "            K_windows[i, :end-start] = K[start:end]\n",
    "\n",
    "\n",
    "        Q_expanded = Q.unsqueeze(2)  # [num_nodes, num_heads, 1, out_features]\n",
    "        K_expanded = K_windows.permute(0, 2, 1, 3)  # [num_nodes, num_heads, max_window_size, out_features]\n",
    "\n",
    "\n",
    "        all_scores = torch.matmul(Q_expanded, K_expanded.transpose(-1, -2)) / self.scale  # [num_nodes, num_heads, 1, max_window_size]\n",
    "        all_scores = all_scores.squeeze(2)  # [num_nodes, num_heads, max_window_size]\n",
    "        \n",
    "\n",
    "        mask = (torch.arange(self.window_size, device=device).expand(num_nodes, self.window_size) < window_sizes.unsqueeze(1))\n",
    "\n",
    "\n",
    "        all_scores = all_scores.masked_fill(~mask.unsqueeze(1), float('-inf'))  # [num_nodes, num_heads, max_window_size]\n",
    "        attention_weights = F.softmax(all_scores / self.temperature, dim=-1)  # [num_nodes, num_heads, max_window_size]\n",
    "        attention_weights = attention_weights.mean(dim=1) \n",
    "        \n",
    "        # sample edges\n",
    "        for i in range(num_nodes):\n",
    "            start = start_indices[i]\n",
    "            end = end_indices[i]\n",
    "            output[i, start:end] = attention_weights[i, :end-start]\n",
    "\n",
    "        edge_list = self.gumbel_softmax_top_k(output, self.temperature, self.k)\n",
    "\n",
    "        return edge_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = relational_output.shape[-1]\n",
    "output_dim = 256\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 3\n",
    "num_nodes = relational_output.size(0)\n",
    "\n",
    "\n",
    "relational_output = relational_output.to(device)\n",
    "module = Rewirescorelayer(input_dim, output_dim, num_heads, window_size, k).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4200, 4200])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "(tensor([   0,    0,    0,  ..., 4199, 4199, 4199], device='cuda:0'), tensor([   2,    3,    4,  ..., 4196, 4198, 4199], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "attn_output = module(graph, relational_output)\n",
    "print(attn_output.shape)\n",
    "print(attn_output)\n",
    "#a = attn_output[1200:, 599:1199]\n",
    "indices = torch.nonzero(attn_output, as_tuple=True)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试不同degree进行采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds:\n",
      " tensor([0.2000, 0.5000, 0.7000, 0.9000, 0.1000, 0.3000, 0.6000, 0.8000, 0.4000,\n",
      "        0.2000])\n",
      "Sampled Matrix:\n",
      " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gumbel_softmax_sample(logits, tau):\n",
    "    # 从Gumbel(0, 1)分布中采样\n",
    "    gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits)))\n",
    "    y = logits + gumbel_noise\n",
    "    return F.softmax(y / tau, dim=-1)\n",
    "\n",
    "def bernoulli_sampling_with_different_thresholds(probs, thresholds, tau=1.0):\n",
    "    # 对数概率\n",
    "    logits = torch.log(probs) - torch.log(1 - probs)\n",
    "    # 进行Gumbel-Softmax采样\n",
    "    y = gumbel_softmax_sample(logits, tau)\n",
    "    # 硬化处理，根据每行的不同阈值\n",
    "    z = (y > thresholds.unsqueeze(1)).float()\n",
    "    return z\n",
    "\n",
    "# 示例矩阵\n",
    "n = 10\n",
    "P = torch.rand(n, n)\n",
    "\n",
    "# 为每一行设置不同的阈值\n",
    "thresholds = torch.tensor([0.2, 0.5, 0.7, 0.9, 0.1, 0.3, 0.6, 0.8, 0.4, 0.2])\n",
    "\n",
    "# 进行可微分伯努利采样并硬化处理\n",
    "tau = 0.1  # 温度参数\n",
    "sampled_matrix = bernoulli_sampling_with_different_thresholds(P, thresholds, tau)\n",
    "\n",
    "#print(\"Probability Matrix:\\n\", P)\n",
    "print(\"Thresholds:\\n\", thresholds)\n",
    "print(\"Sampled Matrix:\\n\", sampled_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算degree矩阵，变换adjacent matrix形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([600, 4200])\n",
      "(tensor([  0,   0,   0,  ..., 599, 599, 599], device='cuda:0'), tensor([   3,    6,    8,  ..., 4193, 4195, 4196], device='cuda:0'))\n",
      "torch.Size([4200, 21])\n"
     ]
    }
   ],
   "source": [
    "def trans(A, graph):\n",
    "    \n",
    "    Degree_inv_sqrt = torch.diag(torch.pow(torch.sum(A, dim=1), -0.5))\n",
    "    A_norm = torch.mm(torch.mm(Degree_inv_sqrt, A), Degree_inv_sqrt)\n",
    "    \n",
    "    n_rel = graph.num_relation\n",
    "    n = A_norm.size(0)\n",
    "    n_rel = n_rel.item()  # 将 n_rel 从 Tensor 转换为 int\n",
    "    assert n % n_rel == 0, \"n must be divisible by n_rel\"\n",
    "    \n",
    "    block_size = n // n_rel\n",
    "    \n",
    "    # 初始化一个张量来存储累加结果\n",
    "    accumulated = torch.zeros_like(A_norm[:block_size])\n",
    "    \n",
    "    # 将后面的所有块累加到第一块\n",
    "    for i in range(n_rel):\n",
    "        accumulated += A_norm[i * block_size: (i + 1) * block_size]\n",
    "    \n",
    "    # 用累加后的第一块替换原始矩阵的第一块\n",
    "    A_trans = accumulated\n",
    "    \n",
    "    \n",
    "    \n",
    "    return A_trans\n",
    "\n",
    "A_norm = trans(attn_output, graph)\n",
    "print(A_norm)\n",
    "print(A_norm.shape)\n",
    "\n",
    "indices = torch.nonzero(A_norm, as_tuple=True)\n",
    "print(indices)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "update = torch.mm(A_norm.t(), graph.node_feature.to(device).to(torch.float))\n",
    "print(update.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewired_gearnet 用于diffusion模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewireGearnet(nn.Module):\n",
    "    gradient_checkpoint = False\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(RewireGearnet, self).__init__()\n",
    "        self.num_relation = num_relation\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.linear = nn.Linear(num_relation * input_dim, output_dim)\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(output_dim) if batch_norm else None\n",
    "        self.activation = getattr(F, activation) if activation else None\n",
    "        self.edge_linear = nn.Linear(edge_input_dim, output_dim) if edge_input_dim else None\n",
    "\n",
    "    def trans(self, A, graph):\n",
    "        n_rel = graph.num_relation\n",
    "        n = A.size(0)\n",
    "        n_rel = n_rel.item()  # 将 n_rel 从 Tensor 转换为 int\n",
    "        assert n % n_rel == 0, \"n must be divisible by n_rel\"\n",
    "        \n",
    "        block_size = n // n_rel\n",
    "        \n",
    "        # 初始化一个张量来存储累加结果\n",
    "        accumulated = torch.zeros_like(A[:block_size])\n",
    "        \n",
    "        # 将后面的所有块累加到第一块\n",
    "        for i in range(n_rel):\n",
    "            accumulated += A[i * block_size: (i + 1) * block_size]\n",
    "        \n",
    "        # 用累加后的第一块替换原始矩阵的第一块\n",
    "        A_trans = accumulated\n",
    "    \n",
    "        return A_trans\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list=None):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "\n",
    "        device = input.device  # Ensure device consistency\n",
    "\n",
    "        if new_edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "            node_out = node_out * self.num_relation + relation\n",
    "            adjacency = torch.sparse_coo_tensor(\n",
    "                torch.stack([node_in, node_out]),\n",
    "                graph.edge_weight.to(device),\n",
    "                (graph.num_node, graph.num_node * graph.num_relation),\n",
    "                device=device\n",
    "            )\n",
    "            update = torch.sparse.mm(adjacency.t(), input)\n",
    "        else:\n",
    "            adjacency = self.trans(new_edge_list, graph).to(device)\n",
    "            update = torch.mm(adjacency.t(), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = graph.edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(\n",
    "                edge_input * edge_weight, node_out, dim=0,\n",
    "                dim_size=graph.num_node * graph.num_relation\n",
    "            )\n",
    "            update += edge_update\n",
    "            \n",
    "        return update.view(graph.num_node, self.num_relation * self.input_dim).to(device)\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm.to(device)\n",
    "        \n",
    "        output = self.linear(update) + self.self_loop(input)\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, graph, input, new_edge_list=None, new_edge_weight=None):\n",
    "        \"\"\"\n",
    "        Perform message passing over the graph(s).\n",
    "\n",
    "        Parameters:\n",
    "            graph (Graph): graph(s)\n",
    "            input (Tensor): node representations of shape :math:`(|V|, ...)`\n",
    "        \"\"\"\n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self.message_and_aggregate, graph, input)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph, input, new_edge_list)\n",
    "        output = self.combine(input, update)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 512])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1172, 0.0000],\n",
      "        [0.0000, 0.2223, 0.0000,  ..., 0.0000, 0.3193, 0.1439],\n",
      "        [0.0903, 0.1699, 0.3812,  ..., 0.0723, 0.2250, 0.0000],\n",
      "        ...,\n",
      "        [0.2997, 0.1000, 0.0000,  ..., 0.4028, 0.8425, 0.0000],\n",
      "        [0.3565, 0.4857, 0.1668,  ..., 0.0000, 0.1029, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.3299, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 512\n",
    "num_relations = graph.num_relation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "new_node_feature = RewireGearnet(input_dim, output_dim, num_relations)(graph, graph.node_feature.to(device).float(), attn_output).to(device)\n",
    "\n",
    "print(new_node_feature.shape)\n",
    "print(new_node_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMGearnet(nn.Module, core.Configurable):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims, num_relation, num_heads, window_size, k, edge_input_dim=None, num_angle_bin=None,\n",
    "                 short_cut=False, batch_norm=False, activation=\"relu\", concat_hidden=False, readout=\"sum\"):\n",
    "        super(DGMGearnet, self).__init__()\n",
    "\n",
    "        #if not isinstance(hidden_dims, Sequence):\n",
    "            #hidden_dims = [hidden_dims]\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = sum(hidden_dims) if concat_hidden else hidden_dims[-1]\n",
    "        self.dims = [input_dim] + list(hidden_dims)\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.k = k\n",
    "        self.edge_dims = [edge_input_dim] + self.dims[:-1]\n",
    "        self.num_relation = num_relation\n",
    "        self.num_angle_bin = num_angle_bin\n",
    "        self.short_cut = short_cut\n",
    "        self.concat_hidden = concat_hidden\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.score_layers = nn.ModuleList()\n",
    "        for i in range(len(self.dims) - 1):\n",
    "            \n",
    "            self.score_layers.append(relationalGraph(self.dims[i], self.dims[i + 1], num_relation, edge_input_dim, \n",
    "                                            batch_norm, activation))\n",
    "            \n",
    "            self.score_layers.append(Rewirescorelayer(self.dims[i + 1], self.dims[i + 1], self.num_heads, self.window_size, \n",
    "                                            self.k, temperature=0.5, dropout=0.1))\n",
    "            \n",
    "            self.layers.append(RewireGearnet(self.dims[i], self.dims[i + 1], num_relation,\n",
    "                                            edge_input_dim=None, batch_norm=False, activation=\"relu\"))\n",
    "        \n",
    "        if num_angle_bin:\n",
    "            self.spatial_line_graph = layers.SpatialLineGraph(num_angle_bin)\n",
    "            self.edge_layers = nn.ModuleList()\n",
    "            for i in range(len(self.edge_dims) - 1):\n",
    "                self.edge_layers.append(layers.GeometricRelationalGraphConv(\n",
    "                    self.edge_dims[i], self.edge_dims[i + 1], num_angle_bin, None, batch_norm, activation))\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norms = nn.ModuleList()\n",
    "            for i in range(len(self.dims) - 1):\n",
    "                self.batch_norms.append(nn.BatchNorm1d(self.dims[i + 1]))\n",
    "\n",
    "        if readout == \"sum\":\n",
    "            self.readout = layers.SumReadout()\n",
    "        elif readout == \"mean\":\n",
    "            self.readout = layers.MeanReadout()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown readout `%s`\" % readout)\n",
    "\n",
    "    def forward(self, graph, input, edge_list=None, all_loss=None, metric=None):\n",
    "        \"\"\"\n",
    "        Compute the node representations and the graph representation(s).\n",
    "\n",
    "        Parameters:\n",
    "            graph (Graph): :math:`n` graph(s)\n",
    "            input (Tensor): input node representations\n",
    "            all_loss (Tensor, optional): if specified, add loss to this tensor\n",
    "            metric (dict, optional): if specified, output metrics to this dict\n",
    "\n",
    "        Returns:\n",
    "            dict with ``node_feature`` and ``graph_feature`` fields:\n",
    "                node representations of shape :math:`(|V|, d)`, graph representations of shape :math:`(n, d)`\n",
    "        \"\"\"\n",
    "        hiddens = []\n",
    "        layer_input = input\n",
    "        if self.num_angle_bin:\n",
    "            line_graph = self.spatial_line_graph(graph)\n",
    "            edge_input = line_graph.node_feature.float()\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            \n",
    "            relational_output = self.score_layers[2*i](graph, layer_input, edge_list)\n",
    "            new_edge_list = self.score_layers[2*i+1](graph, relational_output)\n",
    "            \n",
    "            hidden = self.layers[i](graph, layer_input, new_edge_list)\n",
    "            \n",
    "            if self.short_cut and hidden.shape == layer_input.shape:\n",
    "                hidden = hidden + layer_input\n",
    "                \n",
    "            if self.num_angle_bin:\n",
    "                edge_hidden = self.edge_layers[i](line_graph, edge_input)\n",
    "                edge_weight = graph.edge_weight.unsqueeze(-1)\n",
    "                if new_edge_list is None:\n",
    "                    node_out = graph.edge_list[:, 1] * self.num_relation + graph.edge_list[:, 2]\n",
    "                else:\n",
    "                    node_out = new_edge_list[:, 1] * self.num_relation + new_edge_list[:, 2]\n",
    "                \n",
    "                    update = scatter_add(edge_hidden * edge_weight, node_out, dim=0,\n",
    "                                        dim_size=graph.num_node * self.num_relation)\n",
    "                update = update.view(graph.num_node, self.num_relation * edge_hidden.shape[1])\n",
    "                update = self.layers[i].linear(update)\n",
    "                update = self.layers[i].activation(update)\n",
    "                hidden = hidden + update\n",
    "                edge_input = edge_hidden\n",
    "            if self.batch_norm:\n",
    "                hidden = self.batch_norms[i](hidden)\n",
    "                \n",
    "            hiddens.append(hidden)\n",
    "            layer_input = hidden\n",
    "            edge_list = new_edge_list\n",
    "\n",
    "        if self.concat_hidden:\n",
    "            node_feature = torch.cat(hiddens, dim=-1)\n",
    "        else:\n",
    "            node_feature = hiddens[-1]\n",
    "        graph_feature = self.readout(graph, node_feature)\n",
    "        print(\"node_feature: \", node_feature.shape)\n",
    "\n",
    "        return {\n",
    "            \"graph_feature\": graph_feature,\n",
    "            \"node_feature\": node_feature\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_feature:  torch.Size([600, 128])\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "hidden_dims = [64, 128, 128, 128, 128]\n",
    "num_relations = graph.num_relation\n",
    "num_heads = 8\n",
    "window_size = 50\n",
    "k = 5\n",
    "\n",
    "\n",
    "output = DGMGearnet(input_dim, hidden_dims, num_relations, num_heads, window_size, k).to(device)(graph.to(device), graph.node_feature.to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17.8948,  0.0000, 17.3570,  ...,  0.0000, 26.0307,  0.0000],\n",
      "        [ 2.3947,  7.2990,  0.0000,  ...,  0.0000,  0.0000,  2.3281],\n",
      "        [20.3003, 14.6250,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000, 23.7315,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.5055,  ...,  5.8059,  2.6199,  0.0000],\n",
      "        [13.0536,  0.9474,  8.7509,  ...,  0.0000,  5.5958,  5.5812]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "torch.Size([600, 512])\n",
      "\n",
      "\n",
      "tensor([[2161.1880, 1087.6064, 1541.9747,  ..., 2080.9690, 1970.3794,\n",
      "         1159.7115],\n",
      "        [5667.3657, 2416.1787, 4092.4651,  ..., 4103.5674, 4787.2236,\n",
      "         1832.0172]], device='cuda:0', grad_fn=<ScatterAddBackward>)\n",
      "torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "print(output[\"node_feature\"])\n",
    "print(output[\"node_feature\"].shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(output[\"graph_feature\"])\n",
    "print(output[\"graph_feature\"].shape)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from local_attention import LocalAttention\n",
    "\n",
    "node = 512\n",
    "dimm = 64\n",
    "b = 5\n",
    "\n",
    "q = torch.randn(b, 8, node, dimm)\n",
    "k = torch.randn(b, 8, node, dimm)\n",
    "v = torch.randn(b, 8, node, dimm)\n",
    "\n",
    "attn = LocalAttention(\n",
    "    dim = 64,                # dimension of each head (you need to pass this in for relative positional encoding)\n",
    "    window_size = 16,       # window size. 512 is optimal, but 256 or 128 yields good enough results\n",
    "    causal = True,           # auto-regressive or not\n",
    "    look_backward = 1,       # each window looks at the window before\n",
    "    look_forward = 0,        # for non-auto-regressive case, will default to 1, so each window looks at the window before and after it\n",
    "    dropout = 0.1,           # post-attention dropout\n",
    "    exact_windowsize = False # if this is set to true, in the causal setting, each query will see at maximum the number of keys equal to the window size\n",
    ")\n",
    "\n",
    "mask = torch.ones(b, node).bool()\n",
    "out = attn(q, k, v, mask = mask) # (2, 8, 2048, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 512, 64])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gearnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
