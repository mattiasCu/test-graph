{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdrug import  layers, datasets,transforms,core\n",
    "from torchdrug.core import Registry as R\n",
    "from torchdrug.layers import geometry\n",
    "\n",
    "import torch\n",
    "from torchdrug import data\n",
    "\n",
    "from torch_scatter import scatter_add\n",
    "import torch.nn as nn\n",
    "from torchdrug import utils\n",
    "from torch.utils import checkpoint\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "    device = torch.device('cuda:1')\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    raise ValueError(\"CUDA device 2 is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:22:36   Extracting /home/xiaotong/scratch/protein-datasets/EnzymeCommission.zip to /home/xiaotong/scratch/protein-datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/xiaotong/scratch/protein-datasets/EnzymeCommission/enzyme_commission.pkl.gz: 100%|██████████| 18716/18716 [00:50<00:00, 370.72it/s]\n"
     ]
    }
   ],
   "source": [
    "EnzymeCommission = R.search(\"datasets.EnzymeCommission\")\n",
    "PV = R.search(\"transforms.ProteinView\")\n",
    "trans = PV(view = \"residue\")\n",
    "dataset = EnzymeCommission(\"~/scratch/protein-datasets/\", test_cutoff=0.95, \n",
    "                           atom_feature=\"full\", bond_feature=\"full\", verbose=1, transform = trans)\n",
    "\n",
    "# 只保留alpha碳的简化格式\n",
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()], \n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(k=10, min_distance=5),\n",
    "                                                                 geometry.SequentialEdge(max_distance=2)\n",
    "                                                                 ],\n",
    "                                                    edge_feature=\"gearnet\"\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "PackedProtein(batch_size=2, num_atoms=[185, 415], num_bonds=[3754, 8999], num_residues=[185, 415])\n"
     ]
    }
   ],
   "source": [
    "graphs = dataset[0:2]\n",
    "graphs = [element[\"graph\"] for element in graphs]\n",
    "graphs = data.Protein.pack(graphs)\n",
    "print(\"\\n\\n\")\n",
    "graph = graph_construction_model(graphs)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([185, 415])\n",
      "2\n",
      "tensor([[ 95,  96,   5],\n",
      "        [109, 110,   5],\n",
      "        [108, 109,   5],\n",
      "        ...,\n",
      "        [438, 470,   0],\n",
      "        [489, 470,   0],\n",
      "        [493, 470,   0]])\n"
     ]
    }
   ],
   "source": [
    "print(graph.num_nodes)\n",
    "print(graph.batch_size)\n",
    "print(graph.edge_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关系卷积神经网络，获取多个不同的嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationalGraph(layers.MessagePassingBase):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(relationalGraph, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_relation = num_relation\n",
    "        self.edge_input_dim = edge_input_dim\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        if edge_input_dim:\n",
    "            self.edge_linear = nn.Linear(edge_input_dim, input_dim)\n",
    "        else:\n",
    "            self.edge_linear = None\n",
    "            \n",
    "    def trans(self, A, graph):\n",
    "    \n",
    "        Degree_inv_sqrt = torch.diag(torch.pow(torch.sum(A, dim=1), -0.5))\n",
    "        A_norm = torch.mm(torch.mm(Degree_inv_sqrt, A), Degree_inv_sqrt)\n",
    "        \n",
    "        n_rel = graph.num_relation\n",
    "        n = A_norm.size(0)\n",
    "        n_rel = n_rel.item()  # 将 n_rel 从 Tensor 转换为 int\n",
    "        assert n % n_rel == 0, \"n must be divisible by n_rel\"\n",
    "        \n",
    "        block_size = n // n_rel\n",
    "        \n",
    "        # 初始化一个张量来存储累加结果\n",
    "        accumulated = torch.zeros_like(A_norm[:block_size])\n",
    "        \n",
    "        # 将后面的所有块累加到第一块\n",
    "        for i in range(n_rel):\n",
    "            accumulated += A_norm[i * block_size: (i + 1) * block_size]\n",
    "        \n",
    "        # 用累加后的第一块替换原始矩阵的第一块\n",
    "        A_trans = accumulated\n",
    "    \n",
    "        return A_trans\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "        device = input.device  # Ensure device consistency\n",
    "        \n",
    "        if new_edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "            node_out = node_out * self.num_relation + relation\n",
    "        \n",
    "            degree_out = scatter_add(graph.edge_weight, node_out, dim_size=graph.num_node * graph.num_relation)\n",
    "            edge_weight = graph.edge_weight / degree_out[node_out]\n",
    "            adjacency = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), edge_weight,\n",
    "                                                (graph.num_node, graph.num_node * graph.num_relation))\n",
    "            update = torch.sparse.mm(adjacency.t().to(device), input.to(device))\n",
    "        else:\n",
    "            adjacency = self.trans(new_edge_list, graph).to(device)\n",
    "            update = torch.mm(adjacency.t().to(device), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(edge_input * edge_weight, node_out, dim=0,\n",
    "                                      dim_size=graph.num_node * graph.num_relation)\n",
    "            update += edge_update\n",
    "\n",
    "        return update\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        # 自环特征\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        input = input.repeat(self.num_relation, 1).to(device)\n",
    "        loop_update = self.self_loop(input).to(device)\n",
    "        \n",
    "        output = self.linear(update)+loop_update\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        \n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self._message_and_aggregate, *graph.to_tensors(), input, new_edge_list)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph, input, new_edge_list)\n",
    "        output = self.combine(input, update).view(graph.num_relation, input.size(0), -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  torch.Size([7, 600, 128])\n",
      "output:  <ViewBackward object at 0x7f523f7f9670>\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 128\n",
    "num_relations = graph.num_relation\n",
    "\n",
    "input = graph.node_feature.float()\n",
    "model = relationalGraph(input_dim, output_dim, num_relations)\n",
    "relational_output = model(graph, input, new_edge_list = None)\n",
    "print(\"output: \", relational_output.shape)\n",
    "print(\"output: \", relational_output.grad_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重连接模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### window self attention + gumble softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewirescorelayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, window_size, k, temperature=0.5, dropout=0.1):\n",
    "        super(Rewirescorelayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_features = out_features\n",
    "        self.window_size = window_size\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.k = k\n",
    "        \n",
    "        self.query = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.key = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.scale = 1 / (out_features ** 0.5)\n",
    "\n",
    "    def split_windows(self, tensor, index, window_size, device):\n",
    "        result = []\n",
    "        index_list = []\n",
    "        start = 0\n",
    "\n",
    "        for idx in index:\n",
    "            end = start + idx - 1\n",
    "            while start <= end:\n",
    "                if start + window_size <= end:\n",
    "                    result.append(tensor[:, start:start + window_size, :])\n",
    "                    index_list.append([start, start + window_size])\n",
    "                    start += window_size\n",
    "                else:\n",
    "                    padding_rows = window_size - (end - start + 1)\n",
    "                    restart = start - padding_rows\n",
    "                    result.append(tensor[:, restart:restart + window_size, :])\n",
    "                    index_list.append([restart, restart + window_size])\n",
    "\n",
    "                    start = end+1\n",
    "        \n",
    "        result_tensor = torch.stack(result, dim=1).to(device)\n",
    "        return result_tensor, index_list\n",
    "\n",
    "    def gumbel_softmax_top_k(self, logits, tau=1.0, hard=False):\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "        gumbels = (logits + gumbels) / tau\n",
    "\n",
    "        y_soft = F.softmax(gumbels, dim=-1)\n",
    "\n",
    "        if hard:\n",
    "            topk_indices = logits.topk(self.k, dim=-1)[1]\n",
    "            y_hard = torch.zeros_like(logits).scatter_(-1, topk_indices, 1.0)\n",
    "            y = (y_hard - y_soft).detach() + y_soft\n",
    "        else:\n",
    "            y = y_soft\n",
    "\n",
    "        return y\n",
    "\n",
    "    def windows2adjacent(self, windows, index_list, output, device):\n",
    "        # 确保所有张量在相同设备上\n",
    "        output = output.to(device)\n",
    "        windows = windows.to(device)\n",
    "\n",
    "        # 创建一个新的张量来存储更新后的输出\n",
    "        new_output = torch.zeros_like(output, device=device)\n",
    "\n",
    "        # 填充新输出张量\n",
    "        for i, index in enumerate(index_list):\n",
    "            start, end = index\n",
    "            new_output[:, start:end, start:end] = torch.clamp(new_output[:, start:end, start:end] + windows[:, i, :, :], 0, 1)\n",
    "\n",
    "        # 获取输出张量的形状\n",
    "        num_relations, num_nodes, _ = new_output.shape\n",
    "        \n",
    "        # 创建一个新的结果张量\n",
    "        result = torch.zeros(num_relations * num_nodes, num_relations * num_nodes, device=device)\n",
    "\n",
    "        # 填充结果张量\n",
    "        for i in range(num_relations):\n",
    "            result[i * num_nodes:(i + 1) * num_nodes, i * num_nodes:(i + 1) * num_nodes] = result[i * num_nodes:(i + 1) * num_nodes, i * num_nodes:(i + 1) * num_nodes] + new_output[i]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def forward(self, graph, node_features):\n",
    "        torch.cuda.synchronize()  # 同步\n",
    "        start0 = time.time()\n",
    "        device = node_features.device\n",
    "        num_relations = node_features.size(0)\n",
    "        num_nodes = node_features.size(1)\n",
    "        index = graph.num_nodes.tolist()\n",
    "        end0 = time.time()\n",
    "        \n",
    "        torch.cuda.synchronize()  # 同步\n",
    "        start1 = time.time()\n",
    "        Q = self.query(node_features).view(num_relations, num_nodes, self.num_heads, self.out_features).permute(0, 2, 1, 3)\n",
    "        K = self.key(node_features).view(num_relations, num_nodes, self.num_heads, self.out_features).permute(0, 2, 1, 3)\n",
    "        Q = Q.reshape(num_relations * self.num_heads, num_nodes, self.out_features).to(device)\n",
    "        K = K.reshape(num_relations * self.num_heads, num_nodes, self.out_features).to(device)\n",
    "        end1 = time.time()\n",
    "        \n",
    "        torch.cuda.synchronize()  # 同步\n",
    "        output = torch.zeros(num_relations, num_nodes, num_nodes).to(device)\n",
    "        result = torch.zeros(num_relations * num_nodes, num_relations * num_nodes).to(device)\n",
    "\n",
    "        torch.cuda.synchronize()  # 同步\n",
    "        start2 = time.time()\n",
    "        Q_windows, Q_index = self.split_windows(Q, index, self.window_size, device)\n",
    "        K_windows, _ = self.split_windows(K, index, self.window_size, device)\n",
    "        end2 = time.time()  \n",
    "        \n",
    "        torch.cuda.synchronize()  # 同步\n",
    "        start3 = time.time()\n",
    "        scores = torch.einsum('b h i e, b h j e -> b h i j', Q_windows, K_windows) / self.scale                                 # (num_relations*num_heads, num_windows, window_size, window_size)\n",
    "        attn = scores.softmax(dim=-1).view(num_relations, self.num_heads, -1, self.window_size, self.window_size).mean(dim=1)   # (num_relations, num_windows, window_size, window_size)\n",
    "        end3 = time.time()\n",
    "        \n",
    "        torch.cuda.synchronize()  # 同步\n",
    "        start4 = time.time()\n",
    "        attn = self.gumbel_softmax_top_k(attn, tau=self.temperature, hard=True)                                                 # (num_relations, num_windows, window_size, window_size)\n",
    "        end4 = time.time()\n",
    "        \n",
    "        torch.cuda.synchronize()  # 同步\n",
    "        start5 = time.time()\n",
    "        result = result + self.windows2adjacent(attn, Q_index, output, device)\n",
    "        end5 = time.time()\n",
    "        \n",
    "        print(f\"0运行时间: {end0 - start0:.6f} 秒\")\n",
    "        print(f\"1运行时间: {end1 - start1:.6f} 秒\")\n",
    "        print(f\"2运行时间: {end2 - start2:.6f} 秒\")\n",
    "        print(f\"3运行时间: {end3 - start3:.6f} 秒\")\n",
    "        print(f\"4运行时间: {end4 - start4:.6f} 秒\")\n",
    "        print(f\"5运行时间: {end5 - start5:.6f} 秒\")\n",
    "        \n",
    "        return attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0运行时间: 0.000011 秒\n",
      "1运行时间: 0.000460 秒\n",
      "2运行时间: 0.001107 秒\n",
      "3运行时间: 0.000232 秒\n",
      "4运行时间: 0.000257 秒\n",
      "5运行时间: 0.003100 秒\n",
      "运行时间: 0.044931 秒\n",
      "torch.Size([7, 61, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "input_dim = relational_output.shape[-1]\n",
    "output_dim = 256\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 3\n",
    "num_nodes = relational_output.size(0)\n",
    "\n",
    "\n",
    "relational_output = relational_output.to(device)\n",
    "module = Rewirescorelayer(input_dim, output_dim, num_heads, window_size, k).to(device)\n",
    "start = time.time()\n",
    "attn_output = module(graph, relational_output)\n",
    "end = time.time()\n",
    "print(f\"运行时间: {end - start:.6f} 秒\")\n",
    "print(attn_output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 1., 0., 1.],\n",
      "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "          ...,\n",
      "          [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 1., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0.,  ..., 1., 0., 1.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 1.],\n",
      "          [1., 0., 0.,  ..., 1., 0., 0.],\n",
      "          ...,\n",
      "          [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "          [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 1., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 1.,  ..., 1., 1., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 1., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 1., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 0.,  ..., 0., 1., 0.],\n",
      "          [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 0., 1.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1.,  ..., 1., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 1., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 1., 0., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 1.],\n",
      "          ...,\n",
      "          [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "          [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "          [0., 0., 1.,  ..., 1., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 1., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "          [1., 0., 0.,  ..., 0., 1., 0.],\n",
      "          ...,\n",
      "          [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "          [1., 0., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 1., 1., 0.]],\n",
      "\n",
      "         [[0., 1., 0.,  ..., 1., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "          ...,\n",
      "          [0., 1., 1.,  ..., 0., 1., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "          ...,\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1.,  ..., 1., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "          [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 0., 1.,  ..., 0., 0., 1.],\n",
      "          [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 1.,  ..., 1., 0., 0.],\n",
      "          [0., 1., 1.,  ..., 0., 0., 1.],\n",
      "          [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "          ...,\n",
      "          [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 1., 0., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 1., 1.,  ..., 0., 1., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 1., 1.,  ..., 0., 1., 0.],\n",
      "          ...,\n",
      "          [0., 1., 1.,  ..., 0., 1., 0.],\n",
      "          [0., 1., 1.,  ..., 0., 1., 0.],\n",
      "          [0., 1., 1.,  ..., 0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 1., 1., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 1., 1., 0.]],\n",
      "\n",
      "         [[0., 1., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 1.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 1.,  ..., 1., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "          [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "          ...,\n",
      "          [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "          [0., 1., 1.,  ..., 1., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "          [1., 0., 0.,  ..., 1., 0., 0.],\n",
      "          ...,\n",
      "          [1., 1., 0.,  ..., 1., 0., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 1., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "          [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 1.]],\n",
      "\n",
      "         [[0., 1., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "          [1., 0., 0.,  ..., 0., 1., 0.],\n",
      "          ...,\n",
      "          [0., 1., 0.,  ..., 1., 1., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 1.],\n",
      "          [0., 1., 0.,  ..., 0., 1., 0.]],\n",
      "\n",
      "         [[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1.,  ..., 0., 0., 1.],\n",
      "          [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "          ...,\n",
      "          [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 1.],\n",
      "          [0., 1., 1.,  ..., 1., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 1.],\n",
      "          [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 1.]],\n",
      "\n",
      "         [[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "          ...,\n",
      "          [1., 0., 1.,  ..., 1., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 1., 0.,  ..., 0., 0., 1.],\n",
      "          [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 1.],\n",
      "          ...,\n",
      "          [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "          [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 1., 0.,  ..., 1., 1., 0.]],\n",
      "\n",
      "         [[1., 1., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "          [0., 0., 1.,  ..., 0., 1., 0.]],\n",
      "\n",
      "         [[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "          ...,\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 0.,  ..., 0., 1., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 0., 0.,  ..., 0., 1., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 1.,  ..., 0., 1., 1.],\n",
      "          [0., 0., 1.,  ..., 0., 0., 1.],\n",
      "          [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [0., 0., 1.,  ..., 0., 0., 1.],\n",
      "          [0., 0., 1.,  ..., 0., 1., 1.],\n",
      "          [0., 0., 1.,  ..., 0., 1., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 1., 0., 1.]],\n",
      "\n",
      "         [[0., 1., 1.,  ..., 0., 1., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "          ...,\n",
      "          [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 0., 1.],\n",
      "          [0., 1., 1.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 1.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "          ...,\n",
      "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 1., 0.],\n",
      "          [0., 1., 0.,  ..., 0., 1., 0.]]]], device='cuda:1',\n",
      "       grad_fn=<AddBackward0>)\n",
      "(tensor([0, 0, 0,  ..., 6, 6, 6], device='cuda:1'), tensor([ 0,  0,  0,  ..., 60, 60, 60], device='cuda:1'), tensor([0, 0, 0,  ..., 9, 9, 9], device='cuda:1'), tensor([4, 7, 9,  ..., 1, 3, 8], device='cuda:1'))\n"
     ]
    }
   ],
   "source": [
    "print(attn_output)\n",
    "indices = torch.nonzero(attn_output, as_tuple=True)\n",
    "print(indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试不同degree进行采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds:\n",
      " tensor([0.2000, 0.5000, 0.7000, 0.9000, 0.1000, 0.3000, 0.6000, 0.8000, 0.4000,\n",
      "        0.2000])\n",
      "Sampled Matrix:\n",
      " tensor([[1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gumbel_softmax_sample(logits, tau):\n",
    "    # 从Gumbel(0, 1)分布中采样\n",
    "    gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits)))\n",
    "    y = logits + gumbel_noise\n",
    "    return F.softmax(y / tau, dim=-1)\n",
    "\n",
    "def bernoulli_sampling_with_different_thresholds(probs, thresholds, tau=1.0):\n",
    "    # 对数概率\n",
    "    logits = torch.log(probs) - torch.log(1 - probs)\n",
    "    # 进行Gumbel-Softmax采样\n",
    "    y = gumbel_softmax_sample(logits, tau)\n",
    "    # 硬化处理，根据每行的不同阈值\n",
    "    z = (y > thresholds.unsqueeze(1)).float()\n",
    "    return z\n",
    "\n",
    "# 示例矩阵\n",
    "n = 10\n",
    "P = torch.rand(n, n)\n",
    "\n",
    "# 为每一行设置不同的阈值\n",
    "thresholds = torch.tensor([0.2, 0.5, 0.7, 0.9, 0.1, 0.3, 0.6, 0.8, 0.4, 0.2])\n",
    "\n",
    "# 进行可微分伯努利采样并硬化处理\n",
    "tau = 0.1  # 温度参数\n",
    "sampled_matrix = bernoulli_sampling_with_different_thresholds(P, thresholds, tau)\n",
    "\n",
    "#print(\"Probability Matrix:\\n\", P)\n",
    "print(\"Thresholds:\\n\", thresholds)\n",
    "print(\"Sampled Matrix:\\n\", sampled_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewired_gearnet 用于diffusion模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewireGearnet(nn.Module):\n",
    "    gradient_checkpoint = False\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(RewireGearnet, self).__init__()\n",
    "        self.num_relation = num_relation\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.linear = nn.Linear(num_relation * input_dim, output_dim)\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(output_dim) if batch_norm else None\n",
    "        self.activation = getattr(F, activation) if activation else None\n",
    "        self.edge_linear = nn.Linear(edge_input_dim, output_dim) if edge_input_dim else None\n",
    "\n",
    "    def trans(self, A, graph):\n",
    "        n_rel = graph.num_relation\n",
    "        n = A.size(0)\n",
    "        n_rel = n_rel.item()  # 将 n_rel 从 Tensor 转换为 int\n",
    "        assert n % n_rel == 0, \"n must be divisible by n_rel\"\n",
    "        \n",
    "        block_size = n // n_rel\n",
    "        \n",
    "        # 初始化一个张量来存储累加结果\n",
    "        accumulated = torch.zeros_like(A[:block_size])\n",
    "        \n",
    "        # 将后面的所有块累加到第一块\n",
    "        for i in range(n_rel):\n",
    "            accumulated += A[i * block_size: (i + 1) * block_size]\n",
    "        \n",
    "        # 用累加后的第一块替换原始矩阵的第一块\n",
    "        A_trans = accumulated\n",
    "    \n",
    "        return A_trans\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "\n",
    "        device = input.device  # Ensure device consistency\n",
    "\n",
    "        if new_edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "            node_out = node_out * self.num_relation + relation\n",
    "            adjacency = torch.sparse_coo_tensor(\n",
    "                torch.stack([node_in, node_out]),\n",
    "                graph.edge_weight.to(device),\n",
    "                (graph.num_node, graph.num_node * graph.num_relation),\n",
    "                device=device\n",
    "            )\n",
    "            update = torch.sparse.mm(adjacency.t(), input)\n",
    "        else:\n",
    "            adjacency = self.trans(new_edge_list, graph).to(device)\n",
    "            update = torch.mm(adjacency.t(), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = graph.edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(\n",
    "                edge_input * edge_weight, node_out, dim=0,\n",
    "                dim_size=graph.num_node * graph.num_relation\n",
    "            )\n",
    "            update += edge_update\n",
    "            \n",
    "        return update.view(graph.num_node, self.num_relation * self.input_dim).to(device)\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm.to(device)\n",
    "        \n",
    "        output = self.linear(update) + self.self_loop(input)\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        \"\"\"\n",
    "        Perform message passing over the graph(s).\n",
    "\n",
    "        Parameters:\n",
    "            graph (Graph): graph(s)\n",
    "            input (Tensor): node representations of shape :math:`(|V|, ...)`\n",
    "        \"\"\"\n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self.message_and_aggregate, graph, input)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph, input, new_edge_list)\n",
    "        output = self.combine(input, update)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1165, 512])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1327, 0.0000],\n",
      "        [0.0000, 0.0320, 0.0000,  ..., 0.0000, 0.0880, 0.1370],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3040, 0.2115],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0793,  ..., 0.2069, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.4240],\n",
      "        [0.0000, 0.3808, 0.3437,  ..., 0.0000, 0.0000, 0.1364]],\n",
      "       device='cuda:1', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 512\n",
    "num_relations = graph.num_relation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "new_node_feature = RewireGearnet(input_dim, output_dim, num_relations)(graph, graph.node_feature.to(device).float(), attn_output).to(device)\n",
    "\n",
    "print(new_node_feature.shape)\n",
    "print(new_node_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个装饰器来计时\n",
    "def time_layer(layer, layer_name):\n",
    "    def timed_layer(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        output = layer(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f'{layer_name}: {end_time - start_time:.6f} seconds')\n",
    "        return output\n",
    "    return timed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMGearnet(nn.Module, core.Configurable):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims, score_dim, num_relation, num_heads, window_size, k, edge_input_dim=None, num_angle_bin=None,\n",
    "                 short_cut=False, batch_norm=False, activation=\"relu\", concat_hidden=True, readout=\"sum\"):\n",
    "        super(DGMGearnet, self).__init__()\n",
    "\n",
    "        #if not isinstance(hidden_dims, Sequence):\n",
    "            #hidden_dims = [hidden_dims]\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = sum(hidden_dims) if concat_hidden else hidden_dims[-1]\n",
    "        self.dims = [input_dim] + list(hidden_dims)\n",
    "        self.score_dim = score_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.k = k\n",
    "        self.edge_dims = [edge_input_dim] + self.dims[:-1]\n",
    "        self.num_relation = num_relation\n",
    "        self.num_angle_bin = num_angle_bin\n",
    "        self.short_cut = short_cut\n",
    "        self.concat_hidden = concat_hidden\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.score_layers = nn.ModuleList()\n",
    "        for i in range(len(self.dims) - 1):\n",
    "            \n",
    "            self.score_layers.append(relationalGraph(self.dims[i], self.score_dim, num_relation, \n",
    "                                                     edge_input_dim=None, batch_norm=False, activation=\"relu\")) \n",
    "\n",
    "            \n",
    "            self.score_layers.append(Rewirescorelayer(self.score_dim, self.dims[i+1], self.num_heads, self.window_size, \n",
    "                                            self.k, temperature=0.5, dropout=0.1))\n",
    "            \n",
    "            self.layers.append(RewireGearnet(self.dims[i], self.dims[i + 1], num_relation,\n",
    "                                            edge_input_dim=None, batch_norm=False, activation=\"relu\"))\n",
    "        \n",
    "        if num_angle_bin:\n",
    "            self.spatial_line_graph = layers.SpatialLineGraph(num_angle_bin)\n",
    "            self.edge_layers = nn.ModuleList()\n",
    "            for i in range(len(self.edge_dims) - 1):\n",
    "                self.edge_layers.append(layers.GeometricRelationalGraphConv(\n",
    "                    self.edge_dims[i], self.edge_dims[i + 1], num_angle_bin, None, batch_norm, activation))\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norms = nn.ModuleList()\n",
    "            for i in range(len(self.dims) - 1):\n",
    "                self.batch_norms.append(nn.BatchNorm1d(self.dims[i + 1]))\n",
    "\n",
    "        if readout == \"sum\":\n",
    "            self.readout = layers.SumReadout()\n",
    "        elif readout == \"mean\":\n",
    "            self.readout = layers.MeanReadout()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown readout `%s`\" % readout)\n",
    "\n",
    "    def forward(self, graph, input, edge_list=None, all_loss=None, metric=None):\n",
    "        \"\"\"\n",
    "        Compute the node representations and the graph representation(s).\n",
    "\n",
    "        Parameters:\n",
    "            graph (Graph): :math:`n` graph(s)\n",
    "            input (Tensor): input node representations\n",
    "            all_loss (Tensor, optional): if specified, add loss to this tensor\n",
    "            metric (dict, optional): if specified, output metrics to this dict\n",
    "\n",
    "        Returns:\n",
    "            dict with ``node_feature`` and ``graph_feature`` fields:\n",
    "                node representations of shape :math:`(|V|, d)`, graph representations of shape :math:`(n, d)`\n",
    "        \"\"\"\n",
    "        hiddens = []\n",
    "        layer_input = input\n",
    "        if self.num_angle_bin:\n",
    "            line_graph = self.spatial_line_graph(graph)\n",
    "            edge_input = line_graph.node_feature.float()\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            \n",
    "            relational_output = time_layer(self.score_layers[2*i], 'relational output')(graph, layer_input, edge_list)\n",
    "            new_edge_list = time_layer(self.score_layers[2*i+1], 'new edge list')(graph, relational_output)\n",
    "            \n",
    "            hidden = time_layer(self.layers[i], 'hidden')(graph, layer_input, new_edge_list)\n",
    "            \n",
    "            if self.short_cut and hidden.shape == layer_input.shape:\n",
    "                hidden = hidden + layer_input\n",
    "            \n",
    "            if self.num_angle_bin:\n",
    "                edge_hidden = self.edge_layers[i](line_graph, edge_input)\n",
    "                edge_weight = graph.edge_weight.unsqueeze(-1)\n",
    "                if new_edge_list is None:\n",
    "                    node_out = graph.edge_list[:, 1] * self.num_relation + graph.edge_list[:, 2]\n",
    "                else:\n",
    "                    node_out = new_edge_list[:, 1] * self.num_relation + new_edge_list[:, 2]\n",
    "                \n",
    "                    update = scatter_add(edge_hidden * edge_weight, node_out, dim=0,\n",
    "                                        dim_size=graph.num_node * self.num_relation)\n",
    "                update = update.view(graph.num_node, self.num_relation * edge_hidden.shape[1])\n",
    "                update = self.layers[i].linear(update)\n",
    "                update = self.layers[i].activation(update)\n",
    "                hidden = hidden + update\n",
    "                edge_input = edge_hidden\n",
    "                \n",
    "            if self.batch_norm:\n",
    "                hidden = self.batch_norms[i](hidden)\n",
    "                \n",
    "            hiddens.append(hidden)\n",
    "            layer_input = hidden\n",
    "            edge_list = new_edge_list\n",
    "\n",
    "        if self.concat_hidden:\n",
    "            node_feature = torch.cat(hiddens, dim=-1)\n",
    "        else:\n",
    "            node_feature = hiddens[-1]\n",
    "        graph_feature = self.readout(graph, node_feature)\n",
    "\n",
    "        return {\n",
    "            \"graph_feature\": graph_feature,\n",
    "            \"node_feature\": node_feature\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relational output: 0.001634 seconds\n",
      "new edge list: 0.131950 seconds\n",
      "hidden: 0.003069 seconds\n",
      "relational output: 0.044167 seconds\n",
      "new edge list: 0.135501 seconds\n",
      "hidden: 0.003238 seconds\n",
      "relational output: 0.043538 seconds\n",
      "new edge list: 0.131423 seconds\n",
      "hidden: 0.003260 seconds\n",
      "relational output: 0.042964 seconds\n",
      "new edge list: 0.130124 seconds\n",
      "hidden: 0.003250 seconds\n",
      "relational output: 0.042751 seconds\n",
      "new edge list: 0.147749 seconds\n",
      "hidden: 0.003250 seconds\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "hidden_dims = [512, 512, 512, 512, 512]\n",
    "score_dim = 128\n",
    "num_relations = graph.num_relation\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "k = 5\n",
    "\n",
    "\n",
    "output = DGMGearnet(input_dim, hidden_dims, score_dim, num_relations, num_heads, window_size, k).to(device)(graph.to(device), graph.node_feature.to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5056,  0.0000,  0.0000,  ...,  0.0000, 12.6781,  0.0000],\n",
      "        [ 0.1331,  0.0000,  0.0000,  ...,  0.0000,  1.6680,  3.1325],\n",
      "        [ 0.4216,  0.0000,  0.0000,  ...,  0.0000,  4.0469,  6.2530],\n",
      "        ...,\n",
      "        [ 0.4255,  0.0000,  0.0000,  ...,  0.0000, 12.3883,  0.0000],\n",
      "        [ 0.6212,  0.0000,  0.0000,  ...,  0.0000,  5.0837,  1.6823],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  7.2654,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>)\n",
      "torch.Size([572, 2560])\n",
      "\n",
      "\n",
      "tensor([[  60.2262,    4.7113,   13.5840,  ...,   23.6377, 2026.7983,\n",
      "          116.4902],\n",
      "        [  66.2441,    3.1500,   15.5229,  ...,   13.4601, 2375.6035,\n",
      "          124.7936]], device='cuda:0', grad_fn=<ScatterAddBackward>)\n",
      "torch.Size([2, 2560])\n"
     ]
    }
   ],
   "source": [
    "print(output[\"node_feature\"])\n",
    "print(output[\"node_feature\"].shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(output[\"graph_feature\"])\n",
    "print(output[\"graph_feature\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 10], [10, 20], [20, 30], [30, 40], [40, 50], [50, 60], [60, 70], [70, 80], [80, 90], [90, 100], [100, 110], [110, 120], [120, 130], [130, 140], [140, 150], [150, 160], [160, 170], [170, 180], [180, 190], [190, 200], [200, 210], [210, 220], [220, 230], [230, 240], [240, 250], [250, 260], [254, 264], [264, 274], [274, 284], [284, 294], [294, 304], [304, 314], [314, 324], [324, 334], [334, 344], [344, 354], [354, 364], [364, 374], [374, 384], [384, 394], [394, 404], [404, 414], [414, 424], [424, 434], [434, 444], [444, 454], [454, 464], [464, 474], [474, 484], [484, 494], [494, 504], [504, 514], [514, 524], [524, 534], [534, 544], [544, 554], [554, 564], [562, 572]]\n",
      "[[0, 10], [10, 20], [20, 30], [30, 40], [40, 50], [50, 60], [60, 70], [70, 80], [80, 90], [90, 100], [100, 110], [110, 120], [120, 130], [130, 140], [140, 150], [150, 160], [160, 170], [170, 180], [180, 190], [190, 200], [200, 210], [210, 220], [220, 230], [230, 240], [240, 250], [250, 260], [254, 264], [264, 274], [274, 284], [284, 294], [294, 304], [304, 314], [314, 324], [324, 334], [334, 344], [344, 354], [354, 364], [364, 374], [374, 384], [384, 394], [394, 404], [404, 414], [414, 424], [424, 434], [434, 444], [444, 454], [454, 464], [464, 474], [474, 484], [484, 494], [494, 504], [504, 514], [514, 524], [524, 534], [534, 544], [544, 554], [554, 564], [562, 572]]\n",
      "[[0, 10], [10, 20], [20, 30], [30, 40], [40, 50], [50, 60], [60, 70], [70, 80], [80, 90], [90, 100], [100, 110], [110, 120], [120, 130], [130, 140], [140, 150], [150, 160], [160, 170], [170, 180], [180, 190], [190, 200], [200, 210], [210, 220], [220, 230], [230, 240], [240, 250], [250, 260], [254, 264], [264, 274], [274, 284], [284, 294], [294, 304], [304, 314], [314, 324], [324, 334], [334, 344], [344, 354], [354, 364], [364, 374], [374, 384], [384, 394], [394, 404], [404, 414], [414, 424], [424, 434], [434, 444], [444, 454], [454, 464], [464, 474], [474, 484], [484, 494], [494, 504], [504, 514], [514, 524], [524, 534], [534, 544], [544, 554], [554, 564], [562, 572]]\n",
      "[[0, 10], [10, 20], [20, 30], [30, 40], [40, 50], [50, 60], [60, 70], [70, 80], [80, 90], [90, 100], [100, 110], [110, 120], [120, 130], [130, 140], [140, 150], [150, 160], [160, 170], [170, 180], [180, 190], [190, 200], [200, 210], [210, 220], [220, 230], [230, 240], [240, 250], [250, 260], [254, 264], [264, 274], [274, 284], [284, 294], [294, 304], [304, 314], [314, 324], [324, 334], [334, 344], [344, 354], [354, 364], [364, 374], [374, 384], [384, 394], [394, 404], [404, 414], [414, 424], [424, 434], [434, 444], [444, 454], [454, 464], [464, 474], [474, 484], [484, 494], [494, 504], [504, 514], [514, 524], [524, 534], [534, 544], [544, 554], [554, 564], [562, 572]]\n",
      "[[0, 10], [10, 20], [20, 30], [30, 40], [40, 50], [50, 60], [60, 70], [70, 80], [80, 90], [90, 100], [100, 110], [110, 120], [120, 130], [130, 140], [140, 150], [150, 160], [160, 170], [170, 180], [180, 190], [190, 200], [200, 210], [210, 220], [220, 230], [230, 240], [240, 250], [250, 260], [254, 264], [264, 274], [274, 284], [284, 294], [294, 304], [304, 314], [314, 324], [324, 334], [334, 344], [344, 354], [354, 364], [364, 374], [374, 384], [384, 394], [394, 404], [404, 414], [414, 424], [424, 434], [434, 444], [444, 454], [454, 464], [464, 474], [474, 484], [484, 494], [494, 504], [504, 514], [514, 524], [524, 534], [534, 544], [544, 554], [554, 564], [562, 572]]\n",
      "tensor(159.2843, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Layer layers.0.linear.weight - Gradient_norm: 142.38612365722656\n",
      "Layer layers.0.linear.bias - Gradient_norm: 42.30305862426758\n",
      "Layer layers.0.self_loop.weight - Gradient_norm: 10.722688674926758\n",
      "Layer layers.0.self_loop.bias - Gradient_norm: 42.30305862426758\n",
      "Layer layers.1.linear.weight - Gradient_norm: 1085.5723876953125\n",
      "Layer layers.1.linear.bias - Gradient_norm: 19.160188674926758\n",
      "Layer layers.1.self_loop.weight - Gradient_norm: 80.69200134277344\n",
      "Layer layers.1.self_loop.bias - Gradient_norm: 19.160188674926758\n",
      "Layer layers.2.linear.weight - Gradient_norm: 1028.5391845703125\n",
      "Layer layers.2.linear.bias - Gradient_norm: 7.7760796546936035\n",
      "Layer layers.2.self_loop.weight - Gradient_norm: 76.77202606201172\n",
      "Layer layers.2.self_loop.bias - Gradient_norm: 7.7760796546936035\n",
      "Layer layers.3.linear.weight - Gradient_norm: 810.7118530273438\n",
      "Layer layers.3.linear.bias - Gradient_norm: 2.6841704845428467\n",
      "Layer layers.3.self_loop.weight - Gradient_norm: 60.10576629638672\n",
      "Layer layers.3.self_loop.bias - Gradient_norm: 2.6841704845428467\n",
      "Layer layers.4.linear.weight - Gradient_norm: 534.0957641601562\n",
      "Layer layers.4.linear.bias - Gradient_norm: 0.8030843734741211\n",
      "Layer layers.4.self_loop.weight - Gradient_norm: 39.5841178894043\n",
      "Layer layers.4.self_loop.bias - Gradient_norm: 0.8030843734741211\n",
      "Layer score_layers.0.self_loop.weight - Gradient_norm: 0.294857382774353\n",
      "Layer score_layers.0.self_loop.bias - Gradient_norm: 0.21014831960201263\n",
      "Layer score_layers.0.linear.weight - Gradient_norm: 0.21081219613552094\n",
      "Layer score_layers.0.linear.bias - Gradient_norm: 0.21014831960201263\n",
      "Layer score_layers.1.query.weight - Gradient_norm: 0.6939507126808167\n",
      "Layer score_layers.1.query.bias - Gradient_norm: 0.4370785057544708\n",
      "Layer score_layers.1.key.weight - Gradient_norm: 0.8381465673446655\n",
      "Layer score_layers.1.key.bias - Gradient_norm: 7.266617467394099e-05\n",
      "Layer score_layers.2.self_loop.weight - Gradient_norm: 1.1193978786468506\n",
      "Layer score_layers.2.self_loop.bias - Gradient_norm: 0.18650929629802704\n",
      "Layer score_layers.2.linear.weight - Gradient_norm: 1.2206106185913086\n",
      "Layer score_layers.2.linear.bias - Gradient_norm: 0.18650929629802704\n",
      "Layer score_layers.3.query.weight - Gradient_norm: 0.4565069079399109\n",
      "Layer score_layers.3.query.bias - Gradient_norm: 0.39186304807662964\n",
      "Layer score_layers.3.key.weight - Gradient_norm: 0.5839003324508667\n",
      "Layer score_layers.3.key.bias - Gradient_norm: 3.3122698368970305e-05\n",
      "Layer score_layers.4.self_loop.weight - Gradient_norm: 2.8158364295959473\n",
      "Layer score_layers.4.self_loop.bias - Gradient_norm: 0.22871729731559753\n",
      "Layer score_layers.4.linear.weight - Gradient_norm: 3.94661283493042\n",
      "Layer score_layers.4.linear.bias - Gradient_norm: 0.22871729731559753\n",
      "Layer score_layers.5.query.weight - Gradient_norm: 1.6127146482467651\n",
      "Layer score_layers.5.query.bias - Gradient_norm: 0.5434131026268005\n",
      "Layer score_layers.5.key.weight - Gradient_norm: 1.7648046016693115\n",
      "Layer score_layers.5.key.bias - Gradient_norm: 6.1237376939971e-05\n",
      "Layer score_layers.6.self_loop.weight - Gradient_norm: 3.2567684650421143\n",
      "Layer score_layers.6.self_loop.bias - Gradient_norm: 0.10878679156303406\n",
      "Layer score_layers.6.linear.weight - Gradient_norm: 3.897597074508667\n",
      "Layer score_layers.6.linear.bias - Gradient_norm: 0.10878679156303406\n",
      "Layer score_layers.7.query.weight - Gradient_norm: 1.78696870803833\n",
      "Layer score_layers.7.query.bias - Gradient_norm: 0.23747429251670837\n",
      "Layer score_layers.7.key.weight - Gradient_norm: 1.7926740646362305\n",
      "Layer score_layers.7.key.bias - Gradient_norm: 5.7579512940719724e-05\n",
      "Layer score_layers.8.self_loop.weight - Gradient_norm: 5.970613479614258\n",
      "Layer score_layers.8.self_loop.bias - Gradient_norm: 0.0917118638753891\n",
      "Layer score_layers.8.linear.weight - Gradient_norm: 5.376163482666016\n",
      "Layer score_layers.8.linear.bias - Gradient_norm: 0.0917118638753891\n",
      "Layer score_layers.9.query.weight - Gradient_norm: 2.5112555027008057\n",
      "Layer score_layers.9.query.bias - Gradient_norm: 0.17762146890163422\n",
      "Layer score_layers.9.key.weight - Gradient_norm: 2.477689266204834\n",
      "Layer score_layers.9.key.bias - Gradient_norm: 1.4803063095314428e-05\n"
     ]
    }
   ],
   "source": [
    "model = DGMGearnet(input_dim, hidden_dims, score_dim, num_relations, num_heads, window_size, k).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "output = model(graph.to(device), graph.node_feature.to(device).float())\n",
    "\n",
    "output = output[\"graph_feature\"]\n",
    "\n",
    "# 定义一个简单的损失函数，例如均方误差\n",
    "criterion = F.binary_cross_entropy_with_logits\n",
    "target = torch.rand_like(output)\n",
    "\n",
    "# 计算损失\n",
    "loss = criterion(output, target, reduction=\"mean\")\n",
    "print(loss)\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "\n",
    "# 检查梯度是否为 NaN 或 inf\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        if torch.isnan(param.grad).any():\n",
    "            print(f\"Gradient of {name} contains NaN values.\")\n",
    "        if torch.isinf(param.grad).any():\n",
    "            print(f\"Gradient of {name} contains inf values.\")\n",
    "        else:\n",
    "            print(f\"Layer {name} - Gradient_norm: {torch.norm(param.grad)}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"No gradient found for {name}\")\n",
    "\n",
    "# 检查损失是否为 NaN\n",
    "if torch.isnan(loss):\n",
    "    print(\"Loss is NaN.\")\n",
    "        \n",
    "# 更新参数\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gearnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
