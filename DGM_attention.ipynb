{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdrug import  layers, datasets,transforms,core\n",
    "from torchdrug.core import Registry as R\n",
    "from torchdrug.layers import geometry\n",
    "\n",
    "import torch\n",
    "from torchdrug import data\n",
    "\n",
    "from torch_scatter import scatter_add\n",
    "import torch.nn as nn\n",
    "from torchdrug import utils\n",
    "from torch.utils import checkpoint\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:24:09   Extracting /home/xiaotong/scratch/protein-datasets/EnzymeCommission.zip to /home/xiaotong/scratch/protein-datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/xiaotong/scratch/protein-datasets/EnzymeCommission/enzyme_commission.pkl.gz: 100%|██████████| 18716/18716 [00:46<00:00, 406.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "PackedProtein(batch_size=2, num_atoms=[185, 415], num_bonds=[3754, 8999], num_residues=[185, 415])\n"
     ]
    }
   ],
   "source": [
    "EnzymeCommission = R.search(\"datasets.EnzymeCommission\")\n",
    "PV = R.search(\"transforms.ProteinView\")\n",
    "trans = PV(view = \"residue\")\n",
    "dataset = EnzymeCommission(\"~/scratch/protein-datasets/\", test_cutoff=0.95, \n",
    "                           atom_feature=\"full\", bond_feature=\"full\", verbose=1, transform = trans)\n",
    "\n",
    "# 只保留alpha碳的简化格式\n",
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()], \n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(k=10, min_distance=5),\n",
    "                                                                 geometry.SequentialEdge(max_distance=2)\n",
    "                                                                 ],\n",
    "                                                    edge_feature=\"gearnet\"\n",
    "                                                    )\n",
    "\n",
    "\n",
    "graphs = dataset[:2]\n",
    "graphs = [element[\"graph\"] for element in graphs]\n",
    "graphs = data.Protein.pack(graphs)\n",
    "print(\"\\n\\n\")\n",
    "graph = graph_construction_model(graphs)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([185, 415])\n",
      "2\n",
      "tensor([[ 95,  96,   5],\n",
      "        [109, 110,   5],\n",
      "        [108, 109,   5],\n",
      "        ...,\n",
      "        [438, 470,   0],\n",
      "        [489, 470,   0],\n",
      "        [493, 470,   0]])\n"
     ]
    }
   ],
   "source": [
    "#graph = graph.to(device)\n",
    "\n",
    "print(graph.num_nodes)\n",
    "print(graph.batch_size)\n",
    "print(graph.edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关系卷积神经网络，获取多个不同的嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationalGraph(layers.MessagePassingBase):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(relationalGraph, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_relation = num_relation\n",
    "        self.edge_input_dim = edge_input_dim\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        if edge_input_dim:\n",
    "            self.edge_linear = nn.Linear(edge_input_dim, input_dim)\n",
    "        else:\n",
    "            self.edge_linear = None\n",
    "            \n",
    "    def trans(self, A, graph):\n",
    "    \n",
    "        Degree_inv_sqrt = torch.diag(torch.pow(torch.sum(A, dim=1), -0.5))\n",
    "        A_norm = torch.mm(torch.mm(Degree_inv_sqrt, A), Degree_inv_sqrt)\n",
    "        \n",
    "        n_rel = graph.num_relation\n",
    "        n = A_norm.size(0)\n",
    "        n_rel = n_rel.item()  # 将 n_rel 从 Tensor 转换为 int\n",
    "        assert n % n_rel == 0, \"n must be divisible by n_rel\"\n",
    "        \n",
    "        block_size = n // n_rel\n",
    "        \n",
    "        # 初始化一个张量来存储累加结果\n",
    "        accumulated = torch.zeros_like(A_norm[:block_size])\n",
    "        \n",
    "        # 将后面的所有块累加到第一块\n",
    "        for i in range(n_rel):\n",
    "            accumulated += A_norm[i * block_size: (i + 1) * block_size]\n",
    "        \n",
    "        # 用累加后的第一块替换原始矩阵的第一块\n",
    "        A_trans = accumulated\n",
    "    \n",
    "        return A_trans\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "        device = input.device  # Ensure device consistency\n",
    "        \n",
    "        if new_edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "            node_out = node_out * self.num_relation + relation\n",
    "        \n",
    "            edge_weight = torch.ones_like(node_out)\n",
    "            degree_out = scatter_add(edge_weight, node_out, dim_size=graph.num_node * graph.num_relation)\n",
    "            degree_out = degree_out\n",
    "            edge_weight = edge_weight / degree_out[node_out]\n",
    "            adjacency = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), edge_weight,\n",
    "                                                (graph.num_node, graph.num_node * graph.num_relation))\n",
    "            update = torch.sparse.mm(adjacency.t(), input)\n",
    "        \n",
    "        else:\n",
    "            adjacency = self.trans(new_edge_list, graph).to(device)\n",
    "            update = torch.mm(adjacency.t().to(device), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(edge_input * edge_weight, node_out, dim=0,\n",
    "                                      dim_size=graph.num_node * graph.num_relation)\n",
    "            update += edge_update\n",
    "\n",
    "        return update\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        # 自环特征\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        input = input.repeat(self.num_relation, 1).to(device)\n",
    "        loop_update = self.self_loop(input).to(device)\n",
    "        \n",
    "        output = self.linear(update)+loop_update\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        \n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self._message_and_aggregate, *graph.to_tensors(), input)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph, input, new_edge_list)\n",
    "        output = self.combine(input, update)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0387, 0.2849, 0.0469,  ..., 0.2518, 0.0481, 0.0000],\n",
      "        [0.2962, 0.1971, 0.1415,  ..., 0.1966, 0.0247, 0.0000],\n",
      "        [0.5053, 0.3646, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.4512, 0.3146, 0.0038,  ..., 0.0000, 0.1177, 0.0000],\n",
      "        [0.5053, 0.3646, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.2075, 0.3166, 0.0000,  ..., 0.0000, 0.0713, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "output:  torch.Size([4200, 512])\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 512\n",
    "num_relations = graph.num_relation\n",
    "\n",
    "relational_output = relationalGraph(input_dim, output_dim, num_relations)(graph, graph.node_feature.float(), new_edge_list = None)\n",
    "print(relational_output)\n",
    "print(\"output: \", relational_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重连接模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 只从点所在的图进行reconnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 185\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_start_end(current, graph):\n",
    "    \"\"\"\n",
    "    根据一维张量 a 生成新的张量 b。\n",
    "    \n",
    "    :param a: 输入的一维张量\n",
    "    :return: 输出的一维张量 b\n",
    "    \"\"\"\n",
    "    # 初始化 b，第一个元素是 0\n",
    "    segment = graph.num_nodes.repeat(graph.num_relation)\n",
    "    index = torch.zeros(segment.size(0) + 1, dtype=segment.dtype)\n",
    "    \n",
    "    # 计算 b 的每个元素\n",
    "    for i in range(1, len(index)):\n",
    "        index[i] = index[i - 1] + segment[i - 1]\n",
    "    \n",
    "    # 遍历张量以找到索引值的位置\n",
    "    for i in range(len(index) - 1):\n",
    "        if index[i] <= current < index[i + 1]:\n",
    "            return (index[i].item(), index[i + 1].item())\n",
    "        elif index[i] == current:\n",
    "            return (index[i].item(), index[i + 1].item())\n",
    "    \n",
    "    # 如果索引值恰好等于张量的最后一个元素\n",
    "    if current == index[-1]:\n",
    "        return (index[-1].item(), index[-1].item())\n",
    "    \n",
    "    \n",
    "\n",
    "# 示例使用\n",
    "start, end = get_start_end(184, graph)\n",
    "print(start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 785\n"
     ]
    }
   ],
   "source": [
    "def get_start_end2( current, graph):\n",
    "    segment = graph.num_nodes.repeat(graph.num_relation)\n",
    "    index = torch.cumsum(segment, dim=0)\n",
    "    \n",
    "    # Use torch.searchsorted to find the appropriate segment\n",
    "    pos = torch.searchsorted(index, current, right=True)\n",
    "\n",
    "    if pos == 0:\n",
    "        return (0, index[0].item())\n",
    "    elif pos >= len(index):\n",
    "        return (index[-1].item(), index[-1].item())\n",
    "    else:\n",
    "        return (index[pos-1].item(), index[pos].item())\n",
    "    \n",
    "start, end = get_start_end2(600, graph)\n",
    "print(start, end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gumble-softmax采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 1., 1.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 1., 1.]])\n",
      "tensor([[4.8048e-01, 3.7910e-04, 2.6873e-04, 4.0213e-03, 1.1619e-01, 5.7953e-04,\n",
      "         3.6010e-01, 6.4904e-04, 1.2300e-02, 2.5035e-02],\n",
      "        [5.5033e-04, 6.5851e-05, 1.9387e-05, 5.5963e-05, 9.5151e-02, 1.5899e-01,\n",
      "         5.4683e-05, 1.9378e-01, 5.4991e-01, 1.4249e-03],\n",
      "        [9.8238e-01, 2.3563e-05, 5.6371e-04, 5.5736e-03, 8.7534e-05, 5.6787e-05,\n",
      "         8.2759e-03, 2.6395e-03, 7.3919e-05, 3.2825e-04],\n",
      "        [6.5204e-05, 4.4989e-04, 9.1505e-03, 3.3549e-02, 8.4767e-03, 9.2767e-05,\n",
      "         4.9709e-03, 1.1033e-03, 9.4183e-01, 3.1603e-04],\n",
      "        [2.2411e-04, 3.5781e-06, 1.3032e-03, 9.5245e-07, 5.9685e-05, 1.1207e-04,\n",
      "         5.7596e-05, 5.7783e-03, 9.1864e-01, 7.3822e-02]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gumbel_softmax_top_k(logits, tau=0.5, k=1, hard=False):\n",
    "    \"\"\"\n",
    "    Gumbel-Softmax采样方法，每一步都是可微分的，并选择最大的k个元素。\n",
    "    \n",
    "    参数:\n",
    "        logits (torch.Tensor): 输入logits张量，维度为 (batch_size, num_classes)。\n",
    "        tau (float): Gumbel-Softmax的温度参数，控制平滑程度。\n",
    "        k (int): 选择最大的k个元素。\n",
    "        hard (bool): 是否返回硬分类结果。\n",
    "    \n",
    "    返回:\n",
    "        torch.Tensor: Gumbel-Softmax采样结果，维度为 (batch_size, num_classes)。\n",
    "    \"\"\"\n",
    "    # 获取Gumbel分布噪声\n",
    "    gumbels = -torch.empty_like(logits).exponential_().log()  # 生成Gumbel(0,1)噪声\n",
    "    gumbels = (logits + gumbels) / tau  # 添加噪声并除以温度参数\n",
    "\n",
    "    # 计算softmax\n",
    "    y_soft = F.softmax(gumbels, dim=-1)  # 维度为 (batch_size, num_classes)\n",
    "\n",
    "    if hard:\n",
    "        # 硬分类结果：选取原始logits最大的k个位置\n",
    "        topk_indices = logits.topk(k, dim=-1)[1]  # 获取前k个元素的索引\n",
    "        y_hard = torch.zeros_like(logits).scatter_(-1, topk_indices, 1.0)  # 生成one-hot向量\n",
    "        # 使用直通估计器\n",
    "        y = (y_hard - y_soft).detach() + y_soft\n",
    "    else:\n",
    "        y = y_soft\n",
    "\n",
    "    return y\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == \"__main__\":\n",
    "    logits = torch.randn(5, 10)  # 维度为 (batch_size=5, num_classes=10)\n",
    "    tau = 0.5\n",
    "    k = 3\n",
    "    hard = True\n",
    "    samples = gumbel_softmax_top_k(logits, tau, k, hard)\n",
    "    print(samples)\n",
    "    samples = gumbel_softmax_top_k(logits, tau, k)\n",
    "    print(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### window self attention + gumble softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Rewirescorelayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, window_size, k, temperature=0.5, dropout=0.1):\n",
    "        super(Rewirescorelayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_features = out_features\n",
    "        self.window_size = window_size\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.k = k\n",
    "\n",
    "        self.query = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.key = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.value = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([out_features])).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    def get_start_end(self, current, graph):\n",
    "        segment = graph.num_nodes.repeat(graph.num_relation)\n",
    "        index = torch.cumsum(segment, dim=0)\n",
    "        \n",
    "        # Use torch.searchsorted to find the appropriate segment\n",
    "        pos = torch.searchsorted(index, current, right=True)\n",
    "\n",
    "        if pos == 0:\n",
    "            return (0, index[0].item())\n",
    "        elif pos >= len(index):\n",
    "            return (index[-1].item(), index[-1].item())\n",
    "        else:\n",
    "            return (index[pos-1].item(), index[pos].item())\n",
    "\n",
    "    def gumbel_softmax_top_k(self, logits, tau=1.0, hard=False):\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "        gumbels = (logits + gumbels) / tau\n",
    "\n",
    "        y_soft = F.softmax(gumbels, dim=-1)\n",
    "\n",
    "        if hard:\n",
    "            topk_indices = logits.topk(self.k, dim=-1)[1]\n",
    "            y_hard = torch.zeros_like(logits).scatter_(-1, topk_indices, 1.0)\n",
    "            y = (y_hard - y_soft).detach() + y_soft\n",
    "        else:\n",
    "            y = y_soft\n",
    "\n",
    "        return y\n",
    "\n",
    "    def forward(self, graph, node_features):\n",
    "        device = node_features.device\n",
    "        num_nodes = node_features.size(0)\n",
    "        half_window = self.window_size // 2\n",
    "\n",
    "        Q = self.query(node_features).view(num_nodes, self.num_heads, self.out_features)\n",
    "        K = self.key(node_features).view(num_nodes, self.num_heads, self.out_features)\n",
    "\n",
    "        output = torch.zeros(num_nodes, num_nodes, device=device)\n",
    "\n",
    "        start_end_indices = [self.get_start_end(i, graph) for i in range(num_nodes)]\n",
    "\n",
    "        # Precompute K_windows\n",
    "        K_windows_padded = torch.zeros((num_nodes, self.window_size, self.num_heads, self.out_features), device=device)\n",
    "        for i in range(num_nodes):\n",
    "            start_index, end_index = start_end_indices[i]\n",
    "            start = max(start_index, i - half_window)\n",
    "            end = min(end_index, i + half_window)\n",
    "            K_window = K[start:end]\n",
    "            K_windows_padded[i, :K_window.size(0)] = K_window\n",
    "\n",
    "        #scores = torch.einsum(\"nhd,nmhd->nhm\", Q, K_windows_padded) / self.scale        # (num_nodes, num_heads, window_size), nhm\n",
    "        batch_size = 100  # 根据GPU内存调整批量大小\n",
    "        scores_list = []\n",
    "\n",
    "        for i in range(0, num_nodes, batch_size):\n",
    "            end_i = min(i + batch_size, num_nodes)\n",
    "            scores_batch = torch.einsum(\"nhd,nmhd->nhm\", Q[i:end_i], K_windows_padded[i:end_i]) / self.scale\n",
    "            scores_list.append(scores_batch)\n",
    "\n",
    "        # 将所有批次的得分合并为一个张量\n",
    "        scores = torch.cat(scores_list, dim=0)\n",
    "        \n",
    "        scores = scores / self.temperature   # nhm\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1).mean(dim=1)   # (num_nodes, window_size)\n",
    "        \n",
    "\n",
    "        for i in range(num_nodes):\n",
    "            start_index, end_index = start_end_indices[i]\n",
    "            start = max(start_index, i - half_window)\n",
    "            end = min(end_index, i + half_window )\n",
    "            output[i, start:end] = attention_weights[i, :end-start]\n",
    "\n",
    "        edge_list = self.gumbel_softmax_top_k(output, self.temperature, self.k)\n",
    "\n",
    "        return edge_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dim = relational_output.shape[-1]\n",
    "output_dim = 1024\n",
    "num_heads = 8\n",
    "window_size = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 5\n",
    "\n",
    "\n",
    "relational_output = relational_output.to(device)\n",
    "module = Rewirescorelayer(input_dim, output_dim, num_heads, window_size, k).to(device)\n",
    "attn_output = module(graph, relational_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4200, 4200])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "(tensor([   0,    0,    0,  ..., 4199, 4199, 4199], device='cuda:0'), tensor([   7,   10,   12,  ..., 4193, 4194, 4195], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(attn_output.shape)\n",
    "print(attn_output)\n",
    "#a = attn_output[1200:, 599:1199]\n",
    "indices = torch.nonzero(attn_output, as_tuple=True)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算degree矩阵，变换adjacent matrix形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([600, 4200])\n",
      "(tensor([  0,   0,   0,  ..., 599, 599, 599], device='cuda:0'), tensor([   3,    6,    8,  ..., 4193, 4195, 4196], device='cuda:0'))\n",
      "torch.Size([4200, 21])\n"
     ]
    }
   ],
   "source": [
    "def trans(A, graph):\n",
    "    \n",
    "    Degree_inv_sqrt = torch.diag(torch.pow(torch.sum(A, dim=1), -0.5))\n",
    "    A_norm = torch.mm(torch.mm(Degree_inv_sqrt, A), Degree_inv_sqrt)\n",
    "    \n",
    "    n_rel = graph.num_relation\n",
    "    n = A_norm.size(0)\n",
    "    n_rel = n_rel.item()  # 将 n_rel 从 Tensor 转换为 int\n",
    "    assert n % n_rel == 0, \"n must be divisible by n_rel\"\n",
    "    \n",
    "    block_size = n // n_rel\n",
    "    \n",
    "    # 初始化一个张量来存储累加结果\n",
    "    accumulated = torch.zeros_like(A_norm[:block_size])\n",
    "    \n",
    "    # 将后面的所有块累加到第一块\n",
    "    for i in range(n_rel):\n",
    "        accumulated += A_norm[i * block_size: (i + 1) * block_size]\n",
    "    \n",
    "    # 用累加后的第一块替换原始矩阵的第一块\n",
    "    A_trans = accumulated\n",
    "    \n",
    "    \n",
    "    \n",
    "    return A_trans\n",
    "\n",
    "A_norm = trans(attn_output, graph)\n",
    "print(A_norm)\n",
    "print(A_norm.shape)\n",
    "\n",
    "indices = torch.nonzero(A_norm, as_tuple=True)\n",
    "print(indices)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "update = torch.mm(A_norm.t(), graph.node_feature.to(device).to(torch.float))\n",
    "print(update.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewired_gearnet 用于diffusion模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewireGearnet(nn.Module):\n",
    "    gradient_checkpoint = False\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(RewireGearnet, self).__init__()\n",
    "        self.num_relation = num_relation\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.linear = nn.Linear(num_relation * input_dim, output_dim)\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(output_dim) if batch_norm else None\n",
    "        self.activation = getattr(F, activation) if activation else None\n",
    "        self.edge_linear = nn.Linear(edge_input_dim, output_dim) if edge_input_dim else None\n",
    "\n",
    "    def trans(self, A, graph):\n",
    "        n_rel = graph.num_relation\n",
    "        n = A.size(0)\n",
    "        n_rel = n_rel.item()  # 将 n_rel 从 Tensor 转换为 int\n",
    "        assert n % n_rel == 0, \"n must be divisible by n_rel\"\n",
    "        \n",
    "        block_size = n // n_rel\n",
    "        \n",
    "        # 初始化一个张量来存储累加结果\n",
    "        accumulated = torch.zeros_like(A[:block_size])\n",
    "        \n",
    "        # 将后面的所有块累加到第一块\n",
    "        for i in range(n_rel):\n",
    "            accumulated += A[i * block_size: (i + 1) * block_size]\n",
    "        \n",
    "        # 用累加后的第一块替换原始矩阵的第一块\n",
    "        A_trans = accumulated\n",
    "    \n",
    "        return A_trans\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list=None):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "\n",
    "        device = input.device  # Ensure device consistency\n",
    "\n",
    "        if new_edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "            node_out = node_out * self.num_relation + relation\n",
    "            adjacency = torch.sparse_coo_tensor(\n",
    "                torch.stack([node_in, node_out]),\n",
    "                graph.edge_weight.to(device),\n",
    "                (graph.num_node, graph.num_node * graph.num_relation),\n",
    "                device=device\n",
    "            )\n",
    "            update = torch.sparse.mm(adjacency.t(), input)\n",
    "        else:\n",
    "            adjacency = self.trans(new_edge_list, graph).to(device)\n",
    "            update = torch.mm(adjacency.t(), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = graph.edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(\n",
    "                edge_input * edge_weight, node_out, dim=0,\n",
    "                dim_size=graph.num_node * graph.num_relation\n",
    "            )\n",
    "            update += edge_update\n",
    "            \n",
    "        return update.view(graph.num_node, self.num_relation * self.input_dim).to(device)\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm.to(device)\n",
    "        \n",
    "        output = self.linear(update) + self.self_loop(input)\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, graph, input, new_edge_list=None, new_edge_weight=None):\n",
    "        \"\"\"\n",
    "        Perform message passing over the graph(s).\n",
    "\n",
    "        Parameters:\n",
    "            graph (Graph): graph(s)\n",
    "            input (Tensor): node representations of shape :math:`(|V|, ...)`\n",
    "        \"\"\"\n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self.message_and_aggregate, graph, input)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph, input, new_edge_list)\n",
    "        output = self.combine(input, update)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 512])\n",
      "tensor([[0.0000, 0.4906, 0.2537,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.4202, 0.9803, 0.0000,  ..., 0.4804, 0.3370, 0.0000],\n",
      "        [0.2416, 0.7412, 0.0000,  ..., 0.8642, 0.6196, 0.0000],\n",
      "        ...,\n",
      "        [0.7878, 1.0133, 0.0000,  ..., 0.8846, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5266, 0.4165,  ..., 0.5259, 0.3598, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 512\n",
    "num_relations = graph.num_relation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "new_node_feature = RewireGearnet(input_dim, output_dim, num_relations)(graph, graph.node_feature.to(device).float(), attn_output).to(device)\n",
    "\n",
    "print(new_node_feature.shape)\n",
    "print(new_node_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMGearnet(nn.Module, core.Configurable):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims, num_relation, num_heads, window_size, k, edge_input_dim=None, num_angle_bin=None,\n",
    "                 short_cut=False, batch_norm=False, activation=\"relu\", concat_hidden=False, readout=\"sum\"):\n",
    "        super(DGMGearnet, self).__init__()\n",
    "\n",
    "        #if not isinstance(hidden_dims, Sequence):\n",
    "            #hidden_dims = [hidden_dims]\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = sum(hidden_dims) if concat_hidden else hidden_dims[-1]\n",
    "        self.dims = [input_dim] + list(hidden_dims)\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.k = k\n",
    "        self.edge_dims = [edge_input_dim] + self.dims[:-1]\n",
    "        self.num_relation = num_relation\n",
    "        self.num_angle_bin = num_angle_bin\n",
    "        self.short_cut = short_cut\n",
    "        self.concat_hidden = concat_hidden\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.score_layers = nn.ModuleList()\n",
    "        for i in range(len(self.dims) - 1):\n",
    "            \n",
    "            self.score_layers.append(relationalGraph(self.dims[i], self.dims[i + 1], num_relation, edge_input_dim, \n",
    "                                            batch_norm, activation))\n",
    "            \n",
    "            self.score_layers.append(Rewirescorelayer(self.dims[i + 1], self.dims[i + 1], self.num_heads, self.window_size, \n",
    "                                            self.k, temperature=0.5, dropout=0.1))\n",
    "            \n",
    "            self.layers.append(RewireGearnet(self.dims[i], self.dims[i + 1], num_relation,\n",
    "                                            edge_input_dim=None, batch_norm=False, activation=\"relu\"))\n",
    "        \n",
    "        if num_angle_bin:\n",
    "            self.spatial_line_graph = layers.SpatialLineGraph(num_angle_bin)\n",
    "            self.edge_layers = nn.ModuleList()\n",
    "            for i in range(len(self.edge_dims) - 1):\n",
    "                self.edge_layers.append(layers.GeometricRelationalGraphConv(\n",
    "                    self.edge_dims[i], self.edge_dims[i + 1], num_angle_bin, None, batch_norm, activation))\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norms = nn.ModuleList()\n",
    "            for i in range(len(self.dims) - 1):\n",
    "                self.batch_norms.append(nn.BatchNorm1d(self.dims[i + 1]))\n",
    "\n",
    "        if readout == \"sum\":\n",
    "            self.readout = layers.SumReadout()\n",
    "        elif readout == \"mean\":\n",
    "            self.readout = layers.MeanReadout()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown readout `%s`\" % readout)\n",
    "\n",
    "    def forward(self, graph, input, edge_list=None, all_loss=None, metric=None):\n",
    "        \"\"\"\n",
    "        Compute the node representations and the graph representation(s).\n",
    "\n",
    "        Parameters:\n",
    "            graph (Graph): :math:`n` graph(s)\n",
    "            input (Tensor): input node representations\n",
    "            all_loss (Tensor, optional): if specified, add loss to this tensor\n",
    "            metric (dict, optional): if specified, output metrics to this dict\n",
    "\n",
    "        Returns:\n",
    "            dict with ``node_feature`` and ``graph_feature`` fields:\n",
    "                node representations of shape :math:`(|V|, d)`, graph representations of shape :math:`(n, d)`\n",
    "        \"\"\"\n",
    "        hiddens = []\n",
    "        layer_input = input\n",
    "        if self.num_angle_bin:\n",
    "            line_graph = self.spatial_line_graph(graph)\n",
    "            edge_input = line_graph.node_feature.float()\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            \n",
    "            relational_output = self.score_layers[2*i](graph, layer_input, edge_list)\n",
    "            new_edge_list = self.score_layers[2*i+1](graph, relational_output)\n",
    "            \n",
    "            hidden = self.layers[i](graph, layer_input, new_edge_list)\n",
    "            \n",
    "            if self.short_cut and hidden.shape == layer_input.shape:\n",
    "                hidden = hidden + layer_input\n",
    "                \n",
    "            if self.num_angle_bin:\n",
    "                edge_hidden = self.edge_layers[i](line_graph, edge_input)\n",
    "                edge_weight = graph.edge_weight.unsqueeze(-1)\n",
    "                if new_edge_list is None:\n",
    "                    node_out = graph.edge_list[:, 1] * self.num_relation + graph.edge_list[:, 2]\n",
    "                else:\n",
    "                    node_out = new_edge_list[:, 1] * self.num_relation + new_edge_list[:, 2]\n",
    "                \n",
    "                    update = scatter_add(edge_hidden * edge_weight, node_out, dim=0,\n",
    "                                        dim_size=graph.num_node * self.num_relation)\n",
    "                update = update.view(graph.num_node, self.num_relation * edge_hidden.shape[1])\n",
    "                update = self.layers[i].linear(update)\n",
    "                update = self.layers[i].activation(update)\n",
    "                hidden = hidden + update\n",
    "                edge_input = edge_hidden\n",
    "            if self.batch_norm:\n",
    "                hidden = self.batch_norms[i](hidden)\n",
    "                \n",
    "            hiddens.append(hidden)\n",
    "            layer_input = hidden\n",
    "            edge_list = new_edge_list\n",
    "\n",
    "        if self.concat_hidden:\n",
    "            node_feature = torch.cat(hiddens, dim=-1)\n",
    "        else:\n",
    "            node_feature = hiddens[-1]\n",
    "        graph_feature = self.readout(graph, node_feature)\n",
    "        print(\"node_feature: \", node_feature.shape)\n",
    "\n",
    "        return {\n",
    "            \"graph_feature\": graph_feature,\n",
    "            \"node_feature\": node_feature\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_feature:  torch.Size([600, 512])\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "hidden_dims = [512, 512, 512, 512, 512]\n",
    "num_relations = graph.num_relation\n",
    "num_heads = 8\n",
    "window_size = 50\n",
    "k = 5\n",
    "\n",
    "\n",
    "output = DGMGearnet(input_dim, hidden_dims, num_relations, num_heads, window_size, k).to(device)(graph.to(device), graph.node_feature.to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10.6871,   0.0000,   0.0000,  ...,   0.0000,  30.8024,   0.0000],\n",
      "        [  0.0000,   0.0000,   9.9722,  ...,   6.0842,  27.4922,  23.0079],\n",
      "        [  0.0000,   0.0000,  37.9458,  ...,  40.1465, 118.7449,  22.7782],\n",
      "        ...,\n",
      "        [ 71.0768,   0.0000,   0.0000,  ...,  12.7928,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   8.7190,  ...,   5.3802,  54.0361,   9.6626],\n",
      "        [  0.0000,   0.0000,   2.7798,  ...,   1.5191,  11.1723,   0.0000]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "torch.Size([600, 512])\n",
      "\n",
      "\n",
      "tensor([[1667.0217,  445.2009,  418.8975,  ..., 1931.7217, 4053.8359,\n",
      "         1067.4402],\n",
      "        [4312.1523,  880.1165, 1152.3745,  ..., 4668.5718, 9066.7549,\n",
      "         2825.6218]], device='cuda:0', grad_fn=<ScatterAddBackward>)\n",
      "torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "print(output[\"node_feature\"])\n",
    "print(output[\"node_feature\"].shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(output[\"graph_feature\"])\n",
    "print(output[\"graph_feature\"].shape)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Rewirescorelayer2(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, window_size, k, temperature=0.5, dropout=0.1):\n",
    "        super(Rewirescorelayer2, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_features = out_features\n",
    "        self.window_size = window_size\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.k = k\n",
    "\n",
    "        self.query = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.key = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.value = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([out_features])).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    def get_start_end(self, current, graph):\n",
    "        segment = graph.num_nodes.repeat(graph.num_relation)\n",
    "        index = torch.zeros(segment.size(0) + 1, dtype=segment.dtype)\n",
    "\n",
    "        for i in range(1, len(index)):\n",
    "            index[i] = index[i - 1] + segment[i - 1]\n",
    "\n",
    "        for i in range(len(index) - 1):\n",
    "            if index[i] <= current < index[i + 1]:\n",
    "                return (index[i].item(), index[i + 1].item())\n",
    "            elif index[i] == current:\n",
    "                return (index[i].item(), index[i + 1].item())\n",
    "\n",
    "        if current == index[-1]:\n",
    "            return (index[-1].item(), index[-1].item())\n",
    "\n",
    "    def gumbel_softmax_top_k(self, logits, tau=1.0, hard=False):\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "        gumbels = (logits + gumbels) / tau\n",
    "\n",
    "        y_soft = F.softmax(gumbels, dim=-1)\n",
    "\n",
    "        if hard:\n",
    "            topk_indices = logits.topk(self.k, dim=-1)[1]\n",
    "            y_hard = torch.zeros_like(logits).scatter_(-1, topk_indices, 1.0)\n",
    "            y = (y_hard - y_soft).detach() + y_soft\n",
    "        else:\n",
    "            y = y_soft\n",
    "\n",
    "        return y\n",
    "\n",
    "    def forward(self, graph, node_features):\n",
    "        device = node_features.device\n",
    "        num_nodes = node_features.size(0)\n",
    "        half_window = self.window_size // 2\n",
    "\n",
    "        Q = self.query(node_features).view(num_nodes, self.num_heads, self.out_features)\n",
    "        K = self.key(node_features).view(num_nodes, self.num_heads, self.out_features)\n",
    "\n",
    "        output = torch.zeros(num_nodes, num_nodes, device=device)\n",
    "\n",
    "        start_end_indices = [self.get_start_end(i, graph) for i in range(num_nodes)]\n",
    "\n",
    "        # Precompute K_windows\n",
    "        K_windows_padded = torch.zeros((num_nodes, self.window_size, self.num_heads, self.out_features), device=device)\n",
    "        for i in range(num_nodes):\n",
    "            start_index, end_index = start_end_indices[i]\n",
    "            start = max(start_index, i - half_window)\n",
    "            end = min(end_index, i + half_window)\n",
    "            K_window = K[start:end]\n",
    "            K_windows_padded[i, :K_window.size(0)] = K_window\n",
    "        \n",
    "\n",
    "        scores = torch.einsum(\"nhd,nmhd->nhm\", Q, K_windows_padded) / self.scale\n",
    "        scores = scores / self.temperature\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1).mean(dim=1)  # [num_nodes, max_window_size, out_features]\n",
    "        print(attention_weights.shape)\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "            start_index, end_index = start_end_indices[i]\n",
    "            start = max(start_index, i - half_window)\n",
    "            end = min(end_index, i + half_window )\n",
    "            output[i, start:end] = attention_weights[i, :end-start]\n",
    "\n",
    "        edge_list = self.gumbel_softmax_top_k(output, self.temperature, self.k)\n",
    "\n",
    "        return edge_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 6.41 GiB (GPU 0; 47.54 GiB total capacity; 38.90 GiB already allocated; 926.31 MiB free; 45.11 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m relational_output \u001b[38;5;241m=\u001b[39m relational_output\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m module \u001b[38;5;241m=\u001b[39m Rewirescorelayer2(input_dim, output_dim, num_heads, window_size, k)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelational_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(attn_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(attn_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/gearnet/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "Cell \u001b[0;32mIn[20], line 64\u001b[0m, in \u001b[0;36mRewirescorelayer2.forward\u001b[0;34m(self, graph, node_features)\u001b[0m\n\u001b[1;32m     61\u001b[0m start_end_indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_start_end(i, graph) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_nodes)]\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Precompute K_windows\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m K_windows_padded \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_nodes):\n\u001b[1;32m     66\u001b[0m     start_index, end_index \u001b[38;5;241m=\u001b[39m start_end_indices[i]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 6.41 GiB (GPU 0; 47.54 GiB total capacity; 38.90 GiB already allocated; 926.31 MiB free; 45.11 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_dim = relational_output.shape[-1]\n",
    "output_dim = 1024\n",
    "num_heads = 8\n",
    "window_size = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 5\n",
    "\n",
    "\n",
    "relational_output = relational_output.to(device)\n",
    "module = Rewirescorelayer2(input_dim, output_dim, num_heads, window_size, k).to(device)\n",
    "attn_output = module(graph, relational_output)\n",
    "print(attn_output.shape)\n",
    "print(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gearnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
