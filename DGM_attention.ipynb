{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdrug import  layers, datasets,transforms,core\n",
    "from torchdrug.core import Registry as R\n",
    "from torchdrug.layers import geometry\n",
    "\n",
    "import torch\n",
    "from torchdrug import data\n",
    "\n",
    "from torch_scatter import scatter_add\n",
    "import torch.nn as nn\n",
    "from torchdrug import utils\n",
    "from torch.utils import checkpoint\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from einops import rearrange, repeat, pack, unpack\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    raise ValueError(\"CUDA device 0 is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:11:18   Extracting /home/xiaotong/scratch/protein-datasets/EnzymeCommission.zip to /home/xiaotong/scratch/protein-datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/xiaotong/scratch/protein-datasets/EnzymeCommission/enzyme_commission.pkl.gz: 100%|██████████| 18716/18716 [00:48<00:00, 383.40it/s]\n"
     ]
    }
   ],
   "source": [
    "EnzymeCommission = R.search(\"datasets.EnzymeCommission\")\n",
    "PV = R.search(\"transforms.ProteinView\")\n",
    "trans = PV(view = \"residue\")\n",
    "dataset = EnzymeCommission(\"~/scratch/protein-datasets/\", test_cutoff=0.95, \n",
    "                           atom_feature=\"full\", bond_feature=\"full\", verbose=1, transform = trans)\n",
    "\n",
    "# 只保留alpha碳的简化格式\n",
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()], \n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(k=10, min_distance=5),\n",
    "                                                                 geometry.SequentialEdge(max_distance=2)\n",
    "                                                                 ],\n",
    "                                                    edge_feature=\"gearnet\"\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "PackedProtein(batch_size=2, num_atoms=[185, 415], num_bonds=[3754, 8999], num_residues=[185, 415])\n"
     ]
    }
   ],
   "source": [
    "graphs = dataset[:2]\n",
    "graphs = [element[\"graph\"] for element in graphs]\n",
    "graphs = data.Protein.pack(graphs)\n",
    "print(\"\\n\\n\")\n",
    "graph = graph_construction_model(graphs)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([185, 415])\n",
      "2\n",
      "tensor([[ 95,  96,   5],\n",
      "        [109, 110,   5],\n",
      "        [108, 109,   5],\n",
      "        ...,\n",
      "        [438, 470,   0],\n",
      "        [489, 470,   0],\n",
      "        [493, 470,   0]])\n"
     ]
    }
   ],
   "source": [
    "print(graph.num_nodes)\n",
    "print(graph.batch_size)\n",
    "print(graph.edge_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关系卷积神经网络，获取多个不同的嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationalGraph(layers.MessagePassingBase):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(relationalGraph, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_relation = num_relation\n",
    "        self.edge_input_dim = edge_input_dim\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        if edge_input_dim:\n",
    "            self.edge_linear = nn.Linear(edge_input_dim, input_dim)\n",
    "        else:\n",
    "            self.edge_linear = None\n",
    "            \n",
    "    def trans(self, A, graph):\n",
    "    \n",
    "        Degree_inv_sqrt = torch.diag(torch.pow(torch.sum(A, dim=1), -0.5))\n",
    "        A_norm = torch.mm(torch.mm(Degree_inv_sqrt, A), Degree_inv_sqrt)\n",
    "        \n",
    "        n_rel = graph.num_relation\n",
    "        n = A_norm.size(0)\n",
    "        n_rel = n_rel.item()  # 将 n_rel 从 Tensor 转换为 int\n",
    "        assert n % n_rel == 0, \"n must be divisible by n_rel\"\n",
    "        \n",
    "        block_size = n // n_rel\n",
    "        \n",
    "        # 初始化一个张量来存储累加结果\n",
    "        accumulated = torch.zeros_like(A_norm[:block_size])\n",
    "        \n",
    "        # 将后面的所有块累加到第一块\n",
    "        for i in range(n_rel):\n",
    "            accumulated += A_norm[i * block_size: (i + 1) * block_size]\n",
    "        \n",
    "        # 用累加后的第一块替换原始矩阵的第一块\n",
    "        A_trans = accumulated\n",
    "    \n",
    "        return A_trans\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "        device = input.device  # Ensure device consistency\n",
    "        \n",
    "        if new_edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "            node_out = node_out * self.num_relation + relation\n",
    "        \n",
    "            degree_out = scatter_add(graph.edge_weight, node_out, dim_size=graph.num_node * graph.num_relation)\n",
    "            edge_weight = graph.edge_weight / degree_out[node_out]\n",
    "            adjacency = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), edge_weight,\n",
    "                                                (graph.num_node, graph.num_node * graph.num_relation))\n",
    "            update = torch.sparse.mm(adjacency.t().to(device), input.to(device))\n",
    "        else:\n",
    "            adjacency = self.trans(new_edge_list, graph).to(device)\n",
    "            update = torch.mm(adjacency.t().to(device), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(edge_input * edge_weight, node_out, dim=0,\n",
    "                                      dim_size=graph.num_node * graph.num_relation)\n",
    "            update += edge_update\n",
    "\n",
    "        return update\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        # 自环特征\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        input = input.repeat(self.num_relation, 1).to(device)\n",
    "        loop_update = self.self_loop(input).to(device)\n",
    "        \n",
    "        output = self.linear(update)+loop_update\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        device = input.device\n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self._message_and_aggregate, *graph.to_tensors(), input, new_edge_list)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph.to(device), input, new_edge_list)\n",
    "        output = self.combine(input, update).view(graph.num_relation, input.size(0), -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  torch.Size([7, 600, 128])\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 128\n",
    "num_relations = graph.num_relation\n",
    "\n",
    "input = graph.node_feature.float().to(device)\n",
    "model = relationalGraph(input_dim, output_dim, num_relations)\n",
    "relational_output = model(graph, input, new_edge_list = None)\n",
    "print(\"output: \", relational_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重连接模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### window self attention + gumble softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewirescorelayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, window_size, k, temperature=0.5, dropout=0.1):\n",
    "        super(Rewirescorelayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_features = out_features\n",
    "        self.window_size = window_size\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.k = k\n",
    "        \n",
    "        self.query = nn.Linear(in_features, out_features * num_heads).to(device='cuda:0')\n",
    "        self.key = nn.Linear(in_features, out_features * num_heads).to(device = 'cuda:0')\n",
    "        self.scale = 1 / (out_features ** 0.5)\n",
    "\n",
    "    def split_windows(self, tensor, index, window_size, device):\n",
    "        result = []\n",
    "        index_list = []\n",
    "        start = 0\n",
    "\n",
    "        for idx in index:\n",
    "            end = start + idx - 1\n",
    "            while start <= end:\n",
    "                if start + window_size <= end:\n",
    "                    result.append(tensor[:, start:start + window_size, :])\n",
    "                    index_list.append([start, start + window_size])\n",
    "                    start += window_size\n",
    "                else:\n",
    "                    padding_rows = window_size - (end - start + 1)\n",
    "                    restart = start - padding_rows\n",
    "                    result.append(tensor[:, restart:restart + window_size, :])\n",
    "                    index_list.append([restart, restart + window_size])\n",
    "\n",
    "                    start = end+1\n",
    "        \n",
    "        result_tensor = torch.stack(result, dim=1).to(device)\n",
    "        return result_tensor, index_list\n",
    "\n",
    "    def gumbel_softmax_top_k(self, logits, tau=1.0, hard=False):\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "        gumbels = (logits + gumbels) / tau\n",
    "\n",
    "        y_soft = F.softmax(gumbels, dim=-1)\n",
    "\n",
    "        if hard:\n",
    "            topk_indices = logits.topk(self.k, dim=-1)[1]\n",
    "            y_hard = torch.zeros_like(logits).scatter_(-1, topk_indices, 1.0)\n",
    "            y = (y_hard - y_soft).detach() + y_soft\n",
    "        else:\n",
    "            y = y_soft\n",
    "\n",
    "        return y\n",
    "\n",
    "    def windows2adjacent(self, windows, index_list, output, device):\n",
    "        # 确保所有张量在相同设备上\n",
    "        output = output\n",
    "        windows = windows\n",
    "\n",
    "        # 创建一个新的张量来存储更新后的输出\n",
    "        new_output = torch.zeros_like(output)\n",
    "\n",
    "        # 填充新输出张量\n",
    "        for i, index in enumerate(index_list):\n",
    "            start, end = index\n",
    "            new_output[:, start:end, start:end] = torch.clamp(new_output[:, start:end, start:end] + windows[:, i, :, :], 0, 1)\n",
    "\n",
    "        # 获取输出张量的形状\n",
    "        num_relations, num_nodes, _ = new_output.shape\n",
    "        \n",
    "        # 创建一个新的结果张量\n",
    "        result = torch.zeros(num_relations * num_nodes, num_relations * num_nodes, device=device)\n",
    "\n",
    "        # 填充结果张量\n",
    "        for i in range(num_relations):\n",
    "            result[i * num_nodes:(i + 1) * num_nodes, i * num_nodes:(i + 1) * num_nodes] = result[i * num_nodes:(i + 1) * num_nodes, i * num_nodes:(i + 1) * num_nodes] + new_output[i]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def forward(self, graph, node_features):\n",
    "        start0 = time.time()\n",
    "        device = node_features.device\n",
    "        num_relations = node_features.size(0)\n",
    "        num_nodes = node_features.size(1)\n",
    "        index = graph.num_nodes.tolist()\n",
    "        end0 = time.time()\n",
    "        \n",
    "        start1 = time.time()\n",
    "        Q = self.query(node_features).view(num_relations, num_nodes, self.num_heads, self.out_features).permute(0, 2, 1, 3)\n",
    "        K = self.key(node_features).view(num_relations, num_nodes, self.num_heads, self.out_features).permute(0, 2, 1, 3)\n",
    "        Q = Q.reshape(num_relations * self.num_heads, num_nodes, self.out_features).to(device)                                  # (num_relations*num_heads, num_nodes, out_features)\n",
    "        K = K.reshape(num_relations * self.num_heads, num_nodes, self.out_features).to(device)                                  # (num_relations*num_heads, num_nodes, out_features)\n",
    "        end1 = time.time()\n",
    "        \n",
    "        output = torch.zeros(num_relations, num_nodes, num_nodes).to(device)\n",
    "        result = torch.zeros(num_relations * num_nodes, num_relations * num_nodes).to(device)\n",
    "\n",
    "\n",
    "        start2 = time.time()\n",
    "        Q_windows, Q_index = self.split_windows(Q, index, self.window_size, device)\n",
    "        K_windows, _ = self.split_windows(K, index, self.window_size, device)\n",
    "        end2 = time.time()  \n",
    "        \n",
    "\n",
    "        start3 = time.time()\n",
    "        scores = torch.einsum('b h i e, b h j e -> b h i j', Q_windows, K_windows) / self.scale                                 # (num_relations*num_heads, num_windows, window_size, window_size)\n",
    "        attn = scores.softmax(dim=-1).view(num_relations, self.num_heads, -1, self.window_size, self.window_size).mean(dim=1)   # (num_relations, num_windows, window_size, window_size)\n",
    "        end3 = time.time()\n",
    "        \n",
    "\n",
    "        start4 = time.time()\n",
    "        attn = self.gumbel_softmax_top_k(attn, tau=self.temperature, hard=True)                                                 # (num_relations, num_windows, window_size, window_size)\n",
    "        end4 = time.time()\n",
    "        \n",
    "\n",
    "        start5 = time.time()\n",
    "        result = result + self.windows2adjacent(attn, Q_index, output, device)\n",
    "        end5 = time.time()\n",
    "        \n",
    "        print(f\"0运行时间: {end0 - start0:.6f} 秒\")\n",
    "        print(f\"1运行时间: {end1 - start1:.6f} 秒\")\n",
    "        print(f\"2运行时间: {end2 - start2:.6f} 秒\")\n",
    "        print(f\"3运行时间: {end3 - start3:.6f} 秒\")\n",
    "        print(f\"4运行时间: {end4 - start4:.6f} 秒\")\n",
    "        print(f\"5运行时间: {end5 - start5:.6f} 秒\")\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0运行时间: 0.000011 秒\n",
      "1运行时间: 0.000383 秒\n",
      "2运行时间: 0.001096 秒\n",
      "3运行时间: 0.000247 秒\n",
      "4运行时间: 0.000260 秒\n",
      "5运行时间: 0.002788 秒\n",
      "运行时间: 0.041339 秒\n",
      "torch.Size([4200, 4200])\n"
     ]
    }
   ],
   "source": [
    "input_dim = relational_output.shape[-1]\n",
    "output_dim = 256\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 3\n",
    "num_nodes = relational_output.size(0)\n",
    "\n",
    "\n",
    "relational_output = relational_output.to(device)\n",
    "module = Rewirescorelayer(input_dim, output_dim, num_heads, window_size, k)\n",
    "start = time.time()\n",
    "attn_output = module(graph, relational_output)\n",
    "end = time.time()\n",
    "print(f\"运行时间: {end - start:.6f} 秒\")\n",
    "print(attn_output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "(tensor([   0,    0,    0,  ..., 4199, 4199, 4199], device='cuda:0'), tensor([   6,    7,    8,  ..., 4190, 4194, 4197], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(attn_output)\n",
    "indices = torch.nonzero(attn_output, as_tuple=True)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试不同degree进行采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds:\n",
      " tensor([0.2000, 0.5000, 0.7000, 0.9000, 0.1000, 0.3000, 0.6000, 0.8000, 0.4000,\n",
      "        0.2000])\n",
      "Sampled Matrix:\n",
      " tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gumbel_softmax_sample(logits, tau):\n",
    "    # 从Gumbel(0, 1)分布中采样\n",
    "    gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits)))\n",
    "    y = logits + gumbel_noise\n",
    "    return F.softmax(y / tau, dim=-1)\n",
    "\n",
    "def bernoulli_sampling_with_different_thresholds(probs, thresholds, tau=1.0):\n",
    "    # 对数概率\n",
    "    logits = torch.log(probs) - torch.log(1 - probs)\n",
    "    # 进行Gumbel-Softmax采样\n",
    "    y = gumbel_softmax_sample(logits, tau)\n",
    "    # 硬化处理，根据每行的不同阈值\n",
    "    z = (y > thresholds.unsqueeze(1)).float()\n",
    "    return z\n",
    "\n",
    "# 示例矩阵\n",
    "n = 10\n",
    "P = torch.rand(n, n)\n",
    "\n",
    "# 为每一行设置不同的阈值\n",
    "thresholds = torch.tensor([0.2, 0.5, 0.7, 0.9, 0.1, 0.3, 0.6, 0.8, 0.4, 0.2])\n",
    "\n",
    "# 进行可微分伯努利采样并硬化处理\n",
    "tau = 0.1  # 温度参数\n",
    "sampled_matrix = bernoulli_sampling_with_different_thresholds(P, thresholds, tau)\n",
    "\n",
    "#print(\"Probability Matrix:\\n\", P)\n",
    "print(\"Thresholds:\\n\", thresholds)\n",
    "print(\"Sampled Matrix:\\n\", sampled_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewired_gearnet 用于diffusion模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewireGearnet(nn.Module):\n",
    "    gradient_checkpoint = False\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(RewireGearnet, self).__init__()\n",
    "        self.num_relation = num_relation\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.linear = nn.Linear(num_relation * input_dim, output_dim)\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(output_dim) if batch_norm else None\n",
    "        self.activation = getattr(F, activation) if activation else None\n",
    "        self.edge_linear = nn.Linear(edge_input_dim, output_dim) if edge_input_dim else None\n",
    "\n",
    "    def trans(self, A, graph):\n",
    "        n_rel = graph.num_relation\n",
    "        n = A.size(0)\n",
    "        n_rel = n_rel.item()  # 将 n_rel 从 Tensor 转换为 int\n",
    "        assert n % n_rel == 0, \"n must be divisible by n_rel\"\n",
    "        \n",
    "        block_size = n // n_rel\n",
    "        \n",
    "        # 初始化一个张量来存储累加结果\n",
    "        accumulated = torch.zeros_like(A[:block_size])\n",
    "        \n",
    "        # 将后面的所有块累加到第一块\n",
    "        for i in range(n_rel):\n",
    "            accumulated += A[i * block_size: (i + 1) * block_size]\n",
    "        \n",
    "        # 用累加后的第一块替换原始矩阵的第一块\n",
    "        A_trans = accumulated\n",
    "    \n",
    "        return A_trans\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "\n",
    "        device = input.device  # Ensure device consistency\n",
    "\n",
    "        if new_edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "            node_out = node_out * self.num_relation + relation\n",
    "            adjacency = torch.sparse_coo_tensor(\n",
    "                torch.stack([node_in, node_out]),\n",
    "                graph.edge_weight.to(device),\n",
    "                (graph.num_node, graph.num_node * graph.num_relation),\n",
    "                device=device\n",
    "            )\n",
    "            update = torch.sparse.mm(adjacency.t(), input)\n",
    "        else:\n",
    "            adjacency = self.trans(new_edge_list, graph).to(device)\n",
    "            update = torch.mm(adjacency.t(), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = graph.edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(\n",
    "                edge_input * edge_weight, node_out, dim=0,\n",
    "                dim_size=graph.num_node * graph.num_relation\n",
    "            )\n",
    "            update += edge_update\n",
    "            \n",
    "        return update.view(graph.num_node, self.num_relation * self.input_dim).to(device)\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm.to(device)\n",
    "        \n",
    "        output = self.linear(update) + self.self_loop(input)\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        \"\"\"\n",
    "        Perform message passing over the graph(s).\n",
    "\n",
    "        Parameters:\n",
    "            graph (Graph): graph(s)\n",
    "            input (Tensor): node representations of shape :math:`(|V|, ...)`\n",
    "        \"\"\"\n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self.message_and_aggregate, graph, input)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph, input, new_edge_list)\n",
    "        output = self.combine(input, update)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 512])\n",
      "tensor([[0.0000, 0.1818, 0.0000,  ..., 0.0000, 0.0000, 0.0667],\n",
      "        [0.0928, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3510, 0.0000,  ..., 0.0446, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.3532, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1874, 0.0000,  ..., 0.0000, 0.0000, 0.1011]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 512\n",
    "num_relations = graph.num_relation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "new_node_feature = RewireGearnet(input_dim, output_dim, num_relations)(graph, graph.node_feature.to(device).float(), attn_output).to(device)\n",
    "\n",
    "print(new_node_feature.shape)\n",
    "print(new_node_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个装饰器来计时\n",
    "def time_layer(layer, layer_name):\n",
    "    def timed_layer(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        output = layer(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f'{layer_name}: {end_time - start_time:.6f} seconds')\n",
    "        return output\n",
    "    return timed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMGearnet(nn.Module, core.Configurable):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims, score_dim, num_relation, num_heads, window_size, k, edge_input_dim=None, num_angle_bin=None,\n",
    "                 short_cut=False, batch_norm=False, activation=\"relu\", concat_hidden=True, readout=\"sum\"):\n",
    "        super(DGMGearnet, self).__init__()\n",
    "\n",
    "        #if not isinstance(hidden_dims, Sequence):\n",
    "            #hidden_dims = [hidden_dims]\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = sum(hidden_dims) if concat_hidden else hidden_dims[-1]\n",
    "        self.dims = [input_dim] + list(hidden_dims)\n",
    "        self.score_dim = score_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.k = k\n",
    "        self.edge_dims = [edge_input_dim] + self.dims[:-1]\n",
    "        self.num_relation = num_relation\n",
    "        self.num_angle_bin = num_angle_bin\n",
    "        self.short_cut = short_cut\n",
    "        self.concat_hidden = concat_hidden\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.score_layers = nn.ModuleList()\n",
    "        for i in range(len(self.dims) - 1):\n",
    "            \n",
    "            self.score_layers.append(relationalGraph(self.dims[i], self.score_dim, num_relation, \n",
    "                                                     edge_input_dim=None, batch_norm=False, activation=\"relu\")) \n",
    "\n",
    "            \n",
    "            self.score_layers.append(Rewirescorelayer(self.score_dim, self.dims[i+1], self.num_heads, self.window_size, \n",
    "                                            self.k, temperature=0.5, dropout=0.1))\n",
    "            \n",
    "            self.layers.append(RewireGearnet(self.dims[i], self.dims[i + 1], num_relation,\n",
    "                                            edge_input_dim=None, batch_norm=False, activation=\"relu\"))\n",
    "        \n",
    "        if num_angle_bin:\n",
    "            self.spatial_line_graph = layers.SpatialLineGraph(num_angle_bin)\n",
    "            self.edge_layers = nn.ModuleList()\n",
    "            for i in range(len(self.edge_dims) - 1):\n",
    "                self.edge_layers.append(layers.GeometricRelationalGraphConv(\n",
    "                    self.edge_dims[i], self.edge_dims[i + 1], num_angle_bin, None, batch_norm, activation))\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norms = nn.ModuleList()\n",
    "            for i in range(len(self.dims) - 1):\n",
    "                self.batch_norms.append(nn.BatchNorm1d(self.dims[i + 1]))\n",
    "\n",
    "        if readout == \"sum\":\n",
    "            self.readout = layers.SumReadout()\n",
    "        elif readout == \"mean\":\n",
    "            self.readout = layers.MeanReadout()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown readout `%s`\" % readout)\n",
    "\n",
    "    def forward(self, graph, input, edge_list=None, all_loss=None, metric=None):\n",
    "        \"\"\"\n",
    "        Compute the node representations and the graph representation(s).\n",
    "\n",
    "        Parameters:\n",
    "            graph (Graph): :math:`n` graph(s)\n",
    "            input (Tensor): input node representations\n",
    "            all_loss (Tensor, optional): if specified, add loss to this tensor\n",
    "            metric (dict, optional): if specified, output metrics to this dict\n",
    "\n",
    "        Returns:\n",
    "            dict with ``node_feature`` and ``graph_feature`` fields:\n",
    "                node representations of shape :math:`(|V|, d)`, graph representations of shape :math:`(n, d)`\n",
    "        \"\"\"\n",
    "        hiddens = []\n",
    "        layer_input = input\n",
    "        if self.num_angle_bin:\n",
    "            line_graph = self.spatial_line_graph(graph)\n",
    "            edge_input = line_graph.node_feature.float()\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            \n",
    "            relational_output = time_layer(self.score_layers[2*i], 'relational output')(graph, layer_input, edge_list)\n",
    "            new_edge_list = time_layer(self.score_layers[2*i+1], 'new edge list')(graph, relational_output)\n",
    "            \n",
    "            hidden = time_layer(self.layers[i], 'hidden')(graph, layer_input, new_edge_list)\n",
    "            \n",
    "            if self.short_cut and hidden.shape == layer_input.shape:\n",
    "                hidden = hidden + layer_input\n",
    "            \n",
    "            if self.num_angle_bin:\n",
    "                edge_hidden = self.edge_layers[i](line_graph, edge_input)\n",
    "                edge_weight = graph.edge_weight.unsqueeze(-1)\n",
    "                if new_edge_list is None:\n",
    "                    node_out = graph.edge_list[:, 1] * self.num_relation + graph.edge_list[:, 2]\n",
    "                else:\n",
    "                    node_out = new_edge_list[:, 1] * self.num_relation + new_edge_list[:, 2]\n",
    "                \n",
    "                    update = scatter_add(edge_hidden * edge_weight, node_out, dim=0,\n",
    "                                        dim_size=graph.num_node * self.num_relation)\n",
    "                update = update.view(graph.num_node, self.num_relation * edge_hidden.shape[1])\n",
    "                update = self.layers[i].linear(update)\n",
    "                update = self.layers[i].activation(update)\n",
    "                hidden = hidden + update\n",
    "                edge_input = edge_hidden\n",
    "                \n",
    "            if self.batch_norm:\n",
    "                hidden = self.batch_norms[i](hidden)\n",
    "                \n",
    "            hiddens.append(hidden)\n",
    "            layer_input = hidden\n",
    "            edge_list = new_edge_list\n",
    "\n",
    "        if self.concat_hidden:\n",
    "            node_feature = torch.cat(hiddens, dim=-1)\n",
    "        else:\n",
    "            node_feature = hiddens[-1]\n",
    "        graph_feature = self.readout(graph, node_feature)\n",
    "\n",
    "        return {\n",
    "            \"graph_feature\": graph_feature,\n",
    "            \"node_feature\": node_feature\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relational output: 0.001348 seconds\n",
      "0运行时间: 0.000386 秒\n",
      "1运行时间: 0.000282 秒\n",
      "2运行时间: 0.001381 秒\n",
      "3运行时间: 0.000250 秒\n",
      "4运行时间: 0.000388 秒\n",
      "5运行时间: 0.003135 秒\n",
      "new edge list: 0.043073 seconds\n",
      "hidden: 0.000908 seconds\n",
      "relational output: 0.005213 seconds\n",
      "0运行时间: 0.000031 秒\n",
      "1运行时间: 0.000582 秒\n",
      "2运行时间: 0.001395 秒\n",
      "3运行时间: 0.000245 秒\n",
      "4运行时间: 0.000242 秒\n",
      "5运行时间: 0.003031 秒\n",
      "new edge list: 0.039228 seconds\n",
      "hidden: 0.000983 seconds\n",
      "relational output: 0.005140 seconds\n",
      "0运行时间: 0.000031 秒\n",
      "1运行时间: 0.000505 秒\n",
      "2运行时间: 0.001354 秒\n",
      "3运行时间: 0.000238 秒\n",
      "4运行时间: 0.000250 秒\n",
      "5运行时间: 0.003097 秒\n",
      "new edge list: 0.039709 seconds\n",
      "hidden: 0.000989 seconds\n",
      "relational output: 0.005167 seconds\n",
      "0运行时间: 0.000030 秒\n",
      "1运行时间: 0.000513 秒\n",
      "2运行时间: 0.001430 秒\n",
      "3运行时间: 0.000240 秒\n",
      "4运行时间: 0.000249 秒\n",
      "5运行时间: 0.003175 秒\n",
      "new edge list: 0.040856 seconds\n",
      "hidden: 0.000975 seconds\n",
      "relational output: 0.005148 seconds\n",
      "0运行时间: 0.000031 秒\n",
      "1运行时间: 0.000372 秒\n",
      "2运行时间: 0.001374 秒\n",
      "3运行时间: 0.000237 秒\n",
      "4运行时间: 0.000405 秒\n",
      "5运行时间: 0.003431 秒\n",
      "new edge list: 0.040803 seconds\n",
      "hidden: 0.001251 seconds\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "hidden_dims = [512, 512, 512, 512, 512]\n",
    "score_dim = 128\n",
    "num_relations = graph.num_relation\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "k = 5\n",
    "\n",
    "\n",
    "output = DGMGearnet(input_dim, hidden_dims, score_dim, num_relations, num_heads, window_size, k).to(device)(graph.to(device), graph.node_feature.to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2553, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 9.9050, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2580,  ..., 0.0000, 0.0000, 1.0522],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.1655, 0.0000, 0.0000,  ..., 8.7619, 0.0000, 0.0000]],\n",
      "       device='cuda:1', grad_fn=<CatBackward>)\n",
      "torch.Size([600, 2560])\n",
      "\n",
      "\n",
      "tensor([[3.1324e+01, 2.3192e+00, 7.2755e+00,  ..., 4.6248e+02, 5.4866e-02,\n",
      "         1.8774e+00],\n",
      "        [8.6681e+01, 2.8787e+00, 1.5933e+01,  ..., 9.9931e+02, 0.0000e+00,\n",
      "         0.0000e+00]], device='cuda:1', grad_fn=<ScatterAddBackward>)\n",
      "torch.Size([2, 2560])\n"
     ]
    }
   ],
   "source": [
    "print(output[\"node_feature\"])\n",
    "print(output[\"node_feature\"].shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(output[\"graph_feature\"])\n",
    "print(output[\"graph_feature\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 10], [10, 20], [20, 30], [30, 40], [40, 50], [50, 60], [60, 70], [70, 80], [80, 90], [90, 100], [100, 110], [110, 120], [120, 130], [130, 140], [140, 150], [150, 160], [160, 170], [170, 180], [180, 190], [190, 200], [200, 210], [210, 220], [220, 230], [230, 240], [240, 250], [250, 260], [254, 264], [264, 274], [274, 284], [284, 294], [294, 304], [304, 314], [314, 324], [324, 334], [334, 344], [344, 354], [354, 364], [364, 374], [374, 384], [384, 394], [394, 404], [404, 414], [414, 424], [424, 434], [434, 444], [444, 454], [454, 464], [464, 474], [474, 484], [484, 494], [494, 504], [504, 514], [514, 524], [524, 534], [534, 544], [544, 554], [554, 564], [562, 572]]\n",
      "[[0, 10], [10, 20], [20, 30], [30, 40], [40, 50], [50, 60], [60, 70], [70, 80], [80, 90], [90, 100], [100, 110], [110, 120], [120, 130], [130, 140], [140, 150], [150, 160], [160, 170], [170, 180], [180, 190], [190, 200], [200, 210], [210, 220], [220, 230], [230, 240], [240, 250], [250, 260], [254, 264], [264, 274], [274, 284], [284, 294], [294, 304], [304, 314], [314, 324], [324, 334], [334, 344], [344, 354], [354, 364], [364, 374], [374, 384], [384, 394], [394, 404], [404, 414], [414, 424], [424, 434], [434, 444], [444, 454], [454, 464], [464, 474], [474, 484], [484, 494], [494, 504], [504, 514], [514, 524], [524, 534], [534, 544], [544, 554], [554, 564], [562, 572]]\n",
      "[[0, 10], [10, 20], [20, 30], [30, 40], [40, 50], [50, 60], [60, 70], [70, 80], [80, 90], [90, 100], [100, 110], [110, 120], [120, 130], [130, 140], [140, 150], [150, 160], [160, 170], [170, 180], [180, 190], [190, 200], [200, 210], [210, 220], [220, 230], [230, 240], [240, 250], [250, 260], [254, 264], [264, 274], [274, 284], [284, 294], [294, 304], [304, 314], [314, 324], [324, 334], [334, 344], [344, 354], [354, 364], [364, 374], [374, 384], [384, 394], [394, 404], [404, 414], [414, 424], [424, 434], [434, 444], [444, 454], [454, 464], [464, 474], [474, 484], [484, 494], [494, 504], [504, 514], [514, 524], [524, 534], [534, 544], [544, 554], [554, 564], [562, 572]]\n",
      "[[0, 10], [10, 20], [20, 30], [30, 40], [40, 50], [50, 60], [60, 70], [70, 80], [80, 90], [90, 100], [100, 110], [110, 120], [120, 130], [130, 140], [140, 150], [150, 160], [160, 170], [170, 180], [180, 190], [190, 200], [200, 210], [210, 220], [220, 230], [230, 240], [240, 250], [250, 260], [254, 264], [264, 274], [274, 284], [284, 294], [294, 304], [304, 314], [314, 324], [324, 334], [334, 344], [344, 354], [354, 364], [364, 374], [374, 384], [384, 394], [394, 404], [404, 414], [414, 424], [424, 434], [434, 444], [444, 454], [454, 464], [464, 474], [474, 484], [484, 494], [494, 504], [504, 514], [514, 524], [524, 534], [534, 544], [544, 554], [554, 564], [562, 572]]\n",
      "[[0, 10], [10, 20], [20, 30], [30, 40], [40, 50], [50, 60], [60, 70], [70, 80], [80, 90], [90, 100], [100, 110], [110, 120], [120, 130], [130, 140], [140, 150], [150, 160], [160, 170], [170, 180], [180, 190], [190, 200], [200, 210], [210, 220], [220, 230], [230, 240], [240, 250], [250, 260], [254, 264], [264, 274], [274, 284], [284, 294], [294, 304], [304, 314], [314, 324], [324, 334], [334, 344], [344, 354], [354, 364], [364, 374], [374, 384], [384, 394], [394, 404], [404, 414], [414, 424], [424, 434], [434, 444], [444, 454], [454, 464], [464, 474], [474, 484], [484, 494], [494, 504], [504, 514], [514, 524], [524, 534], [534, 544], [544, 554], [554, 564], [562, 572]]\n",
      "tensor(159.2843, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Layer layers.0.linear.weight - Gradient_norm: 142.38612365722656\n",
      "Layer layers.0.linear.bias - Gradient_norm: 42.30305862426758\n",
      "Layer layers.0.self_loop.weight - Gradient_norm: 10.722688674926758\n",
      "Layer layers.0.self_loop.bias - Gradient_norm: 42.30305862426758\n",
      "Layer layers.1.linear.weight - Gradient_norm: 1085.5723876953125\n",
      "Layer layers.1.linear.bias - Gradient_norm: 19.160188674926758\n",
      "Layer layers.1.self_loop.weight - Gradient_norm: 80.69200134277344\n",
      "Layer layers.1.self_loop.bias - Gradient_norm: 19.160188674926758\n",
      "Layer layers.2.linear.weight - Gradient_norm: 1028.5391845703125\n",
      "Layer layers.2.linear.bias - Gradient_norm: 7.7760796546936035\n",
      "Layer layers.2.self_loop.weight - Gradient_norm: 76.77202606201172\n",
      "Layer layers.2.self_loop.bias - Gradient_norm: 7.7760796546936035\n",
      "Layer layers.3.linear.weight - Gradient_norm: 810.7118530273438\n",
      "Layer layers.3.linear.bias - Gradient_norm: 2.6841704845428467\n",
      "Layer layers.3.self_loop.weight - Gradient_norm: 60.10576629638672\n",
      "Layer layers.3.self_loop.bias - Gradient_norm: 2.6841704845428467\n",
      "Layer layers.4.linear.weight - Gradient_norm: 534.0957641601562\n",
      "Layer layers.4.linear.bias - Gradient_norm: 0.8030843734741211\n",
      "Layer layers.4.self_loop.weight - Gradient_norm: 39.5841178894043\n",
      "Layer layers.4.self_loop.bias - Gradient_norm: 0.8030843734741211\n",
      "Layer score_layers.0.self_loop.weight - Gradient_norm: 0.294857382774353\n",
      "Layer score_layers.0.self_loop.bias - Gradient_norm: 0.21014831960201263\n",
      "Layer score_layers.0.linear.weight - Gradient_norm: 0.21081219613552094\n",
      "Layer score_layers.0.linear.bias - Gradient_norm: 0.21014831960201263\n",
      "Layer score_layers.1.query.weight - Gradient_norm: 0.6939507126808167\n",
      "Layer score_layers.1.query.bias - Gradient_norm: 0.4370785057544708\n",
      "Layer score_layers.1.key.weight - Gradient_norm: 0.8381465673446655\n",
      "Layer score_layers.1.key.bias - Gradient_norm: 7.266617467394099e-05\n",
      "Layer score_layers.2.self_loop.weight - Gradient_norm: 1.1193978786468506\n",
      "Layer score_layers.2.self_loop.bias - Gradient_norm: 0.18650929629802704\n",
      "Layer score_layers.2.linear.weight - Gradient_norm: 1.2206106185913086\n",
      "Layer score_layers.2.linear.bias - Gradient_norm: 0.18650929629802704\n",
      "Layer score_layers.3.query.weight - Gradient_norm: 0.4565069079399109\n",
      "Layer score_layers.3.query.bias - Gradient_norm: 0.39186304807662964\n",
      "Layer score_layers.3.key.weight - Gradient_norm: 0.5839003324508667\n",
      "Layer score_layers.3.key.bias - Gradient_norm: 3.3122698368970305e-05\n",
      "Layer score_layers.4.self_loop.weight - Gradient_norm: 2.8158364295959473\n",
      "Layer score_layers.4.self_loop.bias - Gradient_norm: 0.22871729731559753\n",
      "Layer score_layers.4.linear.weight - Gradient_norm: 3.94661283493042\n",
      "Layer score_layers.4.linear.bias - Gradient_norm: 0.22871729731559753\n",
      "Layer score_layers.5.query.weight - Gradient_norm: 1.6127146482467651\n",
      "Layer score_layers.5.query.bias - Gradient_norm: 0.5434131026268005\n",
      "Layer score_layers.5.key.weight - Gradient_norm: 1.7648046016693115\n",
      "Layer score_layers.5.key.bias - Gradient_norm: 6.1237376939971e-05\n",
      "Layer score_layers.6.self_loop.weight - Gradient_norm: 3.2567684650421143\n",
      "Layer score_layers.6.self_loop.bias - Gradient_norm: 0.10878679156303406\n",
      "Layer score_layers.6.linear.weight - Gradient_norm: 3.897597074508667\n",
      "Layer score_layers.6.linear.bias - Gradient_norm: 0.10878679156303406\n",
      "Layer score_layers.7.query.weight - Gradient_norm: 1.78696870803833\n",
      "Layer score_layers.7.query.bias - Gradient_norm: 0.23747429251670837\n",
      "Layer score_layers.7.key.weight - Gradient_norm: 1.7926740646362305\n",
      "Layer score_layers.7.key.bias - Gradient_norm: 5.7579512940719724e-05\n",
      "Layer score_layers.8.self_loop.weight - Gradient_norm: 5.970613479614258\n",
      "Layer score_layers.8.self_loop.bias - Gradient_norm: 0.0917118638753891\n",
      "Layer score_layers.8.linear.weight - Gradient_norm: 5.376163482666016\n",
      "Layer score_layers.8.linear.bias - Gradient_norm: 0.0917118638753891\n",
      "Layer score_layers.9.query.weight - Gradient_norm: 2.5112555027008057\n",
      "Layer score_layers.9.query.bias - Gradient_norm: 0.17762146890163422\n",
      "Layer score_layers.9.key.weight - Gradient_norm: 2.477689266204834\n",
      "Layer score_layers.9.key.bias - Gradient_norm: 1.4803063095314428e-05\n"
     ]
    }
   ],
   "source": [
    "model = DGMGearnet(input_dim, hidden_dims, score_dim, num_relations, num_heads, window_size, k).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "output = model(graph.to(device), graph.node_feature.to(device).float())\n",
    "\n",
    "output = output[\"graph_feature\"]\n",
    "\n",
    "# 定义一个简单的损失函数，例如均方误差\n",
    "criterion = F.binary_cross_entropy_with_logits\n",
    "target = torch.rand_like(output)\n",
    "\n",
    "# 计算损失\n",
    "loss = criterion(output, target, reduction=\"mean\")\n",
    "print(loss)\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "\n",
    "# 检查梯度是否为 NaN 或 inf\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        if torch.isnan(param.grad).any():\n",
    "            print(f\"Gradient of {name} contains NaN values.\")\n",
    "        if torch.isinf(param.grad).any():\n",
    "            print(f\"Gradient of {name} contains inf values.\")\n",
    "        else:\n",
    "            print(f\"Layer {name} - Gradient_norm: {torch.norm(param.grad)}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"No gradient found for {name}\")\n",
    "\n",
    "# 检查损失是否为 NaN\n",
    "if torch.isnan(loss):\n",
    "    print(\"Loss is NaN.\")\n",
    "        \n",
    "# 更新参数\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewirescorelayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, window_size, k, temperature=0.5, dropout=0.1):\n",
    "        super(Rewirescorelayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_features = out_features\n",
    "        self.window_size = window_size\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.k = k\n",
    "        \n",
    "        self.query = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.key = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.scale = 1 / (out_features ** 0.5)\n",
    "\n",
    "    def split_windows(self, tensor, index, window_size, device):\n",
    "\n",
    "        for id in index:\n",
    "            end = id\n",
    "            window_num = end // window_size\n",
    "            first_part_result_tensor = tensor[:,0:window_num*window_size,:].view(-1, window_num, window_size, tensor.size(-1))\n",
    "            rest_part_result_tensor = tensor[:,window_num*window_size:end,:].view(-1, 1, end-window_num*window_size, tensor.size(-1))\n",
    "\n",
    "        \n",
    "        \n",
    "        result_tensor = torch.stack(result, dim=1).to(device)\n",
    "        return result_tensor, index_list\n",
    "\n",
    "    def gumbel_softmax_top_k(self, logits, tau=1.0, hard=False):\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "        gumbels = (logits + gumbels) / tau\n",
    "\n",
    "        y_soft = F.softmax(gumbels, dim=-1)\n",
    "\n",
    "        if hard:\n",
    "            topk_indices = logits.topk(self.k, dim=-1)[1]\n",
    "            y_hard = torch.zeros_like(logits).scatter_(-1, topk_indices, 1.0)\n",
    "            y = (y_hard - y_soft).detach() + y_soft\n",
    "        else:\n",
    "            y = y_soft\n",
    "\n",
    "        return y\n",
    "\n",
    "    def windows2adjacent(self, windows, index_list, output, device):\n",
    "        # 确保所有张量在相同设备上\n",
    "        output = output.to(device)\n",
    "        windows = windows.to(device)\n",
    "\n",
    "        # 创建一个新的张量来存储更新后的输出\n",
    "        new_output = torch.zeros_like(output, device=device)\n",
    "\n",
    "        # 填充新输出张量\n",
    "        for i, index in enumerate(index_list):\n",
    "            start, end = index\n",
    "            new_output[:, start:end, start:end] = torch.clamp(new_output[:, start:end, start:end] + windows[:, i, :, :], 0, 1)\n",
    "\n",
    "        # 获取输出张量的形状\n",
    "        num_relations, num_nodes, _ = new_output.shape\n",
    "        \n",
    "        # 创建一个新的结果张量\n",
    "        result = torch.zeros(num_relations * num_nodes, num_relations * num_nodes, device=device)\n",
    "\n",
    "        # 填充结果张量\n",
    "        for i in range(num_relations):\n",
    "            result[i * num_nodes:(i + 1) * num_nodes, i * num_nodes:(i + 1) * num_nodes] = result[i * num_nodes:(i + 1) * num_nodes, i * num_nodes:(i + 1) * num_nodes] + new_output[i]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def forward(self, graph, node_features):\n",
    "        torch.cuda.synchronize()  # 同步\n",
    "        start0 = time.time()\n",
    "        device = node_features.device\n",
    "        num_relations = node_features.size(0)\n",
    "        num_nodes = node_features.size(1)\n",
    "        index = graph.num_nodes.tolist()\n",
    "        end0 = time.time()\n",
    "        \n",
    "        torch.cuda.synchronize()  # 同步\n",
    "        start1 = time.time()\n",
    "        Q = self.query(node_features).view(num_relations, num_nodes, self.num_heads, self.out_features).permute(0, 2, 1, 3)\n",
    "        K = self.key(node_features).view(num_relations, num_nodes, self.num_heads, self.out_features).permute(0, 2, 1, 3)\n",
    "        Q = Q.reshape(num_relations * self.num_heads, num_nodes, self.out_features).to(device)                                  # (num_relations*num_heads, num_nodes, out_features)\n",
    "        K = K.reshape(num_relations * self.num_heads, num_nodes, self.out_features).to(device)                                  # (num_relations*num_heads, num_nodes, out_features)\n",
    "        end1 = time.time()\n",
    "        \n",
    "        output = torch.zeros(num_relations, num_nodes, num_nodes).to(device)\n",
    "        result = torch.zeros(num_relations * num_nodes, num_relations * num_nodes).to(device)\n",
    "\n",
    "        torch.cuda.synchronize()  # 同步\n",
    "        start2 = time.time()\n",
    "        Q_windows, Q_index = self.split_windows(Q, index, self.window_size, device)  # (num_relations*num_heads, num_windows, window_size, out_features)\n",
    "        K_windows, _ = self.split_windows(K, index, self.window_size, device)\n",
    "        end2 = time.time()  \n",
    "        \n",
    "        torch.cuda.synchronize()  # 同步\n",
    "        start3 = time.time()\n",
    "        scores = torch.einsum('b h i e, b h j e -> b h i j', Q_windows, K_windows) / self.scale                                 # (num_relations*num_heads, num_windows, window_size, window_size)\n",
    "        attn = scores.softmax(dim=-1).view(num_relations, self.num_heads, -1, self.window_size, self.window_size).mean(dim=1)   # (num_relations, num_windows, window_size, window_size)\n",
    "        end3 = time.time()\n",
    "        \n",
    "        torch.cuda.synchronize()  # 同步\n",
    "        start4 = time.time()\n",
    "        attn = self.gumbel_softmax_top_k(attn, tau=self.temperature, hard=True)                                                 # (num_relations, num_windows, window_size, window_size)\n",
    "        end4 = time.time()\n",
    "        \n",
    "        torch.cuda.synchronize()  # 同步\n",
    "        start5 = time.time()\n",
    "        result = result + self.windows2adjacent(attn, Q_index, output, device)\n",
    "        end5 = time.time()\n",
    "        \n",
    "        print(f\"0运行时间: {end0 - start0:.6f} 秒\")\n",
    "        print(f\"1运行时间: {end1 - start1:.6f} 秒\")\n",
    "        print(f\"2运行时间: {end2 - start2:.6f} 秒\")\n",
    "        print(f\"3运行时间: {end3 - start3:.6f} 秒\")\n",
    "        print(f\"4运行时间: {end4 - start4:.6f} 秒\")\n",
    "        print(f\"5运行时间: {end5 - start5:.6f} 秒\")\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0运行时间: 0.000013 秒\n",
      "1运行时间: 0.000718 秒\n",
      "2运行时间: 0.001931 秒\n",
      "3运行时间: 0.000279 秒\n",
      "4运行时间: 0.000292 秒\n",
      "5运行时间: 0.005413 秒\n",
      "运行时间: 0.149331 秒\n",
      "torch.Size([8155, 8155])\n"
     ]
    }
   ],
   "source": [
    "input_dim = relational_output.shape[-1]\n",
    "output_dim = 256\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 3\n",
    "num_nodes = relational_output.size(0)\n",
    "\n",
    "\n",
    "relational_output = relational_output.to(device)\n",
    "module = Rewirescorelayer(input_dim, output_dim, num_heads, window_size, k).to(device)\n",
    "start = time.time()\n",
    "attn_output = module(graph, relational_output)\n",
    "end = time.time()\n",
    "print(f\"运行时间: {end - start:.6f} 秒\")\n",
    "print(attn_output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间: 0.000435 秒\n",
      "Padded Tensor shape: torch.Size([7, 210, 128])\n",
      "Mask shape: torch.Size([7, 210, 128])\n",
      "Padded Tensor: tensor([[[ 1.9527e-01, -1.2454e+00,  4.3910e-02,  ..., -1.5814e+00,\n",
      "          -1.0835e+00,  1.9632e+00],\n",
      "         [ 1.3307e+00, -6.6778e-01, -1.4237e+00,  ...,  1.0095e+00,\n",
      "          -3.2155e-02, -6.0153e-01],\n",
      "         [-7.4786e-01, -8.6287e-01,  1.2246e+00,  ..., -9.1776e-01,\n",
      "           6.2641e-01,  4.8966e-01],\n",
      "         ...,\n",
      "         [-7.2450e-01,  9.3714e-01, -1.0866e+00,  ..., -1.0222e+00,\n",
      "          -1.4973e+00, -2.6646e-01],\n",
      "         [ 6.6116e-02,  2.7129e-01, -1.9317e-01,  ...,  2.4859e-01,\n",
      "           1.7955e+00,  1.0063e+00],\n",
      "         [ 8.8491e-01,  8.8131e-01,  3.0394e-02,  ..., -8.8912e-02,\n",
      "           1.5006e+00,  1.2274e-01]],\n",
      "\n",
      "        [[-7.8610e-03,  4.4378e-01,  8.7753e-01,  ...,  1.9064e+00,\n",
      "          -8.2079e-01, -4.9162e-01],\n",
      "         [-2.0021e+00,  7.0551e-01,  2.1431e-01,  ...,  7.3756e-01,\n",
      "           3.3861e-01, -2.6382e-01],\n",
      "         [ 4.1596e-01, -5.6321e-01,  2.9007e-01,  ..., -1.2198e+00,\n",
      "          -2.2544e+00, -7.6672e-01],\n",
      "         ...,\n",
      "         [ 2.7164e-01, -7.1270e-01,  1.4282e+00,  ...,  1.6110e+00,\n",
      "          -1.6421e-01, -3.8026e-01],\n",
      "         [ 1.1772e+00,  2.6637e+00, -3.2002e-01,  ...,  6.6351e-01,\n",
      "           2.8067e-01,  5.8968e-01],\n",
      "         [ 8.7073e-01,  1.6090e-01,  7.5828e-01,  ...,  9.9646e-02,\n",
      "           1.3462e-01,  7.6064e-01]],\n",
      "\n",
      "        [[ 1.4525e+00,  1.7749e+00,  2.4371e-01,  ..., -9.3632e-01,\n",
      "           7.2833e-01,  4.7157e-01],\n",
      "         [ 7.5755e-01, -3.1036e-01,  9.6631e-01,  ...,  5.1510e-01,\n",
      "           8.7038e-02,  2.1562e-01],\n",
      "         [ 9.9198e-01, -3.8258e-01, -5.2241e-01,  ..., -1.3973e+00,\n",
      "          -5.6530e-01, -9.7176e-01],\n",
      "         ...,\n",
      "         [ 1.1711e+00, -1.1070e+00, -1.0837e+00,  ..., -1.4583e+00,\n",
      "          -1.1344e+00, -1.8459e-01],\n",
      "         [ 6.3413e-02,  1.9188e+00,  5.1178e-01,  ..., -5.1717e-01,\n",
      "           4.7995e-01, -8.4711e-01],\n",
      "         [ 6.6488e-01,  1.3669e+00,  2.3859e-02,  ..., -1.8172e-01,\n",
      "          -4.4527e-01,  9.2108e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.1385e+00,  1.8667e-01,  2.2336e-01,  ...,  1.0390e+00,\n",
      "           4.8375e-01,  1.5801e+00],\n",
      "         [ 1.2744e+00,  3.3366e-01,  3.2414e-01,  ..., -2.4267e+00,\n",
      "           8.8009e-01,  1.2103e+00],\n",
      "         [ 2.2568e+00,  1.6351e-01, -6.8926e-01,  ..., -1.1016e+00,\n",
      "          -1.7000e-01, -8.5216e-01],\n",
      "         ...,\n",
      "         [ 6.0998e-01,  5.0848e-01, -4.1090e-01,  ..., -8.1317e-01,\n",
      "          -1.2535e+00,  9.0520e-01],\n",
      "         [ 4.8927e-02,  3.4281e-01,  6.1060e-01,  ..., -1.3106e+00,\n",
      "          -2.7368e-01,  1.1168e-01],\n",
      "         [-1.4261e+00, -6.0016e-01,  9.6911e-01,  ..., -3.6770e-01,\n",
      "           8.9653e-02, -4.4853e-01]],\n",
      "\n",
      "        [[ 4.9896e-02,  7.9708e-01,  7.1248e-01,  ..., -2.0392e-01,\n",
      "          -2.3845e-01,  5.7375e-01],\n",
      "         [ 7.5903e-01,  2.2423e+00,  1.3487e+00,  ..., -6.1335e-01,\n",
      "           6.9778e-01, -3.3608e-01],\n",
      "         [ 1.9254e-01, -7.4445e-01,  6.1280e-01,  ...,  1.1896e-01,\n",
      "          -3.9893e-01,  1.6729e-03],\n",
      "         ...,\n",
      "         [-1.6511e-01,  7.5311e-01, -1.2974e-01,  ...,  2.5560e-02,\n",
      "           2.7661e-01,  1.3881e+00],\n",
      "         [ 9.8530e-01, -1.1088e+00, -3.1579e-01,  ..., -6.2282e-01,\n",
      "          -1.0387e+00,  1.1502e+00],\n",
      "         [-9.1584e-01, -3.1754e-01, -6.5540e-01,  ...,  5.1451e-01,\n",
      "          -8.9953e-02,  7.8102e-01]],\n",
      "\n",
      "        [[ 8.0718e-01, -8.0181e-01,  4.0914e-01,  ..., -4.5453e-01,\n",
      "          -4.7838e-01,  8.4102e-01],\n",
      "         [-1.4277e+00, -1.8202e+00,  9.2124e-01,  ...,  6.7733e-01,\n",
      "          -6.7295e-01,  6.8938e-01],\n",
      "         [ 1.3735e+00,  5.6213e-01,  1.5147e+00,  ...,  1.5336e+00,\n",
      "          -1.2132e-01,  2.1928e-01],\n",
      "         ...,\n",
      "         [-1.9144e+00,  7.2661e-01, -3.4607e-01,  ...,  7.1729e-01,\n",
      "           1.5480e+00,  1.9560e+00],\n",
      "         [ 5.2917e-01,  8.0385e-01, -1.1639e+00,  ...,  2.9152e-01,\n",
      "           1.6082e+00, -4.1323e-01],\n",
      "         [-4.6582e-01,  1.3069e+00,  1.2836e+00,  ...,  1.2003e-01,\n",
      "          -3.2311e-01, -6.8139e-01]]], device='cuda:0')\n",
      "Mask: tensor([[[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]],\n",
      "\n",
      "        [[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def insert_zero_rows(tensor, lengths, target_lengths):\n",
    "    assert len(lengths) == len(target_lengths), \"Lengths and target lengths must be of the same length.\"\n",
    "    \n",
    "    # 计算每个位置需要插入的零行数\n",
    "    zero_rows = [target - length for length, target in zip(lengths, target_lengths)]\n",
    "    \n",
    "    # 初始化结果列表\n",
    "    parts = []\n",
    "    mask_parts = []\n",
    "    start = 0\n",
    "    \n",
    "    for i, length in enumerate(lengths):\n",
    "        end = start + length\n",
    "        \n",
    "        # 原始张量部分\n",
    "        parts.append(tensor[:, start:end, :])\n",
    "        mask_parts.append(torch.ones_like(tensor[:, start:end, :], dtype=torch.bool))\n",
    "        \n",
    "        # 插入零行\n",
    "        if zero_rows[i] > 0:\n",
    "            zero_padding = torch.zeros(tensor.size(0), zero_rows[i], tensor.size(2), device=tensor.device)\n",
    "            mask_padding = torch.zeros(tensor.size(0), zero_rows[i], tensor.size(2), dtype=torch.bool, device=tensor.device)\n",
    "            parts.append(zero_padding)\n",
    "            mask_parts.append(mask_padding)\n",
    "        \n",
    "        start = end\n",
    "    \n",
    "    # 拼接所有部分\n",
    "    padded_tensor = torch.cat(parts, dim=1)\n",
    "    mask = torch.cat(mask_parts, dim=1)\n",
    "    \n",
    "    return padded_tensor, mask\n",
    "\n",
    "# 示例输入\n",
    "tensor = torch.randn(7, 181 + 9 + 10, 128).to(device)\n",
    "lengths = [181, 9, 10]\n",
    "target_lengths = [190, 10, 10]\n",
    "\n",
    "\n",
    "\n",
    "# 调用函数\n",
    "start = time.time()\n",
    "padded_tensor, mask = insert_zero_rows(tensor, lengths, target_lengths)\n",
    "end = time.time()\n",
    "print(f\"运行时间: {end - start:.6f} 秒\")\n",
    "\n",
    "print(\"Padded Tensor shape:\", padded_tensor.shape)\n",
    "print(\"Mask shape:\", mask.shape)\n",
    "print(\"Padded Tensor:\", padded_tensor)\n",
    "print(\"Mask:\", mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding mask: tensor([[[[1, 1, 1, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0, 0]]]])\n",
      "Output with padding mask: tensor([[[[0.8881, 0.1050, 0.0069, 0.0000, 0.0000],\n",
      "          [0.3976, 0.5903, 0.0121, 0.0000, 0.0000],\n",
      "          [0.0518, 0.5258, 0.4225, 0.0000, 0.0000],\n",
      "          [0.5889, 0.3400, 0.0711, 0.0000, 0.0000],\n",
      "          [0.5533, 0.3558, 0.0909, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.1894, 0.4026, 0.4080, 0.0000, 0.0000],\n",
      "          [0.1732, 0.3968, 0.4300, 0.0000, 0.0000],\n",
      "          [0.0292, 0.9564, 0.0143, 0.0000, 0.0000],\n",
      "          [0.0345, 0.1203, 0.8452, 0.0000, 0.0000],\n",
      "          [0.1793, 0.7230, 0.0977, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.8942, 0.1058, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4025, 0.5975, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0896, 0.9104, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6339, 0.3661, 0.0000, 0.0000, 0.0000],\n",
      "          [0.6086, 0.3914, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3199, 0.6801, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3039, 0.6961, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0297, 0.9703, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2226, 0.7774, 0.0000, 0.0000, 0.0000],\n",
      "          [0.1987, 0.8013, 0.0000, 0.0000, 0.0000]]]])\n",
      "Output with look-ahead mask: tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4025, 0.5975, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0518, 0.5258, 0.4225, 0.0000, 0.0000],\n",
      "          [0.4765, 0.2751, 0.0575, 0.1909, 0.0000],\n",
      "          [0.2779, 0.1787, 0.0456, 0.1246, 0.3731]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3039, 0.6961, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0292, 0.9564, 0.0143, 0.0000, 0.0000],\n",
      "          [0.0227, 0.0793, 0.5567, 0.3413, 0.0000],\n",
      "          [0.1145, 0.4618, 0.0624, 0.2595, 0.1018]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(Q, K, mask=None):\n",
    "    # Q, K, V: [batch_size, seq_len, d_k]\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # 计算注意力得分\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    \n",
    "    # 应用掩码\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # 计算注意力权重\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 加权求和\n",
    "\n",
    "    return attn_weights\n",
    "\n",
    "# 示例输入\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_k = 4\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "# 填充掩码示例\n",
    "padding_mask = torch.tensor([[1, 1, 1, 0, 0], [1, 1, 0, 0, 0]]).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "\n",
    "# 前向掩码示例\n",
    "look_ahead_mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(1)  # [1, 1, seq_len, seq_len]\n",
    "\n",
    "# 应用填充掩码\n",
    "attn_weights = self_attention(Q, K, mask=padding_mask)\n",
    "print(\"Output with padding mask:\", attn_weights)\n",
    "\n",
    "# 应用前向掩码\n",
    "attn_weights = self_attention(Q, K, mask=look_ahead_mask)\n",
    "print(\"Output with look-ahead mask:\", attn_weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gearnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
