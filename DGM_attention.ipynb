{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdrug import  layers\n",
    "from torchdrug.core import Registry as R\n",
    "from torchdrug.layers import geometry\n",
    "\n",
    "import torch\n",
    "from torchdrug import data\n",
    "\n",
    "from torch_scatter import scatter_add\n",
    "import torch.nn as nn\n",
    "from torchdrug import utils\n",
    "from torch.utils import checkpoint\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:48:29   Extracting /home/xiaotong/scratch/protein-datasets/EnzymeCommission.zip to /home/xiaotong/scratch/protein-datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/xiaotong/scratch/protein-datasets/EnzymeCommission/enzyme_commission.pkl.gz: 100%|██████████| 18716/18716 [00:44<00:00, 416.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "PackedProtein(batch_size=2, num_atoms=[185, 415], num_bonds=[3754, 8999], num_residues=[185, 415])\n"
     ]
    }
   ],
   "source": [
    "EnzymeCommission = R.search(\"datasets.EnzymeCommission\")\n",
    "PV = R.search(\"transforms.ProteinView\")\n",
    "trans = PV(view = \"residue\")\n",
    "dataset = EnzymeCommission(\"~/scratch/protein-datasets/\", test_cutoff=0.95, \n",
    "                           atom_feature=\"full\", bond_feature=\"full\", verbose=1, transform = trans)\n",
    "\n",
    "# 只保留alpha碳的简化格式\n",
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()], \n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(k=10, min_distance=5),\n",
    "                                                                 geometry.SequentialEdge(max_distance=2)\n",
    "                                                                 ],\n",
    "                                                    edge_feature=\"gearnet\"\n",
    "                                                    )\n",
    "\n",
    "\n",
    "graphs = dataset[:2]\n",
    "graphs = [element[\"graph\"] for element in graphs]\n",
    "graphs = data.Protein.pack(graphs)\n",
    "print(\"\\n\\n\")\n",
    "graph = graph_construction_model(graphs)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([185, 415])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(graph.num_nodes)\n",
    "print(graph.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关系卷积神经网络，获取多个不同的嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationalGraph(layers.MessagePassingBase):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(relationalGraph, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_relation = num_relation\n",
    "        self.edge_input_dim = edge_input_dim\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        if edge_input_dim:\n",
    "            self.edge_linear = nn.Linear(edge_input_dim, input_dim)\n",
    "        else:\n",
    "            self.edge_linear = None\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, edge_list):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "\n",
    "        if edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t()\n",
    "        else:\n",
    "            node_in, node_out, relation = edge_list.t()\n",
    "        node_out = node_out * self.num_relation + relation\n",
    "        \n",
    "        edge_weight = torch.ones_like(node_out)\n",
    "        degree_out = scatter_add(edge_weight, node_out, dim_size=graph.num_node * graph.num_relation)\n",
    "        degree_out = degree_out\n",
    "        edge_weight = edge_weight / degree_out[node_out]\n",
    "        adjacency = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), edge_weight,\n",
    "                                            (graph.num_node, graph.num_node * graph.num_relation))\n",
    "        update = torch.sparse.mm(adjacency.t(), input)\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float()\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = edge_weight.unsqueeze(-1)\n",
    "            edge_update = scatter_add(edge_input * edge_weight, node_out, dim=0,\n",
    "                                      dim_size=graph.num_node * graph.num_relation)\n",
    "            update += edge_update\n",
    "\n",
    "        return update\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        # 自环特征\n",
    "        input = input.repeat(self.num_relation, 1)\n",
    "        loop_update = self.self_loop(input)\n",
    "        \n",
    "        output = self.linear(update)+loop_update\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, graph, input, edge_list):\n",
    "        \n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self._message_and_aggregate, *graph.to_tensors(), input)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph, input, edge_list)\n",
    "        output = self.combine(input, update)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  torch.Size([4200, 512])\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 512\n",
    "num_relations = graph.num_relation\n",
    "\n",
    "relational_output = relationalGraph(input_dim, output_dim, num_relations)(graph, graph.node_feature.float(), graph.edge_list)\n",
    "print(\"output: \", relational_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重连接模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 600\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_start_end(current, graph):\n",
    "    \"\"\"\n",
    "    根据一维张量 a 生成新的张量 b。\n",
    "    \n",
    "    :param a: 输入的一维张量\n",
    "    :return: 输出的一维张量 b\n",
    "    \"\"\"\n",
    "    # 初始化 b，第一个元素是 0\n",
    "    segment = graph.num_nodes.repeat(graph.num_relation)\n",
    "    index = torch.zeros(segment.size(0) + 1, dtype=segment.dtype)\n",
    "    \n",
    "    # 计算 b 的每个元素\n",
    "    for i in range(1, len(b)):\n",
    "        index[i] = index[i - 1] + segment[i - 1]\n",
    "    \n",
    "    # 遍历张量以找到索引值的位置\n",
    "    for i in range(len(index) - 1):\n",
    "        if index[i] <= current < index[i + 1]:\n",
    "            return (index[i].item(), index[i + 1].item())\n",
    "        elif index[i] == current:\n",
    "            return (index[i].item(), index[i + 1].item())\n",
    "    \n",
    "    # 如果索引值恰好等于张量的最后一个元素\n",
    "    if current == index[-1]:\n",
    "        return (index[-1].item(), index[-1].item())\n",
    "    \n",
    "    \n",
    "\n",
    "# 示例使用\n",
    "start, end = get_start_end(185, graph)\n",
    "print(start, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, window_size, temperature=0.5, dropout=0.1):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_features = out_features\n",
    "        self.window_size = window_size\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.query = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.key = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.value = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([out_features])).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        \n",
    "    def get_start_end(self, current, graph):\n",
    "        # 初始化 segment，第一个元素是 0\n",
    "        segment = graph.num_nodes.repeat(graph.num_relation)\n",
    "        index = torch.zeros(segment.size(0) + 1, dtype=segment.dtype)\n",
    "        \n",
    "        # 计算 index 的每个元素\n",
    "        for i in range(1, len(index)):\n",
    "            index[i] = index[i - 1] + segment[i - 1]\n",
    "        \n",
    "        # 遍历张量以找到索引值的位置\n",
    "        for i in range(len(index) - 1):\n",
    "            if index[i] <= current < index[i + 1]:\n",
    "                return (index[i].item(), index[i + 1].item())\n",
    "            elif index[i] == current:\n",
    "                return (index[i].item(), index[i + 1].item())\n",
    "        \n",
    "        # 如果索引值恰好等于张量的最后一个元素\n",
    "        if current == index[-1]:\n",
    "            return (index[-1].item(), index[-1].item())\n",
    "\n",
    "    def forward(self, node_features, graph):\n",
    "        device = node_features.device\n",
    "        num_nodes = node_features.size(0)\n",
    "        half_window = self.window_size // 2\n",
    "\n",
    "        # Apply linear layers and split into multiple heads\n",
    "        Q = self.query(node_features).view(num_nodes, self.num_heads, self.out_features)  # [num_nodes, num_heads, out_features]\n",
    "        K = self.key(node_features).view(num_nodes, self.num_heads, self.out_features)    # [num_nodes, num_heads, out_features]\n",
    "        \n",
    "\n",
    "        # Initialize output tensor\n",
    "        output = torch.zeros(num_nodes, num_nodes, device=device)  \n",
    "\n",
    "        # Precompute start and end indices for each node\n",
    "        start_end_indices = [self.get_start_end(i, graph) for i in range(num_nodes)]\n",
    "\n",
    "        # Compute sliding window attention\n",
    "        for i in range(num_nodes):\n",
    "            start_index, end_index = start_end_indices[i]\n",
    "            start = max(start_index, i - half_window)\n",
    "            end = min(end_index, i + half_window + 1)\n",
    "\n",
    "            Q_i = Q[i].unsqueeze(0)  # [1, num_heads, out_features]\n",
    "            K_window = K[start:end]  # [window_size, num_heads, out_features]\n",
    "\n",
    "            scores = torch.einsum(\"nhd,mhd->nhm\", Q_i, K_window) / self.scale  # [num_heads, 1, window_size]\n",
    "            scores = scores / self.temperature\n",
    "\n",
    "            attention_weights = F.softmax(scores, dim=-1)  # [num_heads, 1, window_size]\n",
    "            attention_weights = attention_weights.mean(dim=1)  # [num_heads, window_size]\n",
    "\n",
    "            output[i, start:end] = attention_weights\n",
    "\n",
    "        return output\n",
    "\n",
    "# 示例使用\n",
    "# 假设 graph 和 node_features 已经定义\n",
    "# graph.num_nodes 和 graph.num_relation 需要正确设置\n",
    "# node_features 是一个 [num_nodes, in_features] 的张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_dim = relational_output.shape[-1]\n",
    "output_dim = 1024\n",
    "num_heads = 8\n",
    "window_size = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "relational_output = relational_output.to(device)\n",
    "module = MultiHeadSelfAttention(input_dim, output_dim, num_heads, window_size).to(device)\n",
    "attn_output = module(relational_output, graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gearnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
