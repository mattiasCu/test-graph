{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cu/anaconda3/envs/gearnet/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cu/anaconda3/envs/gearnet/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from torchdrug import  layers, datasets,transforms,core\n",
    "from torchdrug.core import Registry as R\n",
    "from torchdrug.layers import geometry\n",
    "\n",
    "import torch\n",
    "from torchdrug import data\n",
    "\n",
    "from torch_scatter import scatter_add\n",
    "import torch.nn as nn\n",
    "from torchdrug import utils\n",
    "from torch.utils import checkpoint\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:54:34   Extracting /home/cu/scratch/protein-datasets/EnzymeCommission.zip to /home/cu/scratch/protein-datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/cu/scratch/protein-datasets/EnzymeCommission/enzyme_commission.pkl.gz: 100%|██████████| 18716/18716 [00:22<00:00, 843.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "PackedProtein(batch_size=2, num_atoms=[185, 415], num_bonds=[3388, 8173], num_residues=[185, 415])\n"
     ]
    }
   ],
   "source": [
    "EnzymeCommission = R.search(\"datasets.EnzymeCommission\")\n",
    "PV = R.search(\"transforms.ProteinView\")\n",
    "trans = PV(view = \"residue\")\n",
    "dataset = EnzymeCommission(\"~/scratch/protein-datasets/\", test_cutoff=0.95, \n",
    "                           atom_feature=\"full\", bond_feature=\"full\", verbose=1, transform = trans)\n",
    "\n",
    "# 只保留alpha碳的简化格式\n",
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()], \n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(k=10, min_distance=5),\n",
    "                                                                 geometry.SequentialEdge(max_distance=1)\n",
    "                                                                 ],\n",
    "                                                    edge_feature=\"gearnet\"\n",
    "                                                    )\n",
    "\n",
    "\n",
    "graphs = dataset[:2]\n",
    "graphs = [element[\"graph\"] for element in graphs]\n",
    "graphs = data.Protein.pack(graphs)\n",
    "print(\"\\n\\n\")\n",
    "graph = graph_construction_model(graphs)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([185, 415])\n",
      "2\n",
      "tensor([[155, 156,   4],\n",
      "        [168, 169,   4],\n",
      "        [167, 168,   4],\n",
      "        ...,\n",
      "        [426, 435,   0],\n",
      "        [440, 435,   0],\n",
      "        [487, 435,   0]])\n"
     ]
    }
   ],
   "source": [
    "#graph = graph.to(device)\n",
    "\n",
    "print(graph.num_nodes)\n",
    "print(graph.batch_size)\n",
    "print(graph.edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关系卷积神经网络，获取多个不同的嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationalGraph(layers.MessagePassingBase):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(relationalGraph, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_relation = num_relation\n",
    "        self.edge_input_dim = edge_input_dim\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        if edge_input_dim:\n",
    "            self.edge_linear = nn.Linear(edge_input_dim, input_dim)\n",
    "        else:\n",
    "            self.edge_linear = None\n",
    "            \n",
    "    def trans(self, A, graph):\n",
    "    \n",
    "        Degree_inv_sqrt = torch.diag(torch.pow(torch.sum(A, dim=1), -0.5))\n",
    "        A_norm = torch.mm(torch.mm(Degree_inv_sqrt, A), Degree_inv_sqrt)\n",
    "        \n",
    "        n_rel = graph.num_relation\n",
    "        n = A_norm.size(0)\n",
    "        n_rel = n_rel.item()  # 将 n_rel 从 Tensor 转换为 int\n",
    "        assert n % n_rel == 0, \"n must be divisible by n_rel\"\n",
    "        \n",
    "        block_size = n // n_rel\n",
    "        \n",
    "        # 初始化一个张量来存储累加结果\n",
    "        accumulated = torch.zeros_like(A_norm[:block_size])\n",
    "        \n",
    "        # 将后面的所有块累加到第一块\n",
    "        for i in range(n_rel):\n",
    "            accumulated += A_norm[i * block_size: (i + 1) * block_size]\n",
    "        \n",
    "        # 用累加后的第一块替换原始矩阵的第一块\n",
    "        A_trans = accumulated\n",
    "    \n",
    "        return A_trans\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "        device = input.device  # Ensure device consistency\n",
    "        \n",
    "        if new_edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "            node_out = node_out * self.num_relation + relation\n",
    "        \n",
    "            edge_weight = torch.ones_like(node_out)\n",
    "            degree_out = scatter_add(edge_weight, node_out, dim_size=graph.num_node * graph.num_relation)\n",
    "            degree_out = degree_out\n",
    "            edge_weight = edge_weight / degree_out[node_out]\n",
    "            adjacency = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), edge_weight,\n",
    "                                                (graph.num_node, graph.num_node * graph.num_relation))\n",
    "            update = torch.sparse.mm(adjacency.t(), input)\n",
    "        else:\n",
    "            adjacency = self.trans(new_edge_list, graph).to(device)\n",
    "            update = torch.mm(adjacency.t().to(device), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(edge_input * edge_weight, node_out, dim=0,\n",
    "                                      dim_size=graph.num_node * graph.num_relation)\n",
    "            update += edge_update\n",
    "\n",
    "        return update\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        # 自环特征\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        input = input.repeat(self.num_relation, 1).to(device)\n",
    "        loop_update = self.self_loop(input).to(device)\n",
    "        \n",
    "        output = self.linear(update)+loop_update\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        \n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self._message_and_aggregate, *graph.to_tensors(), input, new_edge_list)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph, input, new_edge_list)\n",
    "        output = self.combine(input, update).view(graph.num_relation, input.size(0), -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.1173, 0.1634,  ..., 0.0293, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0007, 0.0000,  ..., 0.2899, 0.0000, 0.0000],\n",
      "         [0.0000, 0.2027, 0.1033,  ..., 0.1484, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0089, 0.1264, 0.0114,  ..., 0.0129, 0.0000, 0.0000],\n",
      "         [0.1178, 0.1718, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.1201, 0.1198,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0114, 0.1481, 0.0909,  ..., 0.0315, 0.0000, 0.0000],\n",
      "         [0.0272, 0.0318, 0.0000,  ..., 0.3047, 0.0000, 0.0000],\n",
      "         [0.1022, 0.2793, 0.3373,  ..., 0.2042, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0690, 0.0000,  ..., 0.0980, 0.0000, 0.1223],\n",
      "         [0.1178, 0.1718, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0097, 0.0788, 0.0947,  ..., 0.0000, 0.0072, 0.0000]],\n",
      "\n",
      "        [[0.0168, 0.1657, 0.0866,  ..., 0.0085, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0385, 0.0000,  ..., 0.2266, 0.0000, 0.0000],\n",
      "         [0.0483, 0.0305, 0.0754,  ..., 0.2700, 0.0000, 0.0067],\n",
      "         ...,\n",
      "         [0.0089, 0.1264, 0.0114,  ..., 0.0129, 0.0000, 0.0000],\n",
      "         [0.1706, 0.1616, 0.0000,  ..., 0.2096, 0.0000, 0.0000],\n",
      "         [0.0000, 0.1964, 0.2117,  ..., 0.0543, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0341, 0.1511, 0.0932,  ..., 0.0354, 0.0000, 0.0000],\n",
      "         [0.0000, 0.1447, 0.0000,  ..., 0.2531, 0.0000, 0.1352],\n",
      "         [0.0000, 0.0481, 0.0543,  ..., 0.1398, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0481, 0.0543,  ..., 0.1398, 0.0000, 0.0000],\n",
      "         [0.0483, 0.0305, 0.0754,  ..., 0.2700, 0.0000, 0.0067],\n",
      "         [0.1606, 0.0000, 0.0390,  ..., 0.0584, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.1005, 0.1479,  ..., 0.0113, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0103, 0.0000,  ..., 0.2974, 0.0000, 0.0000],\n",
      "         [0.1178, 0.1718, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0201, 0.0960, 0.1367,  ..., 0.1635, 0.0000, 0.0000],\n",
      "         [0.0852, 0.0251, 0.3306,  ..., 0.0084, 0.0000, 0.0000],\n",
      "         [0.1606, 0.0000, 0.0390,  ..., 0.0584, 0.0000, 0.0000]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "output:  torch.Size([5, 600, 128])\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 128\n",
    "num_relations = graph.num_relation\n",
    "\n",
    "relational_output = relationalGraph(input_dim, output_dim, num_relations)(graph, graph.node_feature.float(), new_edge_list = None)\n",
    "print(relational_output)\n",
    "print(\"output: \", relational_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重连接模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 只从点所在的图进行reconnection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gumble-softmax采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 1., 0., 0., 0., 1., 1., 0.],\n",
      "        [1., 0., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 1., 0., 0.]])\n",
      "tensor([[9.2066e-02, 4.4660e-06, 2.0334e-02, 1.2134e-02, 1.6932e-02, 4.3917e-03,\n",
      "         2.0580e-03, 5.2264e-02, 7.8937e-01, 1.0444e-02],\n",
      "        [7.2328e-01, 2.0644e-04, 2.0815e-02, 1.9107e-03, 9.1716e-05, 1.0527e-01,\n",
      "         6.1835e-03, 1.1957e-02, 1.2846e-04, 1.3016e-01],\n",
      "        [3.2986e-08, 8.7121e-08, 3.5935e-07, 1.0000e+00, 3.5876e-08, 8.5148e-09,\n",
      "         2.7917e-07, 7.8181e-09, 7.1131e-08, 1.5726e-08],\n",
      "        [2.4566e-03, 1.8842e-02, 1.0022e-03, 1.2845e-03, 9.4768e-01, 1.3542e-04,\n",
      "         2.1959e-04, 1.5191e-02, 9.8077e-04, 1.2210e-02],\n",
      "        [1.3476e-02, 1.7886e-04, 7.4551e-04, 2.7668e-03, 6.0881e-01, 3.5665e-02,\n",
      "         3.0852e-01, 5.2925e-03, 2.3564e-02, 9.8649e-04]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gumbel_softmax_top_k(logits, tau=0.5, k=1, hard=False):\n",
    "    \"\"\"\n",
    "    Gumbel-Softmax采样方法，每一步都是可微分的，并选择最大的k个元素。\n",
    "    \n",
    "    参数:\n",
    "        logits (torch.Tensor): 输入logits张量，维度为 (batch_size, num_classes)。\n",
    "        tau (float): Gumbel-Softmax的温度参数，控制平滑程度。\n",
    "        k (int): 选择最大的k个元素。\n",
    "        hard (bool): 是否返回硬分类结果。\n",
    "    \n",
    "    返回:\n",
    "        torch.Tensor: Gumbel-Softmax采样结果，维度为 (batch_size, num_classes)。\n",
    "    \"\"\"\n",
    "    # 获取Gumbel分布噪声\n",
    "    gumbels = -torch.empty_like(logits).exponential_().log()  # 生成Gumbel(0,1)噪声\n",
    "    gumbels = (logits + gumbels) / tau  # 添加噪声并除以温度参数\n",
    "\n",
    "    # 计算softmax\n",
    "    y_soft = F.softmax(gumbels, dim=-1)  # 维度为 (batch_size, num_classes)\n",
    "\n",
    "    if hard:\n",
    "        # 硬分类结果：选取原始logits最大的k个位置\n",
    "        topk_indices = logits.topk(k, dim=-1)[1]  # 获取前k个元素的索引\n",
    "        y_hard = torch.zeros_like(logits).scatter_(-1, topk_indices, 1.0)  # 生成one-hot向量\n",
    "        # 使用直通估计器\n",
    "        y = (y_hard - y_soft).detach() + y_soft\n",
    "    else:\n",
    "        y = y_soft\n",
    "\n",
    "    return y\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == \"__main__\":\n",
    "    logits = torch.randn(5, 10)  # 维度为 (batch_size=5, num_classes=10)\n",
    "    tau = 0.5\n",
    "    k = 3\n",
    "    hard = True\n",
    "    samples = gumbel_softmax_top_k(logits, tau, k, hard)\n",
    "    print(samples)\n",
    "    samples = gumbel_softmax_top_k(logits, tau, k)\n",
    "    print(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### window self attention + gumble softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Rewirescorelayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, window_size, k, temperature=0.5, dropout=0.1):\n",
    "        super(Rewirescorelayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_features = out_features\n",
    "        self.window_size = window_size\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.k = k\n",
    "        \n",
    "        self.query = nn.Linear( in_features, out_features* num_heads)\n",
    "        self.key = nn.Linear( in_features, out_features* num_heads)\n",
    "        self.scale = 1 / (out_features ** 0.5)\n",
    "\n",
    "    def split_windows(self, tensor, index, window_size):\n",
    "        result = []\n",
    "        index_list  = []\n",
    "        # 初始的开始索引\n",
    "        start = 0\n",
    "\n",
    "        for idx in index:\n",
    "            end = start + idx - 1\n",
    "            while start < end:\n",
    "                if start + window_size <= end:\n",
    "                    result.append(tensor[:, start:start+window_size, :])\n",
    "                    index_list.append([start, start+window_size])\n",
    "                    start += window_size\n",
    "                else:\n",
    "                    # 计算还需要填充多少行\n",
    "                    padding_rows = window_size - (end - start + 1)\n",
    "                    restart = start - padding_rows\n",
    "                    result.append(tensor[:, restart:restart + window_size , :])\n",
    "                    index_list.append([restart, restart+window_size])\n",
    "                    start = end + 1\n",
    "                    \n",
    "        # 转换结果列表为 tensor\n",
    "        result_tensor = torch.stack(result, dim=1)\n",
    "        return result_tensor, index_list\n",
    "\n",
    "    def gumbel_softmax_top_k(self, logits, tau=1.0, hard=False):\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "        gumbels = (logits + gumbels) / tau\n",
    "\n",
    "        y_soft = F.softmax(gumbels, dim=-1)\n",
    "\n",
    "        if hard:\n",
    "            topk_indices = logits.topk(self.k, dim=-1)[1]\n",
    "            y_hard = torch.zeros_like(logits).scatter_(-1, topk_indices, 1.0)\n",
    "            y = (y_hard - y_soft).detach() + y_soft\n",
    "        else:\n",
    "            y = y_soft\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def windows2adjacent(self, windows, index_list, output):\n",
    "    \n",
    "        for i, index in enumerate(index_list):\n",
    "            start, end = index\n",
    "            output[:, start:end, start:end] = windows[:, i, :, :]\n",
    "            \n",
    "        num_relations, num_nodes, _ = output.shape\n",
    "        result = torch.zeros(num_relations* num_nodes, num_relations*num_nodes)\n",
    "        for i in range(num_relations):\n",
    "            result[i*num_nodes:(i+1)*num_nodes, i*num_nodes:(i+1)*num_nodes] = output[i]\n",
    "        return result\n",
    "\n",
    "    def forward(self, graph, node_features):\n",
    "        device = node_features.device\n",
    "        num_relations = node_features.size(0)\n",
    "        num_nodes = node_features.size(1)\n",
    "        index = graph.num_nodes.tolist()\n",
    "        \n",
    "        Q = self.query(node_features).view(num_relations, num_nodes, self.num_heads, self.out_features).permute(0, 2, 1, 3)\n",
    "        K = self.key(node_features).view(num_relations, num_nodes, self.num_heads, self.out_features).permute(0, 2, 1, 3)   \n",
    "        Q = Q.reshape(num_relations * self.num_heads, num_nodes, self.out_features)\n",
    "        K = K.reshape(num_relations * self.num_heads, num_nodes, self.out_features)\n",
    "        \n",
    "        output = torch.zeros(num_relations, num_nodes, num_nodes)\n",
    "       \n",
    "        Q_windows, Q_index = self.split_windows(Q, index, self.window_size)\n",
    "        K_windows, _ = self.split_windows(K, index, self.window_size)\n",
    "        \n",
    "        # 计算 attention\n",
    "        scores  = torch.einsum('b h i e, b h j e -> b h i j', Q_windows, K_windows) / self.scale\n",
    "        attn = scores.softmax(dim=-1).view(num_relations, self.num_heads, -1, self.window_size, self.window_size).mean(dim=1)\n",
    "        attn = self.gumbel_softmax_top_k(attn, tau=self.temperature, hard=True)\n",
    "        \n",
    "        result = self.windows2adjacent(attn, Q_index, output)\n",
    "        \n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间: 0.042305 秒\n",
      "torch.Size([3000, 3000])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<CopySlices>)\n",
      "(tensor([   0,    0,    0,  ..., 2999, 2999, 2999]), tensor([   1,    7,    8,  ..., 2990, 2995, 2996]))\n"
     ]
    }
   ],
   "source": [
    "input_dim = relational_output.shape[-1]\n",
    "output_dim = 256\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 3\n",
    "num_nodes = relational_output.size(0)\n",
    "\n",
    "start = time.time()\n",
    "relational_output = relational_output.to(device)\n",
    "module = Rewirescorelayer(input_dim, output_dim, num_heads, window_size, k).to(device)\n",
    "attn_output = module(graph, relational_output)\n",
    "end = time.time()\n",
    "print(f\"运行时间: {end - start:.6f} 秒\")\n",
    "print(attn_output.shape)\n",
    "\n",
    "print(attn_output)\n",
    "#a = attn_output[1200:, 599:1199]\n",
    "indices = torch.nonzero(attn_output, as_tuple=True)\n",
    "print(indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试不同degree进行采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds:\n",
      " tensor([0.2000, 0.5000, 0.7000, 0.9000, 0.1000, 0.3000, 0.6000, 0.8000, 0.4000,\n",
      "        0.2000])\n",
      "Sampled Matrix:\n",
      " tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gumbel_softmax_sample(logits, tau):\n",
    "    # 从Gumbel(0, 1)分布中采样\n",
    "    gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits)))\n",
    "    y = logits + gumbel_noise\n",
    "    return F.softmax(y / tau, dim=-1)\n",
    "\n",
    "def bernoulli_sampling_with_different_thresholds(probs, thresholds, tau=1.0):\n",
    "    # 对数概率\n",
    "    logits = torch.log(probs) - torch.log(1 - probs)\n",
    "    # 进行Gumbel-Softmax采样\n",
    "    y = gumbel_softmax_sample(logits, tau)\n",
    "    # 硬化处理，根据每行的不同阈值\n",
    "    z = (y > thresholds.unsqueeze(1)).float()\n",
    "    return z\n",
    "\n",
    "# 示例矩阵\n",
    "n = 10\n",
    "P = torch.rand(n, n)\n",
    "\n",
    "# 为每一行设置不同的阈值\n",
    "thresholds = torch.tensor([0.2, 0.5, 0.7, 0.9, 0.1, 0.3, 0.6, 0.8, 0.4, 0.2])\n",
    "\n",
    "# 进行可微分伯努利采样并硬化处理\n",
    "tau = 0.1  # 温度参数\n",
    "sampled_matrix = bernoulli_sampling_with_different_thresholds(P, thresholds, tau)\n",
    "\n",
    "#print(\"Probability Matrix:\\n\", P)\n",
    "print(\"Thresholds:\\n\", thresholds)\n",
    "print(\"Sampled Matrix:\\n\", sampled_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算degree矩阵，变换adjacent matrix形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.3333, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([600, 3000])\n",
      "(tensor([  0,   0,   0,  ..., 599, 599, 599]), tensor([   1,    7,    8,  ..., 2990, 2995, 2996]))\n",
      "torch.Size([3000, 21])\n"
     ]
    }
   ],
   "source": [
    "def trans(A, graph):\n",
    "    \n",
    "    Degree_inv_sqrt = torch.diag(torch.pow(torch.sum(A, dim=1), -0.5))\n",
    "    A_norm = torch.mm(torch.mm(Degree_inv_sqrt, A), Degree_inv_sqrt)\n",
    "    \n",
    "    n_rel = graph.num_relation\n",
    "    n = A_norm.size(0)\n",
    "    n_rel = n_rel.item()  # 将 n_rel 从 Tensor 转换为 int\n",
    "    assert n % n_rel == 0, \"n must be divisible by n_rel\"\n",
    "    \n",
    "    block_size = n // n_rel\n",
    "    \n",
    "    # 初始化一个张量来存储累加结果\n",
    "    accumulated = torch.zeros_like(A_norm[:block_size])\n",
    "    \n",
    "    # 将后面的所有块累加到第一块\n",
    "    for i in range(n_rel):\n",
    "        accumulated += A_norm[i * block_size: (i + 1) * block_size]\n",
    "    \n",
    "    # 用累加后的第一块替换原始矩阵的第一块\n",
    "    A_trans = accumulated\n",
    "    \n",
    "    \n",
    "    \n",
    "    return A_trans\n",
    "\n",
    "A_norm = trans(attn_output, graph)\n",
    "print(A_norm)\n",
    "print(A_norm.shape)\n",
    "\n",
    "indices = torch.nonzero(A_norm, as_tuple=True)\n",
    "print(indices)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "update = torch.mm(A_norm.t(), graph.node_feature.to(device).to(torch.float))\n",
    "print(update.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewired_gearnet 用于diffusion模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewireGearnet(nn.Module):\n",
    "    gradient_checkpoint = False\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(RewireGearnet, self).__init__()\n",
    "        self.num_relation = num_relation\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.linear = nn.Linear(num_relation * input_dim, output_dim)\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(output_dim) if batch_norm else None\n",
    "        self.activation = getattr(F, activation) if activation else None\n",
    "        self.edge_linear = nn.Linear(edge_input_dim, output_dim) if edge_input_dim else None\n",
    "\n",
    "    def trans(self, A, graph):\n",
    "        n_rel = graph.num_relation\n",
    "        n = A.size(0)\n",
    "        n_rel = n_rel.item()  # 将 n_rel 从 Tensor 转换为 int\n",
    "        assert n % n_rel == 0, \"n must be divisible by n_rel\"\n",
    "        \n",
    "        block_size = n // n_rel\n",
    "        \n",
    "        # 初始化一个张量来存储累加结果\n",
    "        accumulated = torch.zeros_like(A[:block_size])\n",
    "        \n",
    "        # 将后面的所有块累加到第一块\n",
    "        for i in range(n_rel):\n",
    "            accumulated += A[i * block_size: (i + 1) * block_size]\n",
    "        \n",
    "        # 用累加后的第一块替换原始矩阵的第一块\n",
    "        A_trans = accumulated\n",
    "    \n",
    "        return A_trans\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list=None):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "\n",
    "        device = input.device  # Ensure device consistency\n",
    "\n",
    "        if new_edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "            node_out = node_out * self.num_relation + relation\n",
    "            adjacency = torch.sparse_coo_tensor(\n",
    "                torch.stack([node_in, node_out]),\n",
    "                graph.edge_weight.to(device),\n",
    "                (graph.num_node, graph.num_node * graph.num_relation),\n",
    "                device=device\n",
    "            )\n",
    "            update = torch.sparse.mm(adjacency.t(), input)\n",
    "        else:\n",
    "            adjacency = self.trans(new_edge_list, graph).to(device)\n",
    "            update = torch.mm(adjacency.t(), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = graph.edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(\n",
    "                edge_input * edge_weight, node_out, dim=0,\n",
    "                dim_size=graph.num_node * graph.num_relation\n",
    "            )\n",
    "            update += edge_update\n",
    "            \n",
    "        return update.view(graph.num_node, self.num_relation * self.input_dim).to(device)\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm.to(device)\n",
    "        \n",
    "        output = self.linear(update) + self.self_loop(input)\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, graph, input, new_edge_list=None, new_edge_weight=None):\n",
    "        \"\"\"\n",
    "        Perform message passing over the graph(s).\n",
    "\n",
    "        Parameters:\n",
    "            graph (Graph): graph(s)\n",
    "            input (Tensor): node representations of shape :math:`(|V|, ...)`\n",
    "        \"\"\"\n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self.message_and_aggregate, graph, input)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph, input, new_edge_list)\n",
    "        output = self.combine(input, update)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 512])\n",
      "tensor([[0.1449, 0.0628, 0.0000,  ..., 0.3983, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.1705, 0.0874, 0.6224],\n",
      "        [0.0000, 0.0000, 0.2159,  ..., 0.0000, 0.0000, 0.2213],\n",
      "        ...,\n",
      "        [0.1342, 0.0000, 0.3268,  ..., 0.0116, 0.0000, 0.3118],\n",
      "        [0.0000, 0.2666, 0.0000,  ..., 0.4255, 0.1149, 0.3375],\n",
      "        [0.2765, 0.0000, 0.0000,  ..., 0.0175, 0.4929, 0.5345]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 512\n",
    "num_relations = graph.num_relation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "new_node_feature = RewireGearnet(input_dim, output_dim, num_relations)(graph, graph.node_feature.to(device).float(), attn_output).to(device)\n",
    "\n",
    "print(new_node_feature.shape)\n",
    "print(new_node_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMGearnet(nn.Module, core.Configurable):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims, score_dim, num_relation, num_heads, window_size, k, edge_input_dim=None, num_angle_bin=None,\n",
    "                 short_cut=False, batch_norm=False, activation=\"relu\", concat_hidden=False, readout=\"sum\"):\n",
    "        super(DGMGearnet, self).__init__()\n",
    "\n",
    "        #if not isinstance(hidden_dims, Sequence):\n",
    "            #hidden_dims = [hidden_dims]\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = sum(hidden_dims) if concat_hidden else hidden_dims[-1]\n",
    "        self.dims = [input_dim] + list(hidden_dims)\n",
    "        self.score_dim = score_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.k = k\n",
    "        self.edge_dims = [edge_input_dim] + self.dims[:-1]\n",
    "        self.num_relation = num_relation\n",
    "        self.num_angle_bin = num_angle_bin\n",
    "        self.short_cut = short_cut\n",
    "        self.concat_hidden = concat_hidden\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.score_layers = nn.ModuleList()\n",
    "        for i in range(len(self.dims) - 1):\n",
    "            \n",
    "            self.score_layers.append(relationalGraph(self.dims[i], self.score_dim, num_relation, None, \n",
    "                                            batch_norm, activation))\n",
    "            \n",
    "            self.score_layers.append(Rewirescorelayer(self.score_dim, self.dims[i+1], self.num_heads, self.window_size, \n",
    "                                            self.k, temperature=0.5, dropout=0.1))\n",
    "            \n",
    "            self.layers.append(RewireGearnet(self.dims[i], self.dims[i + 1], num_relation,\n",
    "                                            edge_input_dim=None, batch_norm=False, activation=\"relu\"))\n",
    "        \n",
    "        if num_angle_bin:\n",
    "            self.spatial_line_graph = layers.SpatialLineGraph(num_angle_bin)\n",
    "            self.edge_layers = nn.ModuleList()\n",
    "            for i in range(len(self.edge_dims) - 1):\n",
    "                self.edge_layers.append(layers.GeometricRelationalGraphConv(\n",
    "                    self.edge_dims[i], self.edge_dims[i + 1], num_angle_bin, None, batch_norm, activation))\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norms = nn.ModuleList()\n",
    "            for i in range(len(self.dims) - 1):\n",
    "                self.batch_norms.append(nn.BatchNorm1d(self.dims[i + 1]))\n",
    "\n",
    "        if readout == \"sum\":\n",
    "            self.readout = layers.SumReadout()\n",
    "        elif readout == \"mean\":\n",
    "            self.readout = layers.MeanReadout()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown readout `%s`\" % readout)\n",
    "\n",
    "    def forward(self, graph, input, edge_list=None, all_loss=None, metric=None):\n",
    "        \"\"\"\n",
    "        Compute the node representations and the graph representation(s).\n",
    "\n",
    "        Parameters:\n",
    "            graph (Graph): :math:`n` graph(s)\n",
    "            input (Tensor): input node representations\n",
    "            all_loss (Tensor, optional): if specified, add loss to this tensor\n",
    "            metric (dict, optional): if specified, output metrics to this dict\n",
    "\n",
    "        Returns:\n",
    "            dict with ``node_feature`` and ``graph_feature`` fields:\n",
    "                node representations of shape :math:`(|V|, d)`, graph representations of shape :math:`(n, d)`\n",
    "        \"\"\"\n",
    "        hiddens = []\n",
    "        layer_input = input\n",
    "        if self.num_angle_bin:\n",
    "            line_graph = self.spatial_line_graph(graph)\n",
    "            edge_input = line_graph.node_feature.float()\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            \n",
    "            relational_output = self.score_layers[2*i](graph, layer_input, edge_list)\n",
    "            new_edge_list = self.score_layers[2*i+1](graph, relational_output)\n",
    "            \n",
    "            hidden = self.layers[i](graph, layer_input, new_edge_list)\n",
    "            \n",
    "            if self.short_cut and hidden.shape == layer_input.shape:\n",
    "                hidden = hidden + layer_input\n",
    "            \n",
    "            if self.num_angle_bin:\n",
    "                edge_hidden = self.edge_layers[i](line_graph, edge_input)\n",
    "                edge_weight = graph.edge_weight.unsqueeze(-1)\n",
    "                if new_edge_list is None:\n",
    "                    node_out = graph.edge_list[:, 1] * self.num_relation + graph.edge_list[:, 2]\n",
    "                else:\n",
    "                    node_out = new_edge_list[:, 1] * self.num_relation + new_edge_list[:, 2]\n",
    "                \n",
    "                    update = scatter_add(edge_hidden * edge_weight, node_out, dim=0,\n",
    "                                        dim_size=graph.num_node * self.num_relation)\n",
    "                update = update.view(graph.num_node, self.num_relation * edge_hidden.shape[1])\n",
    "                update = self.layers[i].linear(update)\n",
    "                update = self.layers[i].activation(update)\n",
    "                hidden = hidden + update\n",
    "                edge_input = edge_hidden\n",
    "                \n",
    "            if self.batch_norm:\n",
    "                hidden = self.batch_norms[i](hidden)\n",
    "                \n",
    "            hiddens.append(hidden)\n",
    "            layer_input = hidden\n",
    "            edge_list = new_edge_list\n",
    "\n",
    "        if self.concat_hidden:\n",
    "            node_feature = torch.cat(hiddens, dim=-1)\n",
    "        else:\n",
    "            node_feature = hiddens[-1]\n",
    "        graph_feature = self.readout(graph, node_feature)\n",
    "        print(\"node_feature: \", node_feature.shape)\n",
    "\n",
    "        return {\n",
    "            \"graph_feature\": graph_feature,\n",
    "            \"node_feature\": node_feature\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_feature:  torch.Size([600, 512])\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "hidden_dims = [512, 512, 512, 512, 512]\n",
    "score_dim = 128\n",
    "num_relations = graph.num_relation\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "k = 5\n",
    "\n",
    "\n",
    "output = DGMGearnet(input_dim, hidden_dims, score_dim, num_relations, num_heads, window_size, k).to(device)(graph.to(device), graph.node_feature.to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[31.8002,  0.0000, 18.8589,  ...,  0.0000,  8.2624,  0.0000],\n",
      "        [20.0393,  0.0000, 17.0460,  ...,  0.0000,  6.8150,  0.0000],\n",
      "        [ 6.4855,  0.0000,  4.4976,  ...,  0.0000,  7.6434,  0.0000],\n",
      "        ...,\n",
      "        [13.9503,  0.0000, 12.1124,  ...,  0.0000,  7.2026,  0.0000],\n",
      "        [44.9440,  0.0000, 27.2555,  ...,  0.0000, 20.4179,  0.0000],\n",
      "        [26.0715,  0.0000, 11.8351,  ...,  0.0000, 10.0670,  0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "torch.Size([600, 512])\n",
      "\n",
      "\n",
      "tensor([[4341.7954,   26.5939, 2779.7217,  ...,   23.5720, 1896.7634,\n",
      "          137.0831],\n",
      "        [9712.8516,   84.3868, 6230.3101,  ...,   53.5257, 4284.4419,\n",
      "          260.6467]], grad_fn=<ScatterAddBackward>)\n",
      "torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "print(output[\"node_feature\"])\n",
    "print(output[\"node_feature\"].shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(output[\"graph_feature\"])\n",
    "print(output[\"graph_feature\"].shape)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from local_attention import LocalAttention\n",
    "\n",
    "node = 1200\n",
    "dimm = 64\n",
    "b = 10\n",
    "\n",
    "q = torch.randn(b, 8, node, dimm)\n",
    "k = torch.randn(b, 8, node, dimm)\n",
    "v = torch.randn(b, 8, node, dimm)\n",
    "\n",
    "attn = LocalAttention(\n",
    "    dim =64,                # dimension of each head (you need to pass this in for relative positional encoding)\n",
    "    window_size = 10,       # window size. 512 is optimal, but 256 or 128 yields good enough results\n",
    "    causal = None,           # auto-regressive or not\n",
    "    look_backward = 1,       # each window looks at the window before\n",
    "    look_forward = 1,        # for non-auto-regressive case, will default to 1, so each window looks at the window before and after it\n",
    "    dropout = 0.1,           # post-attention dropout\n",
    "    exact_windowsize = False # if this is set to true, in the causal setting, each query will see at maximum the number of keys equal to the window size\n",
    ")\n",
    "\n",
    "mask = torch.ones(b, node).bool()\n",
    "out = attn(q, k, v, mask = mask) # (2, 8, 2048, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8, 1200, 64])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat, pack, unpack\n",
    "\n",
    "# constant\n",
    "\n",
    "TOKEN_SELF_ATTN_VALUE = -5e4\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "# 如果value不存在，返回d\n",
    "def default(value, d):\n",
    "    return d if not exists(value) else value\n",
    "\n",
    "def to(t):\n",
    "    return {'device': t.device, 'dtype': t.dtype}\n",
    "\n",
    "def max_neg_value(tensor):\n",
    "    return -torch.finfo(tensor.dtype).max  #返回给定张量数据类型的所能表示的最大负值\n",
    "\n",
    "def l2norm(tensor):\n",
    "    dtype = tensor.dtype\n",
    "    normed = F.normalize(tensor, dim = -1)\n",
    "    return normed.type(dtype)\n",
    "\n",
    "def pad_to_multiple(tensor, multiple, dim=-1, value=0):\n",
    "    seqlen = tensor.shape[dim]\n",
    "    m = seqlen / multiple\n",
    "    if m.is_integer():\n",
    "        return False, tensor\n",
    "    remainder = math.ceil(m) * multiple - seqlen\n",
    "    pad_offset = (0,) * (-1 - dim) * 2\n",
    "    return True, F.pad(tensor, (*pad_offset, 0, remainder), value = value)\n",
    "\n",
    "def look_around(x, backward = 1, forward = 0, pad_value = -1, dim = 2):  #x = bk: (40, 32, 16, 64)\n",
    "    t = x.shape[1]    #获取一共有多少个窗口，这里是32\n",
    "    dims = (len(x.shape) - dim) * (0, 0)   #一个长度为 len(x.shape) - dim 的元组，每个元素为 (0, 0)；其中len(x.shape) = 4\n",
    "    padded_x = F.pad(x, (*dims, backward, forward), value = pad_value)   #在第二维度上，前面加backward个元素，后面加forward个元素 -> (40, 33, 16, 64)\n",
    "    tensors = [padded_x[:, ind:(ind + t), ...] for ind in range(forward + backward + 1)] #一个张量列表，每个张量的维度为(40, 32, 16, 64), len = 2\n",
    "    return torch.cat(tensors, dim = dim) #在第二维度上拼接 -> (40, 32, 32, 64)\n",
    "\n",
    "# main class\n",
    "\n",
    "class Attention(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_size,\n",
    "        causal = False,\n",
    "        look_backward = 1,\n",
    "        look_forward = None,\n",
    "        dropout = 0.,\n",
    "        shared_qk = False,\n",
    "        rel_pos_emb_config = None,\n",
    "        dim = None,\n",
    "        autopad = False,\n",
    "        exact_windowsize = False,\n",
    "        scale = None,\n",
    "        use_rotary_pos_emb = True,\n",
    "        use_xpos = False,\n",
    "        xpos_scale_base = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        look_forward = default(look_forward, 0 if causal else 1)\n",
    "        assert not (causal and look_forward > 0), 'you cannot look forward if causal'\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.autopad = autopad\n",
    "        self.exact_windowsize = exact_windowsize\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "        self.look_backward = look_backward\n",
    "        self.look_forward = look_forward\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.shared_qk = shared_qk\n",
    "\n",
    "        # relative positions\n",
    "\n",
    "        self.rel_pos = None\n",
    "        self.use_xpos = use_xpos\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q, k,\n",
    "        mask = None,\n",
    "        input_mask = None,\n",
    "        attn_bias = None,\n",
    "        window_size = None\n",
    "    ):\n",
    "\n",
    "        mask = default(mask, input_mask)\n",
    "\n",
    "        assert not (exists(window_size) and not self.use_xpos), 'cannot perform window size extrapolation if xpos is not turned on'\n",
    "\n",
    "        shape, autopad, pad_value, window_size, causal, look_backward, look_forward, shared_qk = q.shape, self.autopad, -1, default(window_size, self.window_size), self.causal, self.look_backward, self.look_forward, self.shared_qk\n",
    "\n",
    "        # https://github.com/arogozhnikov/einops/blob/master/docs/4-pack-and-unpack.ipynb\n",
    "        (q, packed_shape), (k, _)= map(lambda t: pack([t], '* n d'), (q, k))  #打包成[5, 8, 512, 64] -> [40, 512, 64] \n",
    "\n",
    "        # auto padding\n",
    "\n",
    "        if autopad:\n",
    "            orig_seq_len = q.shape[1]\n",
    "            (needed_pad, q), (_, k) = map(lambda t: pad_to_multiple(t, self.window_size, dim = -2), (q, k))\n",
    "\n",
    "        b, n, dim_head, device, dtype = *q.shape, q.device, q.dtype   # 40, 512, 64\n",
    "\n",
    "        scale = default(self.scale, dim_head ** -0.5)\n",
    "\n",
    "        assert (n % window_size) == 0, f'sequence length {n} must be divisible by window size {window_size} for local attention'\n",
    "\n",
    "        windows = n // window_size  # 512 / 16 = 32\n",
    "\n",
    "        if shared_qk:\n",
    "            k = l2norm(k)\n",
    "\n",
    "        seq = torch.arange(n, device = device)                  # 0, 1, 2, 3, ..., 511\n",
    "        b_t = rearrange(seq, '(w n) -> 1 w n', w = windows, n = window_size)    # (1, 32, 16) 排序序列变形后的矩阵\n",
    "\n",
    "        # bucketing\n",
    "\n",
    "        bq, bk = map(lambda t: rearrange(t, 'b (w n) d -> b w n d', w = windows), (q, k)) #重构：（40，512，64）->（40, 32, 16, 64）\n",
    "\n",
    "        bq = bq * scale    # (40, 32, 16, 64)\n",
    "\n",
    "        look_around_kwargs = dict(\n",
    "            backward =  look_backward,\n",
    "            forward =  look_forward,\n",
    "            pad_value = pad_value\n",
    "        )\n",
    "\n",
    "        bk = look_around(bk, **look_around_kwargs)      # (40, 32, 32, 64)\n",
    "\n",
    "        # rotary embeddings\n",
    "\n",
    "        if exists(self.rel_pos):\n",
    "            pos_emb, xpos_scale = self.rel_pos(bk)\n",
    "            bq, bk = self.rel_pos.apply_rotary_pos_emb(bq, bk, pos_emb, scale = xpos_scale)\n",
    "\n",
    "        # calculate positions for masking\n",
    "\n",
    "        bq_t = b_t\n",
    "        bq_k = look_around(b_t, **look_around_kwargs) # (1, 32, 32)\n",
    "\n",
    "        bq_t = rearrange(bq_t, '... i -> ... i 1')      # (1, 32, 16, 1)\n",
    "        bq_k = rearrange(bq_k, '... j -> ... 1 j')      # (1, 32, 1, 16)\n",
    "\n",
    "        pad_mask = bq_k == pad_value\n",
    "\n",
    "        sim = einsum('b h i e, b h j e -> b h i j', bq, bk)  # (40, 32, 16, 64) * (40, 32, 32, 64) -> (40, 32, 16, 32)\n",
    "\n",
    "        if exists(attn_bias):\n",
    "            heads = attn_bias.shape[0]\n",
    "            assert (b % heads) == 0\n",
    "\n",
    "            attn_bias = repeat(attn_bias, 'h i j -> (b h) 1 i j', b = b // heads)\n",
    "            sim = sim + attn_bias\n",
    "\n",
    "        mask_value = max_neg_value(sim)\n",
    "\n",
    "        if shared_qk:\n",
    "            self_mask = bq_t == bq_k\n",
    "            sim = sim.masked_fill(self_mask, TOKEN_SELF_ATTN_VALUE)\n",
    "            del self_mask\n",
    "\n",
    "        if causal:\n",
    "            causal_mask = bq_t < bq_k\n",
    "\n",
    "            if self.exact_windowsize:\n",
    "                max_causal_window_size = (self.window_size * self.look_backward)\n",
    "                causal_mask = causal_mask | (bq_t > (bq_k + max_causal_window_size))\n",
    "\n",
    "            sim = sim.masked_fill(causal_mask, mask_value)\n",
    "            del causal_mask\n",
    "\n",
    "        # masking out for exact window size for non-causal\n",
    "        # as well as masking out for padding value\n",
    "\n",
    "        if not causal and self.exact_windowsize:\n",
    "            max_backward_window_size = (self.window_size * self.look_backward)\n",
    "            max_forward_window_size = (self.window_size * self.look_forward)\n",
    "            window_mask = ((bq_k - max_forward_window_size) > bq_t) | (bq_t > (bq_k + max_backward_window_size)) | pad_mask\n",
    "            sim = sim.masked_fill(window_mask, mask_value)\n",
    "        else:\n",
    "            sim = sim.masked_fill(pad_mask, mask_value)\n",
    "\n",
    "        # take care of key padding mask passed in\n",
    "\n",
    "        if exists(mask):\n",
    "            batch = mask.shape[0]    # 5\n",
    "            assert (b % batch) == 0\n",
    "\n",
    "            h = b // mask.shape[0]  # 8\n",
    "\n",
    "            if autopad:\n",
    "                _, mask = pad_to_multiple(mask, window_size, dim = -1, value = False)\n",
    "\n",
    "            mask = rearrange(mask, '... (w n) -> (...) w n', w = windows, n = window_size)\n",
    "            mask = look_around(mask, **{**look_around_kwargs, 'pad_value': False})\n",
    "            mask = rearrange(mask, '... j -> ... 1 j')\n",
    "            mask = repeat(mask, 'b ... -> (b h) ...', h = h)\n",
    "            sim = sim.masked_fill(~mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        # attention\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class localRewirescorelayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, window_size, k, temperature=0.5, dropout=0.1):\n",
    "        super(localRewirescorelayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_features = out_features\n",
    "        self.window_size = window_size\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.k = k\n",
    "        self.Attention = Attention(\n",
    "                                    dim =128,                # dimension of each head (you need to pass this in for relative positional encoding)\n",
    "                                    window_size = 10,       # window size. 512 is optimal, but 256 or 128 yields good enough results\n",
    "                                    causal = None,           # auto-regressive or not\n",
    "                                    look_backward = 1,       # each window looks at the window before\n",
    "                                    look_forward = 1,        # for non-auto-regressive case, will default to 1, so each window looks at the window before and after it\n",
    "                                    dropout = 0.1,           # post-attention dropout\n",
    "                                    exact_windowsize = False # if this is set to true, in the causal setting, each query will see at maximum the number of keys equal to the window size\n",
    "                                )\n",
    "        \n",
    "        self.query = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.key = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.value = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([out_features])).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "\n",
    "    def gumbel_softmax_top_k(self, logits, tau=1.0, hard=False):\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "        gumbels = (logits + gumbels) / tau\n",
    "\n",
    "        y_soft = F.softmax(gumbels, dim=-1)\n",
    "\n",
    "        if hard:\n",
    "            topk_indices = logits.topk(self.k, dim=-1)[1]\n",
    "            y_hard = torch.zeros_like(logits).scatter_(-1, topk_indices, 1.0)\n",
    "            y = (y_hard - y_soft).detach() + y_soft\n",
    "        else:\n",
    "            y = y_soft\n",
    "\n",
    "        return y\n",
    "\n",
    "    def forward(self, graph, node_features):\n",
    "        device = node_features.device\n",
    "        num_relations = graph.num_relation\n",
    "        num_nodes = node_features.size(0)//num_relations\n",
    "    \n",
    "        Q = self.query(node_features).view(num_relations, self.num_heads, num_nodes, self.out_features)     \n",
    "        K = self.key(node_features).view(num_relations,  self.num_heads, num_nodes, self.out_features)      \n",
    "        \n",
    "        output = self.Attention(Q, K)  # [num_nodes, num_heads, 1, max_window_size]\n",
    "\n",
    "        edge_list = self.gumbel_softmax_top_k(output, self.temperature, self.k)\n",
    "\n",
    "        return edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = relational_output.shape[-1]\n",
    "output_dim = 128\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 3\n",
    "num_nodes = relational_output.size(0)\n",
    "\n",
    "\n",
    "relational_output = relational_output.to(device)\n",
    "module = localRewirescorelayer(input_dim, output_dim, num_heads, window_size, k).to(device)\n",
    "start = time.time()\n",
    "attn_output = module(graph, relational_output)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 60, 10, 30])\n",
      "0.05623292922973633\n",
      "(tensor([ 0,  0,  0,  ..., 39, 39, 39]), tensor([ 0,  0,  0,  ..., 59, 59, 59]), tensor([0, 0, 0,  ..., 9, 9, 9]), tensor([11, 16, 17,  ...,  3, 11, 13]))\n"
     ]
    }
   ],
   "source": [
    "print(attn_output.shape)\n",
    "print(end-start)\n",
    "indices = torch.nonzero(attn_output, as_tuple=True)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class test(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, window_size, k, temperature=0.5, dropout=0.1):\n",
    "        super(test, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_features = out_features\n",
    "        self.window_size = window_size\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.k = k\n",
    "        \n",
    "        self.query = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.key = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.value = nn.Linear(in_features, out_features * num_heads)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([out_features])).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    # get the start and end indices for each window of nodes\n",
    "    def get_start_end(self, current, graph):\n",
    "        segment = graph.num_nodes.repeat(graph.num_relation)\n",
    "        index = torch.cumsum(segment, dim=0)\n",
    "        \n",
    "        # Use torch.searchsorted to find the appropriate segment\n",
    "        pos = torch.searchsorted(index, current, right=True)\n",
    "\n",
    "        if pos == 0:\n",
    "            return (0, index[0].item())\n",
    "        elif pos >= len(index):\n",
    "            return (index[-1].item(), index[-1].item())\n",
    "        else:\n",
    "            return (index[pos-1].item(), index[pos].item())\n",
    "\n",
    "    def gumbel_softmax_top_k(self, logits, tau=1.0, hard=False):\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "        gumbels = (logits + gumbels) / tau\n",
    "\n",
    "        y_soft = F.softmax(gumbels, dim=-1)\n",
    "\n",
    "        if hard:\n",
    "            topk_indices = logits.topk(self.k, dim=-1)[1]\n",
    "            y_hard = torch.zeros_like(logits).scatter_(-1, topk_indices, 1.0)\n",
    "            y = (y_hard - y_soft).detach() + y_soft\n",
    "        else:\n",
    "            y = y_soft\n",
    "\n",
    "        return y\n",
    "\n",
    "    def forward(self, graph, node_features):\n",
    "        device = node_features.device\n",
    "        num_nodes = node_features.size(0)\n",
    "    \n",
    "        # calculate the start and end indices for each node\n",
    "        half_window = self.window_size // 2\n",
    "        start_end_indices = [self.get_start_end(i, graph) for i in range(num_nodes)]\n",
    "        start_indices = torch.tensor([max(start_end_indices[i][0], i - half_window) for i in range(num_nodes)], device=device)\n",
    "        end_indices = torch.tensor([min(start_end_indices[i][1], i + half_window) for i in range(num_nodes)], device=device)\n",
    "        window_sizes = end_indices - start_indices\n",
    "\n",
    "        Q = self.query(node_features).view(self.num_heads, num_nodes, self.out_features)     # [num_heads, num_nodes, out_features]\n",
    "        K = self.key(node_features).view(self.num_heads, num_nodes, self.out_features)       # [num_heads, num_nodes, out_features]\n",
    "        \n",
    "        output = torch.zeros(num_nodes, num_nodes, device=device)\n",
    "        all_scores = torch.zeros(num_nodes, self.num_heads, self.window_size, device=device)\n",
    "        \n",
    "        # construct the windowed K matrix\n",
    "        K_windows = torch.zeros((num_nodes, self.window_size, self.num_heads, self.out_features), device=device)\n",
    "        for i in range(num_nodes):\n",
    "            start = start_indices[i]\n",
    "            end = end_indices[i]\n",
    "            K_windows[i, :end-start] = K[:, start:end]\n",
    "            \n",
    "\n",
    "\n",
    "        Q_expanded = Q.unsqueeze(2)  # [num_nodes, num_heads, 1, out_features]\n",
    "        K_expanded = K_windows.permute(0, 2, 1, 3)  # [num_nodes, num_heads, max_window_size, out_features]\n",
    "\n",
    "\n",
    "        all_scores = torch.matmul(Q_expanded, K_expanded.transpose(-1, -2)) / self.scale  # [num_nodes, num_heads, 1, max_window_size]\n",
    "        all_scores = all_scores.squeeze(2)  # [num_nodes, num_heads, max_window_size]\n",
    "        \n",
    "\n",
    "        mask = (torch.arange(self.window_size, device=device).expand(num_nodes, self.window_size) < window_sizes.unsqueeze(1))\n",
    "\n",
    "\n",
    "        all_scores = all_scores.masked_fill(~mask.unsqueeze(1), float('-inf'))  # [num_nodes, num_heads, max_window_size]\n",
    "        attention_weights = F.softmax(all_scores / self.temperature, dim=-1)  # [num_nodes, num_heads, max_window_size]\n",
    "        attention_weights = attention_weights.mean(dim=1) \n",
    "        \n",
    "        # sample edges\n",
    "        for i in range(num_nodes):\n",
    "            start = start_indices[i]\n",
    "            end = end_indices[i]\n",
    "            output[i, start:end] = attention_weights[i, :end-start]\n",
    "\n",
    "        edge_list = self.gumbel_softmax_top_k(output, self.temperature, self.k)\n",
    "\n",
    "        return edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3000, 3000])\n"
     ]
    }
   ],
   "source": [
    "input_dim = relational_output.shape[-1]\n",
    "output_dim = 128\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 3\n",
    "num_nodes = relational_output.size(0)\n",
    "\n",
    "\n",
    "relational_output = relational_output.to(device)\n",
    "module = test(input_dim, output_dim, num_heads, window_size, k).to(device)\n",
    "attn_output = module(graph, relational_output)\n",
    "print(attn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([185, 600])\n"
     ]
    }
   ],
   "source": [
    "input= torch.randn(56, 600, 512)\n",
    "\n",
    "segment = graph.num_nodes\n",
    "index = torch.cumsum(segment, dim=0)\n",
    "window_size = 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "def split_tensor(tensor, index, window_size):\n",
    "    result = []\n",
    "    index_list  = []\n",
    "    # 初始的开始索引\n",
    "    start = 0\n",
    "\n",
    "    for idx in index:\n",
    "        end = start + idx - 1\n",
    "        while start < end:\n",
    "            if start + window_size <= end:\n",
    "                result.append(tensor[:, start:start+window_size, :])\n",
    "                index_list.append([start, start+window_size])\n",
    "                start += window_size\n",
    "            else:\n",
    "                # 计算还需要填充多少行\n",
    "                padding_rows = window_size - (end - start + 1)\n",
    "                restart = start - padding_rows\n",
    "                result.append(tensor[:, restart:restart + window_size , :])\n",
    "                index_list.append([restart, restart+window_size])\n",
    "                start = end + 1\n",
    "                \n",
    "\n",
    "    # 转换结果列表为 tensor\n",
    "    result_tensor = torch.stack(result, dim=1)\n",
    "    return result_tensor, index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间: 0.014563 秒\n",
      "torch.Size([56, 61, 10, 512])\n",
      "[[0, 10], [10, 20], [20, 30], [30, 40], [40, 50], [50, 60], [60, 70], [70, 80], [80, 90], [90, 100], [100, 110], [110, 120], [120, 130], [130, 140], [140, 150], [150, 160], [160, 170], [170, 180], [175, 185], [185, 195], [195, 205], [205, 215], [215, 225], [225, 235], [235, 245], [245, 255], [255, 265], [265, 275], [275, 285], [285, 295], [295, 305], [305, 315], [315, 325], [325, 335], [335, 345], [345, 355], [355, 365], [365, 375], [375, 385], [385, 395], [395, 405], [405, 415], [415, 425], [425, 435], [435, 445], [445, 455], [455, 465], [465, 475], [475, 485], [485, 495], [495, 505], [505, 515], [515, 525], [525, 535], [535, 545], [545, 555], [555, 565], [565, 575], [575, 585], [585, 595], [590, 600]]\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "# 示例输入\n",
    "tensor = torch.randn(56, 600, 512, requires_grad=True)\n",
    "index = [185, 415]\n",
    "window_size = 10\n",
    "\n",
    "start_time = time.time()\n",
    "result, index_list = split_tensor(tensor, index, window_size)\n",
    "end_time = time.time()\n",
    "\n",
    "# 打印运行时间\n",
    "print(f\"运行时间: {end_time - start_time:.6f} 秒\")\n",
    "print(result.shape)  # 应输出 (56, n, 10, 512)\n",
    "\n",
    "# 打印索引列表\n",
    "print(index_list)\n",
    "print(len(index_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "atten_output = torch.randn(56, 61, 10, 10, device=device)\n",
    "output = torch.zeros(56, 600, 600, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[8, 4200, 4200]' is invalid for input of size 20160000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     output[:, start:end, start:end] \u001b[38;5;241m=\u001b[39m atten_output[:, i, :, :]\n\u001b[1;32m      5\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 示例输出\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m运行时间: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 秒\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[8, 4200, 4200]' is invalid for input of size 20160000"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "for i, index in enumerate(index_list):\n",
    "    start, end = index\n",
    "    output[:, start:end, start:end] = atten_output[:, i, :, :]\n",
    "end_time = time.time()\n",
    "output = output.view(8, 4200, 4200)\n",
    "# 示例输出\n",
    "print(f\"运行时间: {end_time - start_time:.6f} 秒\")\n",
    "print(output.shape)\n",
    "print(output)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gearnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
