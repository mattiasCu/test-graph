{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前期准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cu/anaconda3/envs/gearnet/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cu/anaconda3/envs/gearnet/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from torchdrug import  layers, datasets,transforms,core\n",
    "from torchdrug.core import Registry as R\n",
    "from torchdrug.layers import geometry\n",
    "\n",
    "import torch\n",
    "from torchdrug import data\n",
    "\n",
    "from torch_scatter import scatter_add\n",
    "import torch.nn as nn\n",
    "from torchdrug import utils\n",
    "from torch.utils import checkpoint\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:40:02   Extracting /home/cu/scratch/protein-datasets/EnzymeCommission.zip to /home/cu/scratch/protein-datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/cu/scratch/protein-datasets/EnzymeCommission/enzyme_commission.pkl.gz: 100%|██████████| 18716/18716 [00:22<00:00, 831.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "PackedProtein(batch_size=2, num_atoms=[185, 415], num_bonds=[3388, 8173], num_residues=[185, 415])\n"
     ]
    }
   ],
   "source": [
    "EnzymeCommission = R.search(\"datasets.EnzymeCommission\")\n",
    "PV = R.search(\"transforms.ProteinView\")\n",
    "trans = PV(view = \"residue\")\n",
    "dataset = EnzymeCommission(\"~/scratch/protein-datasets/\", test_cutoff=0.95, \n",
    "                           atom_feature=\"full\", bond_feature=\"full\", verbose=1, transform = trans)\n",
    "\n",
    "# 只保留alpha碳的简化格式\n",
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()], \n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(k=10, min_distance=5),\n",
    "                                                                 geometry.SequentialEdge(max_distance=1)\n",
    "                                                                 ],\n",
    "                                                    edge_feature=\"gearnet\"\n",
    "                                                    )\n",
    "\n",
    "\n",
    "graphs = dataset[:2]\n",
    "graphs = [element[\"graph\"] for element in graphs]\n",
    "graphs = data.Protein.pack(graphs)\n",
    "print(\"\\n\\n\")\n",
    "graph = graph_construction_model(graphs)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationalGraph(layers.MessagePassingBase):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(relationalGraph, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_relation = num_relation\n",
    "        self.edge_input_dim = edge_input_dim\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        if edge_input_dim:\n",
    "            self.edge_linear = nn.Linear(edge_input_dim, input_dim)\n",
    "        else:\n",
    "            self.edge_linear = None\n",
    "            \n",
    "    def trans(self, A, graph):\n",
    "    \n",
    "        Degree_inv_sqrt = torch.diag(torch.pow(torch.sum(A, dim=1), -0.5))\n",
    "        A_norm = torch.mm(torch.mm(Degree_inv_sqrt, A), Degree_inv_sqrt)\n",
    "        \n",
    "        n_rel = graph.num_relation\n",
    "        n = A_norm.size(0)\n",
    "        n_rel = n_rel.item()  # 将 n_rel 从 Tensor 转换为 int\n",
    "        assert n % n_rel == 0, \"n must be divisible by n_rel\"\n",
    "        \n",
    "        block_size = n // n_rel\n",
    "        \n",
    "        # 初始化一个张量来存储累加结果\n",
    "        accumulated = torch.zeros_like(A_norm[:block_size])\n",
    "        \n",
    "        # 将后面的所有块累加到第一块\n",
    "        for i in range(n_rel):\n",
    "            accumulated += A_norm[i * block_size: (i + 1) * block_size]\n",
    "        \n",
    "        # 用累加后的第一块替换原始矩阵的第一块\n",
    "        A_trans = accumulated\n",
    "    \n",
    "        return A_trans\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "        device = input.device  # Ensure device consistency\n",
    "        \n",
    "        if new_edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "            node_out = node_out * self.num_relation + relation\n",
    "        \n",
    "            edge_weight = torch.ones_like(node_out)\n",
    "            degree_out = scatter_add(edge_weight, node_out, dim_size=graph.num_node * graph.num_relation)\n",
    "            degree_out = degree_out\n",
    "            edge_weight = edge_weight / degree_out[node_out]\n",
    "            adjacency = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), edge_weight,\n",
    "                                                (graph.num_node, graph.num_node * graph.num_relation))\n",
    "            update = torch.sparse.mm(adjacency.t(), input)\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(edge_input * edge_weight, node_out, dim=0,\n",
    "                                      dim_size=graph.num_node * graph.num_relation)\n",
    "            update += edge_update\n",
    "\n",
    "        return update\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        # 自环特征\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        input = input.repeat(self.num_relation, 1).to(device)\n",
    "        loop_update = self.self_loop(input).to(device)\n",
    "        \n",
    "        output = self.linear(update)+loop_update\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        \n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self._message_and_aggregate, *graph.to_tensors(), input)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph, input, new_edge_list)\n",
    "        output = self.combine(input, update).view(graph.num_relation, input.size(0), -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0504, 0.0000, 0.4579,  ..., 0.0000, 0.0063, 0.3589],\n",
      "         [0.0000, 0.0000, 0.0664,  ..., 0.0092, 0.0000, 0.2898],\n",
      "         [0.2564, 0.0778, 0.6395,  ..., 0.0000, 0.0000, 0.2653],\n",
      "         ...,\n",
      "         [0.0000, 0.1378, 0.4195,  ..., 0.0000, 0.0000, 0.3237],\n",
      "         [0.1255, 0.0667, 0.2849,  ..., 0.0000, 0.0000, 0.2467],\n",
      "         [0.0073, 0.0000, 0.0689,  ..., 0.0589, 0.0401, 0.3647]],\n",
      "\n",
      "        [[0.0000, 0.0292, 0.4592,  ..., 0.0000, 0.0000, 0.3837],\n",
      "         [0.0000, 0.0165, 0.2323,  ..., 0.1008, 0.0000, 0.2555],\n",
      "         [0.0000, 0.0338, 0.3991,  ..., 0.0000, 0.0000, 0.3721],\n",
      "         ...,\n",
      "         [0.2455, 0.0450, 0.2922,  ..., 0.0449, 0.0000, 0.4263],\n",
      "         [0.1255, 0.0667, 0.2849,  ..., 0.0000, 0.0000, 0.2467],\n",
      "         [0.1074, 0.0109, 0.1830,  ..., 0.2791, 0.0126, 0.3196]],\n",
      "\n",
      "        [[0.0132, 0.0319, 0.4318,  ..., 0.0000, 0.0000, 0.3909],\n",
      "         [0.0000, 0.1368, 0.0188,  ..., 0.0974, 0.0000, 0.2854],\n",
      "         [0.2005, 0.0696, 0.5348,  ..., 0.0295, 0.0000, 0.5922],\n",
      "         ...,\n",
      "         [0.0000, 0.1378, 0.4195,  ..., 0.0000, 0.0000, 0.3237],\n",
      "         [0.1737, 0.0000, 0.7062,  ..., 0.0000, 0.0000, 0.2893],\n",
      "         [0.2835, 0.0000, 0.2889,  ..., 0.0000, 0.0000, 0.3062]],\n",
      "\n",
      "        [[0.0000, 0.0042, 0.4617,  ..., 0.0000, 0.0000, 0.4582],\n",
      "         [0.0000, 0.0000, 0.0886,  ..., 0.0000, 0.0000, 0.5673],\n",
      "         [0.0491, 0.0000, 0.6469,  ..., 0.0000, 0.0000, 0.2837],\n",
      "         ...,\n",
      "         [0.0491, 0.0000, 0.6469,  ..., 0.0000, 0.0000, 0.2837],\n",
      "         [0.2005, 0.0696, 0.5348,  ..., 0.0295, 0.0000, 0.5922],\n",
      "         [0.0000, 0.0000, 0.2742,  ..., 0.0000, 0.0000, 0.2519]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.4893,  ..., 0.0000, 0.0000, 0.4139],\n",
      "         [0.0000, 0.0000, 0.1564,  ..., 0.0806, 0.0000, 0.3279],\n",
      "         [0.1255, 0.0667, 0.2849,  ..., 0.0000, 0.0000, 0.2467],\n",
      "         ...,\n",
      "         [0.0728, 0.0000, 0.4935,  ..., 0.0000, 0.0000, 0.3891],\n",
      "         [0.0000, 0.0000, 0.3932,  ..., 0.0000, 0.0000, 0.4801],\n",
      "         [0.0000, 0.0000, 0.2742,  ..., 0.0000, 0.0000, 0.2519]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "output:  torch.Size([5, 600, 128])\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 128\n",
    "num_relations = graph.num_relation\n",
    "\n",
    "relational_output = relationalGraph(input_dim, output_dim, num_relations)(graph, graph.node_feature.float(), new_edge_list = None)\n",
    "print(relational_output)\n",
    "print(\"output: \", relational_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# local attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class test(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, window_size, k, temperature=0.5, dropout=0.1):\n",
    "        super(test, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_features = out_features\n",
    "        self.window_size = window_size\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.k = k\n",
    "        \n",
    "        self.query = nn.Linear( in_features, out_features* num_heads)\n",
    "        self.key = nn.Linear( in_features, out_features* num_heads)\n",
    "        self.scale = 1 / (out_features ** 0.5)\n",
    "\n",
    "    def split_windows(self, tensor, index, window_size):\n",
    "        result = []\n",
    "        index_list  = []\n",
    "        # 初始的开始索引\n",
    "        start = 0\n",
    "\n",
    "        for idx in index:\n",
    "            end = start + idx - 1\n",
    "            while start < end:\n",
    "                if start + window_size <= end:\n",
    "                    result.append(tensor[:, start:start+window_size, :])\n",
    "                    index_list.append([start, start+window_size])\n",
    "                    start += window_size\n",
    "                else:\n",
    "                    # 计算还需要填充多少行\n",
    "                    padding_rows = window_size - (end - start + 1)\n",
    "                    restart = start - padding_rows\n",
    "                    result.append(tensor[:, restart:restart + window_size , :])\n",
    "                    index_list.append([restart, restart+window_size])\n",
    "                    start = end + 1\n",
    "                    \n",
    "        # 转换结果列表为 tensor\n",
    "        result_tensor = torch.stack(result, dim=1)\n",
    "        return result_tensor, index_list\n",
    "\n",
    "    def gumbel_softmax_top_k(self, logits, tau=1.0, hard=False):\n",
    "        gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "        gumbels = (logits + gumbels) / tau\n",
    "\n",
    "        y_soft = F.softmax(gumbels, dim=-1)\n",
    "\n",
    "        if hard:\n",
    "            topk_indices = logits.topk(self.k, dim=-1)[1]\n",
    "            y_hard = torch.zeros_like(logits).scatter_(-1, topk_indices, 1.0)\n",
    "            y = (y_hard - y_soft).detach() + y_soft\n",
    "        else:\n",
    "            y = y_soft\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def windows2adjacent(self, windows, index_list, output):\n",
    "    \n",
    "        for i, index in enumerate(index_list):\n",
    "            start, end = index\n",
    "            output[:, start:end, start:end] = windows[:, i, :, :]\n",
    "            \n",
    "        num_relations, num_nodes, _ = output.shape\n",
    "        result = torch.zeros(num_relations* num_nodes, num_relations*num_nodes)\n",
    "        for i in range(num_relations):\n",
    "            result[i*num_nodes:(i+1)*num_nodes, i*num_nodes:(i+1)*num_nodes] = output[i]\n",
    "        return result\n",
    "\n",
    "    def forward(self, graph, node_features):\n",
    "        device = node_features.device\n",
    "        num_relations = node_features.size(0)\n",
    "        num_nodes = node_features.size(1)\n",
    "        index = graph.num_nodes.tolist()\n",
    "        \n",
    "        Q = self.query(node_features).view(num_relations, num_nodes, self.num_heads, self.out_features).permute(0, 2, 1, 3)\n",
    "        K = self.key(node_features).view(num_relations, num_nodes, self.num_heads, self.out_features).permute(0, 2, 1, 3)   \n",
    "        Q = Q.reshape(num_relations * self.num_heads, num_nodes, self.out_features)\n",
    "        K = K.reshape(num_relations * self.num_heads, num_nodes, self.out_features)\n",
    "        \n",
    "        output = torch.zeros(num_relations, num_nodes, num_nodes)\n",
    "       \n",
    "        Q_windows, Q_index = self.split_windows(Q, index, self.window_size)\n",
    "        K_windows, _ = self.split_windows(K, index, self.window_size)\n",
    "        \n",
    "        # 计算 attention\n",
    "        scores  = torch.einsum('b h i e, b h j e -> b h i j', Q_windows, K_windows) / self.scale\n",
    "        attn = scores.softmax(dim=-1).view(num_relations, self.num_heads, -1, self.window_size, self.window_size).mean(dim=1)\n",
    "        attn = self.gumbel_softmax_top_k(attn, tau=self.temperature, hard=True)\n",
    "        \n",
    "        result = self.windows2adjacent(attn, Q_index, output)\n",
    "        \n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间: 0.029165 秒\n"
     ]
    }
   ],
   "source": [
    "input_dim = relational_output.shape[-1]\n",
    "output_dim = 128\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "k = 5\n",
    "num_nodes = relational_output.size(0)\n",
    "\n",
    "\n",
    "relational_output = relational_output.to(device)\n",
    "start_time = time.time()\n",
    "module = test(input_dim, output_dim, num_heads, window_size, k).to(device)\n",
    "attn_output = module(graph, relational_output)\n",
    "end_time = time.time()\n",
    "print(f\"运行时间: {end_time - start_time:.6f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3000, 3000])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.]], grad_fn=<CopySlices>)\n",
      "(tensor([   0,    0,    0,  ..., 2999, 2999, 2999]), tensor([   3,    4,    5,  ..., 2995, 2998, 2999]))\n"
     ]
    }
   ],
   "source": [
    "attn_output = module(graph, relational_output)\n",
    "print(attn_output.shape)\n",
    "print(attn_output)\n",
    "#a = attn_output[1200:, 599:1199]\n",
    "indices = torch.nonzero(attn_output, as_tuple=True)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 创建一个示例矩阵 (可以是随机矩阵或其他数据)\n",
    "matrix = attn_output.cpu().detach().numpy()\n",
    "\n",
    "# 绘制矩阵为图像\n",
    "plt.imshow(matrix, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar()  # 显示颜色条\n",
    "plt.title(\"Matrix as Image\")\n",
    "plt.xlabel(\"Column\")\n",
    "plt.ylabel(\"Row\")\n",
    "plt.savefig(\"fig/rewire.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gearnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
