{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdrug import  layers, datasets,transforms,core\n",
    "from torchdrug.core import Registry as R\n",
    "from torchdrug.layers import geometry\n",
    "\n",
    "import torch\n",
    "from torchdrug import data\n",
    "\n",
    "from torch_scatter import scatter_add\n",
    "import torch.nn as nn\n",
    "from torchdrug import utils\n",
    "from torch.utils import checkpoint\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from einops import rearrange, repeat, pack, unpack\n",
    "\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    raise ValueError(\"CUDA device 0 is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:49:04   Extracting /home/xiaotong/scratch/protein-datasets/EnzymeCommission.zip to /home/xiaotong/scratch/protein-datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/xiaotong/scratch/protein-datasets/EnzymeCommission/enzyme_commission.pkl.gz: 100%|██████████| 18716/18716 [00:49<00:00, 377.87it/s]\n"
     ]
    }
   ],
   "source": [
    "EnzymeCommission = R.search(\"datasets.EnzymeCommission\")\n",
    "PV = R.search(\"transforms.ProteinView\")\n",
    "trans = PV(view = \"residue\")\n",
    "dataset = EnzymeCommission(\"~/scratch/protein-datasets/\", test_cutoff=0.95, \n",
    "                           atom_feature=\"full\", bond_feature=\"full\", verbose=1, transform = trans)\n",
    "\n",
    "# 只保留alpha碳的简化格式\n",
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()], \n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(k=10, min_distance=5),\n",
    "                                                                 geometry.SequentialEdge(max_distance=2)\n",
    "                                                                 ],\n",
    "                                                    edge_feature=\"gearnet\"\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "PackedProtein(batch_size=4, num_atoms=[437, 437, 332, 224], num_bonds=[8141, 8207, 6874, 5209], num_residues=[437, 437, 332, 224])\n"
     ]
    }
   ],
   "source": [
    "graphs = dataset[4:8]\n",
    "graphs = [element[\"graph\"] for element in graphs]\n",
    "graphs = data.Protein.pack(graphs)\n",
    "print(\"\\n\\n\")\n",
    "graph = graph_construction_model(graphs)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([437, 437, 332, 224])\n",
      "4\n",
      "tensor([[ 132,  131,    3],\n",
      "        [ 147,  146,    3],\n",
      "        [ 146,  145,    3],\n",
      "        ...,\n",
      "        [1262, 1429,    0],\n",
      "        [1260, 1429,    0],\n",
      "        [1263, 1429,    0]])\n"
     ]
    }
   ],
   "source": [
    "print(graph.num_nodes)\n",
    "print(graph.batch_size)\n",
    "print(graph.edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationnal conv graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationalGraph(layers.MessagePassingBase):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(relationalGraph, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_relation = num_relation\n",
    "        self.edge_input_dim = edge_input_dim\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        if edge_input_dim:\n",
    "            self.edge_linear = nn.Linear(edge_input_dim, input_dim)\n",
    "        else:\n",
    "            self.edge_linear = None\n",
    "            \n",
    "    def trans(self, A, graph):\n",
    "        n_rel = graph.num_relation\n",
    "        A = A.view(A.size(0),n_rel, A.size(1)//n_rel).permute(1, 0, 2)\n",
    "       # 初始化结果张量\n",
    "        A_norm = torch.zeros_like(A)\n",
    "\n",
    "        # 对每个 (1024, 1024) 矩阵进行处理\n",
    "        for i in range(A.size(0)):\n",
    "            # 计算度矩阵 (按行求和)\n",
    "            degree = A[i].sum(dim=1)\n",
    "            \n",
    "            # 计算度矩阵的逆平方根\n",
    "            degree_inv_sqrt = torch.pow(degree, -0.5)\n",
    "            \n",
    "            # 将度矩阵逆平方根转换为对角矩阵\n",
    "            Degree_inv_sqrt = torch.diag(degree_inv_sqrt)\n",
    "            \n",
    "            # 进行归一化操作\n",
    "            A_norm[i] = torch.mm(torch.mm(Degree_inv_sqrt, A[i]), Degree_inv_sqrt)\n",
    "        Anorm = A_norm.permute(1, 0, 2).contiguous().view(A_norm.size(1), A_norm.size(0)*A_norm.size(2))\n",
    "    \n",
    "        return Anorm\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "        device = input.device  # Ensure device consistency\n",
    "        \n",
    "        if new_edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "            node_out = node_out * self.num_relation + relation\n",
    "        \n",
    "            degree_out = scatter_add(graph.edge_weight, node_out, dim_size=graph.num_node * graph.num_relation)\n",
    "            edge_weight = graph.edge_weight / degree_out[node_out]\n",
    "            adjacency = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), edge_weight,\n",
    "                                                (graph.num_node, graph.num_node * graph.num_relation))\n",
    "            update = torch.sparse.mm(adjacency.t().to(device), input.to(device))\n",
    "        else:\n",
    "            adjacency = self.trans(new_edge_list, graph).to(device)\n",
    "            update = torch.mm(adjacency.t().to(device), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(edge_input * edge_weight, node_out, dim=0,\n",
    "                                      dim_size=graph.num_node * graph.num_relation)\n",
    "            update += edge_update\n",
    "\n",
    "        return update\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        # 自环特征\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        input = input.repeat(self.num_relation, 1).to(device)\n",
    "        loop_update = self.self_loop(input).to(device)\n",
    "        \n",
    "        output = self.linear(update)+loop_update\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        device = input.device\n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self._message_and_aggregate, *graph.to_tensors(), input, new_edge_list)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph.to(device), input, new_edge_list)\n",
    "        output = self.combine(input, update).view(graph.num_relation, input.size(0), -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  torch.Size([7, 1430, 128])\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 128\n",
    "num_relations = graph.num_relation\n",
    "\n",
    "input = graph.node_feature.float().to(device)\n",
    "model = relationalGraph(input_dim, output_dim, num_relations)\n",
    "relational_output = model(graph, input, new_edge_list = None)\n",
    "print(\"output: \", relational_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local attention for rewire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewirescorelayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, window_size, k, temperature=0.5):\n",
    "        super(Rewirescorelayer, self).__init__()\n",
    "        self.input_dim = in_features\n",
    "        self.output_dim = out_features\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.k = k\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.query = nn.Linear(in_features, out_features * num_heads).to(device)\n",
    "        self.key = nn.Linear(in_features, out_features * num_heads).to(device)\n",
    "        self.scale = 1 / (out_features ** 0.5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    class LocalAttention(nn.Module):\n",
    "        def __init__(\n",
    "            self,\n",
    "            window_size,\n",
    "            look_backward = 1,\n",
    "            look_forward = None,\n",
    "            dim = None,\n",
    "            scale = None,\n",
    "            pad_start_position = None\n",
    "        ):\n",
    "            super().__init__()\n",
    "\n",
    "            self.scale = scale\n",
    "\n",
    "            self.window_size = window_size\n",
    "\n",
    "            self.look_backward = look_backward\n",
    "            self.look_forward = look_forward\n",
    "            \n",
    "            self.pad_start_position = pad_start_position\n",
    "\n",
    "        def exists(self,val):\n",
    "            return val is not None\n",
    "\n",
    "        # 如果value不存在，返回d\n",
    "        def default(self,value, d):\n",
    "            return d if not self.exists(value) else value\n",
    "\n",
    "        def to(self, t):\n",
    "            return {'device': t.device, 'dtype': t.dtype}\n",
    "\n",
    "        def max_neg_value(self, tensor):\n",
    "            return -torch.finfo(tensor.dtype).max  #返回给定张量数据类型的所能表示的最大负值\n",
    "\n",
    "        def look_around(self, x, backward = 1, forward = 0, pad_value = -1, dim = 2):  #x = bk: (40, 32, 16, 64)\n",
    "            t = x.shape[1]    #获取一共有多少个窗口，这里是32\n",
    "            dims = (len(x.shape) - dim) * (0, 0)   #一个长度为 len(x.shape) - dim 的元组，每个元素为 (0, 0)；其中len(x.shape) = 4\n",
    "            padded_x = F.pad(x, (*dims, backward, forward), value = pad_value)   #在第二维度上，前面加backward个元素，后面加forward个元素 -> (40, 33, 16, 64)\n",
    "            tensors = [padded_x[:, ind:(ind + t), ...] for ind in range(forward + backward + 1)] #一个张量列表，每个张量的维度为(40, 32, 16, 64), len = 2\n",
    "            return torch.cat(tensors, dim = dim) #在第二维度上拼接 -> (40, 32, 32, 64)\n",
    "    \n",
    "        def forward(\n",
    "            self,\n",
    "            q, k,\n",
    "            mask = None,\n",
    "            input_mask = None,\n",
    "            window_size = None\n",
    "        ):\n",
    "\n",
    "            mask = self.default(mask, input_mask)\n",
    "            assert not (self.exists(window_size) and not self.use_xpos), 'cannot perform window size extrapolation if xpos is not turned on'\n",
    "            shape, pad_value, window_size, look_backward, look_forward = q.shape, -1, self.default(window_size, self.window_size), self.look_backward, self.look_forward\n",
    "            (q, packed_shape), (k, _) = map(lambda t: pack([t], '* n d'), (q, k))  #打包成[5, 8, 512, 64] -> [40, 512, 64] \n",
    "\n",
    "\n",
    "            b, n, dim_head, device, dtype = *q.shape, q.device, q.dtype   # 40, 512, 64\n",
    "            scale = self.default(self.scale, dim_head ** -0.5)\n",
    "            assert (n % window_size) == 0, f'sequence length {n} must be divisible by window size {window_size} for local attention'\n",
    "\n",
    "            windows = n // window_size  # 512 / 16 = 32\n",
    "\n",
    "            seq = torch.arange(n, device = device)                  # 0, 1, 2, 3, ..., 511\n",
    "            b_t = rearrange(seq, '(w n) -> 1 w n', w = windows, n = window_size)    # (1, 32, 16) 排序序列变形后的矩阵\n",
    "\n",
    "            # bucketing\n",
    "\n",
    "            bq, bk = map(lambda t: rearrange(t, 'b (w n) d -> b w n d', w = windows), (q, k)) #重构：（40，512，64）->（40, 32, 16, 64）\n",
    "\n",
    "            bq = bq * scale    # (40, 32, 16, 64)\n",
    "\n",
    "            look_around_kwargs = dict(\n",
    "                backward =  look_backward,\n",
    "                forward =  look_forward,\n",
    "                pad_value = pad_value\n",
    "            )\n",
    "\n",
    "            bk = self.look_around(bk, **look_around_kwargs)      # (40, 32, 32, 64)\n",
    "    \n",
    "\n",
    "            # calculate positions for masking\n",
    "\n",
    "            bq_t = b_t\n",
    "            bq_k = self.look_around(b_t, **look_around_kwargs) # (1, 32, 32)\n",
    "\n",
    "            bq_t = rearrange(bq_t, '... i -> ... i 1')      # (1, 32, 16, 1)\n",
    "            bq_k = rearrange(bq_k, '... j -> ... 1 j')      # (1, 32, 1, 16)\n",
    "\n",
    "            pad_mask = bq_k == pad_value\n",
    "\n",
    "            sim = torch.einsum('b h i e, b h j e -> b h i j', bq, bk)  # (40, 32, 16, 64) * (40, 32, 32, 64) -> (40, 32, 16, 32)\n",
    "\n",
    "            mask_value = self.max_neg_value(sim)\n",
    "\n",
    "            sim = sim.masked_fill(pad_mask, mask_value)\n",
    "\n",
    "\n",
    "            if self.exists(mask):\n",
    "                batch = mask.shape[0]    # 5\n",
    "                assert (b % batch) == 0\n",
    "\n",
    "                h = b // mask.shape[0]  # 8\n",
    "\n",
    "                mask = rearrange(mask, '... (w n) -> (...) w n', w = windows, n = window_size)\n",
    "                mask = self.look_around(mask, **{**look_around_kwargs, 'pad_value': False})\n",
    "                mask = rearrange(mask, '... j -> ... 1 j')\n",
    "                mask = repeat(mask, 'b ... -> (b h) ...', h = h)\n",
    "\n",
    "                sim = sim.masked_fill(~mask, mask_value)\n",
    "                del mask\n",
    "                \n",
    "            indices = [self.pad_start_position[i] // window_size for i in range(len(self.pad_start_position)) if i % 2 != 0]\n",
    "            all_indices = list(range(windows))\n",
    "            remaining_indices = [idx for idx in all_indices if idx not in indices]\n",
    "            \n",
    "            # 使用剩余的索引选择元素\n",
    "            rest_sim = sim[:, remaining_indices, :, :]\n",
    "\n",
    "            # attention\n",
    "            attn = rest_sim.softmax(dim = -1)\n",
    "\n",
    "            return attn\n",
    "\n",
    "    def insert_zero_rows(self, tensor, lengths, target_lengths):\n",
    "        assert len(lengths) == len(target_lengths), \"Lengths and target lengths must be of the same length.\"\n",
    "        \n",
    "        # 计算每个位置需要插入的零行数\n",
    "        zero_rows = [target - length for length, target in zip(lengths, target_lengths)]\n",
    "        \n",
    "        # 初始化结果列表\n",
    "        parts = []\n",
    "        mask_parts = []\n",
    "        start = 0\n",
    "        \n",
    "        for i, length in enumerate(lengths):\n",
    "            end = start + length\n",
    "            \n",
    "            # 原始张量部分\n",
    "            parts.append(tensor[:, start:end, :])\n",
    "            mask_parts.append(torch.ones(tensor.size(0), length, dtype=torch.bool, device=tensor.device))\n",
    "            \n",
    "            # 插入零行\n",
    "            if zero_rows[i] > 0:\n",
    "                zero_padding = torch.zeros(tensor.size(0), zero_rows[i], tensor.size(2), device=tensor.device)\n",
    "                mask_padding = torch.zeros(tensor.size(0), zero_rows[i], dtype=torch.bool, device=tensor.device)\n",
    "                parts.append(zero_padding)\n",
    "                mask_parts.append(mask_padding)\n",
    "            \n",
    "            start = end\n",
    "        \n",
    "        # 拼接所有部分\n",
    "        padded_tensor = torch.cat(parts, dim=1)\n",
    "        mask = torch.cat(mask_parts, dim=1)\n",
    "        \n",
    "        return padded_tensor, mask\n",
    "\n",
    "\n",
    "    def round_up_to_nearest_k_and_a_window_size(self, lst, k):\n",
    "        pad_start_position = []\n",
    "        result_lst = [(x + k - 1) // k * k +k for x in lst]\n",
    "        for i in range(len(lst)):\n",
    "            pad_start_position.append(sum(result_lst[:i])-i*k + lst[i])\n",
    "            pad_start_position.append(sum(result_lst[:i+1])-k)\n",
    "        return result_lst, pad_start_position\n",
    "\n",
    "    def gumbel_softmax_top_k(self, logits,  top_k,  hard=False):\n",
    "            gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "            gumbels = (logits + gumbels) / self.temperature\n",
    "\n",
    "            y_soft = F.softmax(gumbels, dim=-1)\n",
    "\n",
    "            if hard:\n",
    "                topk_indices = logits.topk(top_k, dim=-1)[1]\n",
    "                y_hard = torch.zeros_like(logits).scatter_(-1, topk_indices, 1.0)\n",
    "                y = (y_hard - y_soft).detach() + y_soft\n",
    "            else:\n",
    "                y = y_soft\n",
    "\n",
    "            return y\n",
    "        \n",
    "    def displace_tensor_blocks_to_rectangle(self, tensor, displacement):\n",
    "        batch_size, num_blocks, block_height, block_width = tensor.shape\n",
    "\n",
    "        # 计算新矩阵的宽度和高度\n",
    "        height = num_blocks * displacement\n",
    "        width =  (2 + num_blocks) * displacement\n",
    "\n",
    "        # 初始化新的大张量，确保其形状为 (batch_size, height, width)\n",
    "        new_tensor = torch.zeros(batch_size, height, width, device=tensor.device, dtype=tensor.dtype)\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            start_pos_height = i * displacement\n",
    "            start_pos_width = i * displacement\n",
    "            end_pos_height = start_pos_height + block_height\n",
    "            end_pos_width = start_pos_width + block_width\n",
    "\n",
    "            new_tensor[:, start_pos_height:end_pos_height, start_pos_width:end_pos_width] = tensor[:, i, :, :]\n",
    "\n",
    "        return new_tensor\n",
    "    \n",
    "    def forward(self, graph, node_features):\n",
    "        \n",
    "        num_relation = node_features.size(0)\n",
    "        index = graph.num_nodes.tolist()\n",
    "        \n",
    "        target_input, pad_start_position = self.round_up_to_nearest_k_and_a_window_size(index, self.window_size)\n",
    "        padding_input, mask = self.insert_zero_rows(node_features, index, target_input)\n",
    "        \n",
    "        Q = self.query(padding_input).view(num_relation, padding_input.size(1), self.num_heads, self.output_dim).permute(0, 2, 1, 3)                           # (num_relations, num_nodes, num_heads, out_features\n",
    "        K = self.key(padding_input).view(num_relation, padding_input.size(1), self.num_heads, self.output_dim).permute(0, 2, 1, 3)                             # (num_relations, num_nodes, num_heads, out_features)\n",
    "        Q = Q.reshape(num_relation * self.num_heads, padding_input.size(1), self.output_dim)                                                  # (num_relations*num_heads, num_nodes, out_features)\n",
    "        K = K.reshape(num_relation * self.num_heads, padding_input.size(1), self.output_dim) \n",
    "        \n",
    "        attn = self.LocalAttention(\n",
    "            dim = self.output_dim,                   # dimension of each head (you need to pass this in for relative positional encoding)\n",
    "            window_size = self.window_size,          # window size. 512 is optimal, but 256 or 128 yields good enough results\n",
    "            look_backward = 1,                  # each window looks at the window before\n",
    "            look_forward = 1,                   # for non-auto-regressive case, will default to 1, so each window looks at the window before and after it\n",
    "            pad_start_position = pad_start_position\n",
    "        ) \n",
    "        \n",
    "        attn = attn(Q, K, mask = mask).view(num_relation, self.num_heads, -1, self.window_size, 3*self.window_size).mean(dim=1)  \n",
    "        score = self.gumbel_softmax_top_k(attn, self.k, hard=True)\n",
    "        \n",
    "        result_tensor = self.displace_tensor_blocks_to_rectangle(score, self.window_size)\n",
    "        result_tensor = result_tensor[:, :, 10:-10]\n",
    "        indice = [pad_start_position[i] for i in range(len(pad_start_position)) if i % 2 == 0]\n",
    "        indices = []\n",
    "\n",
    "        for num in indice:\n",
    "            next_multiple_of_10 = ((num + 9) // 10) * 10  # 计算向上取10的倍数\n",
    "            sequence = range(num, next_multiple_of_10)  # 生成序列\n",
    "            indices.extend(sequence)  # 直接将序列中的元素添加到结果列表中\n",
    "        all_indices = list(range(result_tensor.size(1)))\n",
    "        remaining_indices = [idx for idx in all_indices if idx not in indices]\n",
    "        \n",
    "        result_tensor = result_tensor[:, remaining_indices, :]\n",
    "        result_tensor = result_tensor[:, :, remaining_indices]\n",
    "        \n",
    "        \n",
    "        return result_tensor.permute(1, 0, 2).contiguous().view(result_tensor.size(1), result_tensor.size(0)*result_tensor.size(2))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间: 0.015247 秒\n",
      "torch.Size([1430, 10010])\n"
     ]
    }
   ],
   "source": [
    "input_dim = relational_output.shape[-1]\n",
    "output_dim = 256\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "k = 3\n",
    "num_nodes = relational_output.size(0)\n",
    "\n",
    "model = Rewirescorelayer(input_dim, output_dim, num_heads, window_size, k)\n",
    "start = time.time()\n",
    "attn_output = model(graph, relational_output)\n",
    "end = time.time()\n",
    "print(f\"运行时间: {end - start:.6f} 秒\")\n",
    "print(attn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewireGearnet(nn.Module):\n",
    "    gradient_checkpoint = False\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(RewireGearnet, self).__init__()\n",
    "        self.num_relation = num_relation\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.linear = nn.Linear(num_relation * input_dim, output_dim)\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(output_dim) if batch_norm else None\n",
    "        self.activation = getattr(F, activation) if activation else None\n",
    "        self.edge_linear = nn.Linear(edge_input_dim, output_dim) if edge_input_dim else None\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "\n",
    "        device = input.device  # Ensure device consistency\n",
    "\n",
    "        if new_edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "            node_out = node_out * self.num_relation + relation\n",
    "            adjacency = torch.sparse_coo_tensor(\n",
    "                torch.stack([node_in, node_out]),\n",
    "                graph.edge_weight.to(device),\n",
    "                (graph.num_node, graph.num_node * graph.num_relation),\n",
    "                device=device\n",
    "            )\n",
    "            update = torch.sparse.mm(adjacency.t(), input)\n",
    "        else:\n",
    "            update = torch.mm(new_edge_list.t(), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = graph.edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(\n",
    "                edge_input * edge_weight, node_out, dim=0,\n",
    "                dim_size=graph.num_node * graph.num_relation\n",
    "            )\n",
    "            update += edge_update\n",
    "            \n",
    "        return update.view(graph.num_node, self.num_relation * self.input_dim).to(device)\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm.to(device)\n",
    "        \n",
    "        output = self.linear(update) + self.self_loop(input)\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        \n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self.message_and_aggregate, graph, input)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph, input, new_edge_list)\n",
    "        output = self.combine(input, update)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1430, 512])\n",
      "tensor([[0.0668, 0.2442, 0.0000,  ..., 0.1006, 0.0000, 0.1292],\n",
      "        [0.0636, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.6651, 0.1436, 0.6689,  ..., 0.0000, 0.0000, 0.0905],\n",
      "        ...,\n",
      "        [0.0514, 0.3041, 0.0401,  ..., 0.0000, 0.0000, 0.1909],\n",
      "        [0.0000, 0.0000, 0.2475,  ..., 0.3640, 0.0000, 0.3992],\n",
      "        [0.0122, 0.1713, 0.0000,  ..., 0.0000, 0.0000, 0.0040]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "output_dim = 512\n",
    "num_relations = graph.num_relation\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "new_node_feature = RewireGearnet(input_dim, output_dim, num_relations)(graph, graph.node_feature.to(device).float(), attn_output).to(device)\n",
    "\n",
    "print(new_node_feature.shape)\n",
    "print(new_node_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个装饰器来计时\n",
    "def time_layer(layer, layer_name):\n",
    "    def timed_layer(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        output = layer(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f'{layer_name}: {end_time - start_time:.6f} seconds')\n",
    "        return output\n",
    "    return timed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMGearnet(nn.Module, core.Configurable):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dims, score_dim, num_relation, num_heads, window_size, k, edge_input_dim=None, num_angle_bin=None,\n",
    "                 short_cut=False, batch_norm=False, activation=\"relu\", concat_hidden=True, readout=\"sum\"):\n",
    "        super(DGMGearnet, self).__init__()\n",
    "\n",
    "        #if not isinstance(hidden_dims, Sequence):\n",
    "            #hidden_dims = [hidden_dims]\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = sum(hidden_dims) if concat_hidden else hidden_dims[-1]\n",
    "        self.dims = [input_dim] + list(hidden_dims)\n",
    "        self.score_dim = score_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.k = k\n",
    "        self.edge_dims = [edge_input_dim] + self.dims[:-1]\n",
    "        self.num_relation = num_relation\n",
    "        self.num_angle_bin = num_angle_bin\n",
    "        self.short_cut = short_cut\n",
    "        self.concat_hidden = concat_hidden\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.score_layers = nn.ModuleList()\n",
    "        for i in range(len(self.dims) - 1):\n",
    "            \n",
    "            self.score_layers.append(relationalGraph(self.dims[i], self.score_dim, num_relation, \n",
    "                                                     edge_input_dim=None, batch_norm=False, activation=\"relu\")) \n",
    "\n",
    "            \n",
    "            self.score_layers.append(Rewirescorelayer(self.score_dim, self.dims[i+1], self.num_heads, self.window_size, \n",
    "                                            self.k, temperature=0.5))\n",
    "            \n",
    "            self.layers.append(RewireGearnet(self.dims[i], self.dims[i + 1], num_relation,\n",
    "                                            edge_input_dim=None, batch_norm=False, activation=\"relu\"))\n",
    "        \n",
    "        if num_angle_bin:\n",
    "            self.spatial_line_graph = layers.SpatialLineGraph(num_angle_bin)\n",
    "            self.edge_layers = nn.ModuleList()\n",
    "            for i in range(len(self.edge_dims) - 1):\n",
    "                self.edge_layers.append(layers.GeometricRelationalGraphConv(\n",
    "                    self.edge_dims[i], self.edge_dims[i + 1], num_angle_bin, None, batch_norm, activation))\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norms = nn.ModuleList()\n",
    "            for i in range(len(self.dims) - 1):\n",
    "                self.batch_norms.append(nn.BatchNorm1d(self.dims[i + 1]))\n",
    "\n",
    "        if readout == \"sum\":\n",
    "            self.readout = layers.SumReadout()\n",
    "        elif readout == \"mean\":\n",
    "            self.readout = layers.MeanReadout()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown readout `%s`\" % readout)\n",
    "\n",
    "    def forward(self, graph, input, edge_list=None, all_loss=None, metric=None):\n",
    "        hiddens = []\n",
    "        layer_input = input\n",
    "        if self.num_angle_bin:\n",
    "            line_graph = self.spatial_line_graph(graph)\n",
    "            edge_input = line_graph.node_feature.float()\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            \n",
    "            relational_output = time_layer(self.score_layers[2*i], 'relational output')(graph, layer_input, edge_list)\n",
    "            new_edge_list = time_layer(self.score_layers[2*i+1], 'new edge list')(graph, relational_output)\n",
    "            \n",
    "            hidden = time_layer(self.layers[i], 'hidden')(graph, layer_input, new_edge_list)\n",
    "            \n",
    "            if self.short_cut and hidden.shape == layer_input.shape:\n",
    "                hidden = hidden + layer_input\n",
    "            \n",
    "            if self.num_angle_bin:\n",
    "                edge_hidden = self.edge_layers[i](line_graph, edge_input)\n",
    "                edge_weight = graph.edge_weight.unsqueeze(-1)\n",
    "                if new_edge_list is None:\n",
    "                    node_out = graph.edge_list[:, 1] * self.num_relation + graph.edge_list[:, 2]\n",
    "                else:\n",
    "                    node_out = new_edge_list[:, 1] * self.num_relation + new_edge_list[:, 2]\n",
    "                \n",
    "                    update = scatter_add(edge_hidden * edge_weight, node_out, dim=0,\n",
    "                                        dim_size=graph.num_node * self.num_relation)\n",
    "                update = update.view(graph.num_node, self.num_relation * edge_hidden.shape[1])\n",
    "                update = self.layers[i].linear(update)\n",
    "                update = self.layers[i].activation(update)\n",
    "                hidden = hidden + update\n",
    "                edge_input = edge_hidden\n",
    "                \n",
    "            if self.batch_norm:\n",
    "                hidden = self.batch_norms[i](hidden)\n",
    "                \n",
    "            hiddens.append(hidden)\n",
    "            layer_input = hidden\n",
    "            edge_list = new_edge_list\n",
    "\n",
    "        if self.concat_hidden:\n",
    "            node_feature = torch.cat(hiddens, dim=-1)\n",
    "        else:\n",
    "            node_feature = hiddens[-1]\n",
    "        graph_feature = self.readout(graph, node_feature)\n",
    "\n",
    "        return {\n",
    "            \"graph_feature\": graph_feature,\n",
    "            \"node_feature\": node_feature\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relational output: 0.001817 seconds\n",
      "new edge list: 0.012704 seconds\n",
      "hidden: 0.000778 seconds\n",
      "relational output: 0.003483 seconds\n",
      "new edge list: 0.011794 seconds\n",
      "hidden: 0.001010 seconds\n",
      "relational output: 0.003436 seconds\n",
      "new edge list: 0.011773 seconds\n",
      "hidden: 0.001004 seconds\n",
      "relational output: 0.003420 seconds\n",
      "new edge list: 0.011732 seconds\n",
      "hidden: 0.000989 seconds\n",
      "relational output: 0.003418 seconds\n",
      "new edge list: 0.011899 seconds\n",
      "hidden: 0.001000 seconds\n"
     ]
    }
   ],
   "source": [
    "input_dim = graph.node_feature.shape[-1]\n",
    "hidden_dims = [512, 512, 512, 512, 512]\n",
    "score_dim = 128\n",
    "num_relations = graph.num_relation\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "k = 5\n",
    "\n",
    "\n",
    "output = DGMGearnet(input_dim, hidden_dims, score_dim, num_relations, num_heads, window_size, k).to(device)(graph.to(device), graph.node_feature.to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  3.4151,  1.6128,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., 12.9921, 14.8914,  0.0000],\n",
      "        [ 0.2311,  0.0000,  0.0000,  ..., 18.6177,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.4006,  0.0000,  ...,  6.1682,  8.7172,  0.0000],\n",
      "        [ 0.4184,  0.0718,  0.0000,  ...,  0.0000,  2.0799,  0.0000],\n",
      "        [ 0.0785,  0.1888,  0.0000,  ...,  3.8867,  0.0000,  0.0000]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>)\n",
      "torch.Size([1430, 2560])\n",
      "\n",
      "\n",
      "tensor([[5.9159e+01, 2.7642e+01, 5.1323e+00,  ..., 4.5037e+03, 1.9291e+03,\n",
      "         3.3170e+03],\n",
      "        [6.2541e+01, 2.6804e+01, 5.2057e+00,  ..., 4.1339e+03, 1.8076e+03,\n",
      "         3.7433e+03],\n",
      "        [4.4461e+01, 2.2220e+01, 5.0708e+00,  ..., 3.4908e+03, 1.5562e+03,\n",
      "         2.3470e+03],\n",
      "        [3.4962e+01, 1.7107e+01, 4.3736e+00,  ..., 1.9345e+03, 9.1557e+02,\n",
      "         1.6042e+03]], device='cuda:0', grad_fn=<ScatterAddBackward>)\n",
      "torch.Size([4, 2560])\n"
     ]
    }
   ],
   "source": [
    "print(output[\"node_feature\"])\n",
    "print(output[\"node_feature\"].shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(output[\"graph_feature\"])\n",
    "print(output[\"graph_feature\"].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gearnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
