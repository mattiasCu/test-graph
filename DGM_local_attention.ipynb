{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdrug import  layers, datasets,transforms,core, models\n",
    "from torchdrug.core import Registry as R\n",
    "from torchdrug.layers import geometry\n",
    "\n",
    "import torch\n",
    "from torchdrug import data\n",
    "\n",
    "from torch_scatter import scatter_add\n",
    "import torch.nn as nn\n",
    "from torchdrug import utils\n",
    "from torch.utils import checkpoint\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from einops import rearrange, repeat, pack, unpack\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    raise ValueError(\"CUDA device 0 is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:21:40   Extracting /home/xiaotong/scratch/protein-datasets/EnzymeCommission.zip to /home/xiaotong/scratch/protein-datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/xiaotong/scratch/protein-datasets/EnzymeCommission/enzyme_commission.pkl.gz: 100%|██████████| 18716/18716 [00:45<00:00, 409.68it/s]\n"
     ]
    }
   ],
   "source": [
    "EnzymeCommission = R.search(\"datasets.EnzymeCommission\")\n",
    "PV = R.search(\"transforms.ProteinView\")\n",
    "trans = PV(view = \"residue\")\n",
    "dataset = EnzymeCommission(\"~/scratch/protein-datasets/\", test_cutoff=0.95, \n",
    "                           atom_feature=\"full\", bond_feature=\"full\", verbose=1, transform = trans)\n",
    "\n",
    "# 只保留alpha碳的简化格式\n",
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()], \n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(k=10, min_distance=5),\n",
    "                                                                 geometry.SequentialEdge(max_distance=2)\n",
    "                                                                 ],\n",
    "                                                    edge_feature=\"gearnet\"\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedProtein(batch_size=1, num_atoms=[185], num_bonds=[3754], num_residues=[185])\n"
     ]
    }
   ],
   "source": [
    "graphs = dataset[0:1]\n",
    "graphs = [element[\"graph\"] for element in graphs]\n",
    "graphs = data.Protein.pack(graphs)\n",
    "graph = graph_construction_model(graphs)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([185])\n",
      "tensor([[ 90,  82,   1],\n",
      "        [ 85,  78,   1],\n",
      "        [ 83,  78,   1],\n",
      "        ...,\n",
      "        [ 75, 107,   0],\n",
      "        [ 50, 108,   0],\n",
      "        [ 51, 108,   0]])\n",
      "torch.Size([3754, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(graph.num_nodes)\n",
    "edge_list = graph.edge_list \n",
    "\n",
    "print(edge_list)\n",
    "print(edge_list.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationnal conv graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationalGraphConv(layers.MessagePassingBase):\n",
    "    \n",
    "    eps = 1e-10\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(relationalGraphConv, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_relation = num_relation\n",
    "        self.edge_input_dim = edge_input_dim\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.linear = nn.Linear(num_relation * input_dim, output_dim)\n",
    "        if edge_input_dim:\n",
    "            self.edge_linear = nn.Linear(edge_input_dim, input_dim)\n",
    "        else:\n",
    "            self.edge_linear = None\n",
    "\n",
    "\n",
    "    def trans(self, A, graph):\n",
    "        n_rel = graph.num_relation\n",
    "        A = A.view(A.size(0),n_rel, A.size(1)//n_rel).permute(1, 0, 2)\n",
    "       # 初始化结果张量\n",
    "        A_norm = torch.zeros_like(A)\n",
    "\n",
    "        # 对每个矩阵进行处理\n",
    "        for i in range(A.size(0)):\n",
    "            # 计算度矩阵 (按行求和)\n",
    "            degree = A[i].sum(dim=1)\n",
    "            \n",
    "            # 计算度矩阵的逆平方根\n",
    "            degree_inv_sqrt = torch.pow(degree, -0.5)\n",
    "            \n",
    "            # 将度矩阵逆平方根转换为对角矩阵\n",
    "            Degree_inv_sqrt = torch.diag(degree_inv_sqrt)\n",
    "            \n",
    "            # 进行归一化操作\n",
    "            A_norm[i] = torch.mm(torch.mm(Degree_inv_sqrt, A[i]), Degree_inv_sqrt)\n",
    "        Anorm = A_norm.permute(1, 0, 2).contiguous().view(A_norm.size(1), A_norm.size(0)*A_norm.size(2))\n",
    "    \n",
    "        return Anorm\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "        device = input.device  # Ensure device consistency\n",
    "        \n",
    "        if new_edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "            node_out = node_out * self.num_relation + relation\n",
    "        \n",
    "            degree_out = scatter_add(graph.edge_weight, node_out, dim_size=graph.num_node * graph.num_relation)\n",
    "            edge_weight = graph.edge_weight / degree_out[node_out]\n",
    "            adjacency = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), edge_weight,\n",
    "                                                (graph.num_node, graph.num_node * graph.num_relation))\n",
    "            update = torch.sparse.mm(adjacency.t().to(device), input.to(device))\n",
    "        else:\n",
    "            adjacency = self.trans(new_edge_list, graph).to(device)\n",
    "            update = torch.mm(adjacency.t().to(device), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(edge_input * edge_weight, node_out, dim=0,\n",
    "                                      dim_size=graph.num_node * graph.num_relation)\n",
    "            update += edge_update\n",
    "\n",
    "        return update.view(input.size(0), self.num_relation * self.input_dim)                           \n",
    "\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        output = self.linear(update) + self.self_loop(input)\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm.to(device)\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        device = input.device\n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self._message_and_aggregate, *graph.to_tensors(), input, new_edge_list)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph.to(device), input, new_edge_list)\n",
    "        output = self.combine(input, update)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationalGraph(layers.MessagePassingBase):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(relationalGraph, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_relation = num_relation\n",
    "        self.edge_input_dim = edge_input_dim\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        if edge_input_dim:\n",
    "            self.edge_linear = nn.Linear(edge_input_dim, input_dim)\n",
    "        else:\n",
    "            self.edge_linear = None\n",
    "            \n",
    "    def trans(self, A, graph):\n",
    "        n_rel = graph.num_relation\n",
    "        A = A.view(A.size(0),n_rel, A.size(1)//n_rel).permute(1, 0, 2)\n",
    "       # 初始化结果张量\n",
    "        A_norm = torch.zeros_like(A)\n",
    "\n",
    "        # 对每个矩阵进行处理\n",
    "        for i in range(A.size(0)):\n",
    "            # 计算度矩阵 (按行求和)\n",
    "            degree = A[i].sum(dim=1)\n",
    "            \n",
    "            # 计算度矩阵的逆平方根\n",
    "            degree_inv_sqrt = torch.pow(degree, -0.5)\n",
    "            \n",
    "            # 将度矩阵逆平方根转换为对角矩阵\n",
    "            Degree_inv_sqrt = torch.diag(degree_inv_sqrt)\n",
    "            \n",
    "            # 进行归一化操作\n",
    "            A_norm[i] = torch.mm(torch.mm(Degree_inv_sqrt, A[i]), Degree_inv_sqrt)\n",
    "        Anorm = A_norm.permute(1, 0, 2).contiguous().view(A_norm.size(1), A_norm.size(0)*A_norm.size(2))\n",
    "    \n",
    "        return Anorm\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "        device = input.device  # Ensure device consistency\n",
    "        \n",
    "        if new_edge_list is None:\n",
    "            node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "            node_out = node_out * self.num_relation + relation\n",
    "        \n",
    "            degree_out = scatter_add(graph.edge_weight, node_out, dim_size=graph.num_node * graph.num_relation)\n",
    "            edge_weight = graph.edge_weight / degree_out[node_out]\n",
    "            adjacency = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), edge_weight,\n",
    "                                                (graph.num_node, graph.num_node * graph.num_relation))\n",
    "            update = torch.sparse.mm(adjacency.t().to(device), input.to(device))\n",
    "        else:\n",
    "            adjacency = self.trans(new_edge_list, graph).to(device)\n",
    "            update = torch.mm(adjacency.t().to(device), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(edge_input * edge_weight, node_out, dim=0,\n",
    "                                      dim_size=graph.num_node * graph.num_relation)\n",
    "            update += edge_update\n",
    "\n",
    "        return update\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        # 自环特征\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        input = input.repeat(self.num_relation, 1).to(device)\n",
    "        loop_update = self.self_loop(input).to(device)\n",
    "        \n",
    "        output = self.linear(update)+loop_update\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm.to(device)\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        device = input.device\n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self._message_and_aggregate, *graph.to_tensors(), input, new_edge_list)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph.to(device), input, new_edge_list)\n",
    "        output = self.combine(input, update)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relationalGraphStack(nn.Module):\n",
    "    \n",
    "    def __init__(self, dims, num_relation, edge_input_dim=None, batch_norm=True, activation=\"relu\"):\n",
    "        super(relationalGraphStack, self).__init__()\n",
    "        self.num_layers = len(dims) - 1\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers.append(relationalGraphConv(dims[i], dims[i + 1], num_relation, edge_input_dim, batch_norm, activation))\n",
    "            \n",
    "        self.layers.append(relationalGraph(dims[-2], dims[-1], num_relation, edge_input_dim, batch_norm, activation))\n",
    "            \n",
    "\n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        device = input.device\n",
    "        x = input\n",
    "        for layer in self.layers:\n",
    "            x = layer(graph.to(device), x, new_edge_list)            \n",
    "        return x.view(graph.num_relation, input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 600, 512])\n"
     ]
    }
   ],
   "source": [
    "rel_dims = [[21, 256, 512, 512]]\n",
    "num_relations = graph.num_relation\n",
    "\n",
    "model = relationalGraphStack(rel_dims[0], num_relations,batch_norm=True)\n",
    "relational_output = model(graph, graph.node_feature.float().to(device))\n",
    "print(relational_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local attention for rewire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewirescorelayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, window_size, k, temperature=0.5):\n",
    "        super(Rewirescorelayer, self).__init__()\n",
    "        self.input_dim = in_features\n",
    "        self.output_dim = out_features\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.k = k\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.query = nn.Linear(in_features, out_features * num_heads).to(device)\n",
    "        self.key = nn.Linear(in_features, out_features * num_heads).to(device)\n",
    "        self.scale = 1 / (out_features ** 0.5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    class LocalAttention(nn.Module):\n",
    "        def __init__(\n",
    "            self,\n",
    "            window_size,\n",
    "            look_backward = 1,\n",
    "            look_forward = None,\n",
    "            dim = None,\n",
    "            scale = None,\n",
    "            pad_start_position = None\n",
    "        ):\n",
    "            super().__init__()\n",
    "\n",
    "            self.scale = scale\n",
    "\n",
    "            self.window_size = window_size\n",
    "\n",
    "            self.look_backward = look_backward\n",
    "            self.look_forward = look_forward\n",
    "            \n",
    "            self.pad_start_position = pad_start_position\n",
    "\n",
    "        def exists(self,val):\n",
    "            return val is not None\n",
    "\n",
    "        # 如果value不存在，返回d\n",
    "        def default(self,value, d):\n",
    "            return d if not self.exists(value) else value\n",
    "\n",
    "        def to(self, t):\n",
    "            return {'device': t.device, 'dtype': t.dtype}\n",
    "\n",
    "        def max_neg_value(self, tensor):\n",
    "            return -torch.finfo(tensor.dtype).max  #返回给定张量数据类型的所能表示的最大负值\n",
    "\n",
    "        def look_around(self, x, backward = 1, forward = 0, pad_value = -1, dim = 2):  #x = bk: (40, 32, 16, 64)\n",
    "            t = x.shape[1]    #获取一共有多少个窗口，这里是32\n",
    "            dims = (len(x.shape) - dim) * (0, 0)   #一个长度为 len(x.shape) - dim 的元组，每个元素为 (0, 0)；其中len(x.shape) = 4\n",
    "            padded_x = F.pad(x, (*dims, backward, forward), value = pad_value)   #在第二维度上，前面加backward个元素，后面加forward个元素 -> (40, 33, 16, 64)\n",
    "            tensors = [padded_x[:, ind:(ind + t), ...] for ind in range(forward + backward + 1)] #一个张量列表，每个张量的维度为(40, 32, 16, 64), len = 2\n",
    "            return torch.cat(tensors, dim = dim) #在第二维度上拼接 -> (40, 32, 32, 64)\n",
    "    \n",
    "        def forward(\n",
    "            self,\n",
    "            q, k,\n",
    "            mask = None,\n",
    "            input_mask = None,\n",
    "            window_size = None\n",
    "        ):\n",
    "\n",
    "            mask = self.default(mask, input_mask)\n",
    "            assert not (self.exists(window_size) and not self.use_xpos), 'cannot perform window size extrapolation if xpos is not turned on'\n",
    "            shape, pad_value, window_size, look_backward, look_forward = q.shape, -1, self.default(window_size, self.window_size), self.look_backward, self.look_forward\n",
    "            (q, packed_shape), (k, _) = map(lambda t: pack([t], '* n d'), (q, k))  #打包成[5, 8, 512, 64] -> [40, 512, 64] \n",
    "\n",
    "\n",
    "            b, n, dim_head, device, dtype = *q.shape, q.device, q.dtype   # 40, 512, 64\n",
    "            scale = self.default(self.scale, dim_head ** -0.5)\n",
    "            assert (n % window_size) == 0, f'sequence length {n} must be divisible by window size {window_size} for local attention'\n",
    "\n",
    "            windows = n // window_size  # 512 / 16 = 32\n",
    "\n",
    "            seq = torch.arange(n, device = device)                  # 0, 1, 2, 3, ..., 511\n",
    "            b_t = rearrange(seq, '(w n) -> 1 w n', w = windows, n = window_size)    # (1, 32, 16) 排序序列变形后的矩阵\n",
    "\n",
    "            # bucketing\n",
    "\n",
    "            bq, bk = map(lambda t: rearrange(t, 'b (w n) d -> b w n d', w = windows), (q, k)) #重构：（40，512，64）->（40, 32, 16, 64）\n",
    "\n",
    "            bq = bq * scale    # (40, 32, 16, 64)\n",
    "\n",
    "            look_around_kwargs = dict(\n",
    "                backward =  look_backward,\n",
    "                forward =  look_forward,\n",
    "                pad_value = pad_value\n",
    "            )\n",
    "\n",
    "            bk = self.look_around(bk, **look_around_kwargs)      # (40, 32, 32, 64)\n",
    "    \n",
    "\n",
    "            # calculate positions for masking\n",
    "\n",
    "            bq_t = b_t\n",
    "            bq_k = self.look_around(b_t, **look_around_kwargs) # (1, 32, 32)\n",
    "\n",
    "            bq_t = rearrange(bq_t, '... i -> ... i 1')      # (1, 32, 16, 1)\n",
    "            bq_k = rearrange(bq_k, '... j -> ... 1 j')      # (1, 32, 1, 16)\n",
    "\n",
    "            pad_mask = bq_k == pad_value\n",
    "\n",
    "            sim = torch.einsum('b h i e, b h j e -> b h i j', bq, bk)  # (40, 32, 16, 64) * (40, 32, 32, 64) -> (40, 32, 16, 32)\n",
    "\n",
    "            mask_value = self.max_neg_value(sim)\n",
    "\n",
    "            sim = sim.masked_fill(pad_mask, mask_value)\n",
    "\n",
    "\n",
    "            if self.exists(mask):\n",
    "                batch = mask.shape[0]    # 5\n",
    "                assert (b % batch) == 0\n",
    "\n",
    "                h = b // mask.shape[0]  # 8\n",
    "\n",
    "                mask = rearrange(mask, '... (w n) -> (...) w n', w = windows, n = window_size)\n",
    "                mask = self.look_around(mask, **{**look_around_kwargs, 'pad_value': False})\n",
    "                mask = rearrange(mask, '... j -> ... 1 j')\n",
    "                mask = repeat(mask, 'b ... -> (b h) ...', h = h)\n",
    "\n",
    "                sim = sim.masked_fill(~mask, mask_value)\n",
    "                del mask\n",
    "                \n",
    "            indices = [self.pad_start_position[i] // window_size for i in range(len(self.pad_start_position)) if i % 2 != 0]\n",
    "            all_indices = list(range(windows))\n",
    "            remaining_indices = [idx for idx in all_indices if idx not in indices]\n",
    "            \n",
    "            # 使用剩余的索引选择元素\n",
    "            rest_sim = sim[:, remaining_indices, :, :]\n",
    "\n",
    "            # attention\n",
    "            attn = rest_sim.softmax(dim = -1)\n",
    "\n",
    "            return attn\n",
    "\n",
    "    def insert_zero_rows(self, tensor, lengths, target_lengths):\n",
    "        assert len(lengths) == len(target_lengths), \"Lengths and target lengths must be of the same length.\"\n",
    "        \n",
    "        # 计算每个位置需要插入的零行数\n",
    "        zero_rows = [target - length for length, target in zip(lengths, target_lengths)]\n",
    "        \n",
    "        # 初始化结果列表\n",
    "        parts = []\n",
    "        mask_parts = []\n",
    "        start = 0\n",
    "        \n",
    "        for i, length in enumerate(lengths):\n",
    "            end = start + length\n",
    "            \n",
    "            # 原始张量部分\n",
    "            parts.append(tensor[:, start:end, :])\n",
    "            mask_parts.append(torch.ones(tensor.size(0), length, dtype=torch.bool, device=tensor.device))\n",
    "            \n",
    "            # 插入零行\n",
    "            if zero_rows[i] > 0:\n",
    "                zero_padding = torch.zeros(tensor.size(0), zero_rows[i], tensor.size(2), device=tensor.device)\n",
    "                mask_padding = torch.zeros(tensor.size(0), zero_rows[i], dtype=torch.bool, device=tensor.device)\n",
    "                parts.append(zero_padding)\n",
    "                mask_parts.append(mask_padding)\n",
    "            \n",
    "            start = end\n",
    "        \n",
    "        # 拼接所有部分\n",
    "        padded_tensor = torch.cat(parts, dim=1)\n",
    "        mask = torch.cat(mask_parts, dim=1)\n",
    "        \n",
    "        return padded_tensor, mask\n",
    "\n",
    "\n",
    "    def round_up_to_nearest_k_and_a_window_size(self, lst, k):\n",
    "        pad_start_position = []\n",
    "        result_lst = [(x + k - 1) // k * k +k for x in lst]\n",
    "        for i in range(len(lst)):\n",
    "            pad_start_position.append(sum(result_lst[:i])-i*k + lst[i])\n",
    "            pad_start_position.append(sum(result_lst[:i+1])-k)\n",
    "        return result_lst, pad_start_position\n",
    "\n",
    "    def gumbel_softmax_top_k(self, logits,  top_k,  hard=False):\n",
    "            gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "            gumbels = (logits + gumbels) / self.temperature\n",
    "\n",
    "            y_soft = F.softmax(gumbels, dim=-1)\n",
    "\n",
    "            if hard:\n",
    "                topk_indices = logits.topk(top_k, dim=-1)[1]\n",
    "                y_hard = torch.zeros_like(logits).scatter_(-1, topk_indices, 1.0)\n",
    "                y = (y_hard - y_soft).detach() + y_soft\n",
    "            else:\n",
    "                y = y_soft\n",
    "\n",
    "            return y\n",
    "        \n",
    "    def displace_tensor_blocks_to_rectangle(self, tensor, displacement):\n",
    "        batch_size, num_blocks, block_height, block_width = tensor.shape\n",
    "\n",
    "        # 计算新矩阵的宽度和高度\n",
    "        height = num_blocks * displacement\n",
    "        width =  (2 + num_blocks) * displacement\n",
    "\n",
    "        # 初始化新的大张量，确保其形状为 (batch_size, height, width)\n",
    "        new_tensor = torch.zeros(batch_size, height, width, device=tensor.device, dtype=tensor.dtype)\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            start_pos_height = i * displacement\n",
    "            start_pos_width = i * displacement\n",
    "            end_pos_height = start_pos_height + block_height\n",
    "            end_pos_width = start_pos_width + block_width\n",
    "\n",
    "            new_tensor[:, start_pos_height:end_pos_height, start_pos_width:end_pos_width] = tensor[:, i, :, :]\n",
    "\n",
    "        return new_tensor\n",
    "    \n",
    "    def forward(self, graph, node_features):\n",
    "        \n",
    "        num_relation = node_features.size(0)\n",
    "        index = graph.num_nodes.tolist()\n",
    "        \n",
    "        target_input, pad_start_position = self.round_up_to_nearest_k_and_a_window_size(index, self.window_size)\n",
    "        padding_input, mask = self.insert_zero_rows(node_features, index, target_input)\n",
    "        \n",
    "        Q = self.query(padding_input).view(num_relation, padding_input.size(1), self.num_heads, self.output_dim).permute(0, 2, 1, 3)                           # (num_relations, num_nodes, num_heads, out_features\n",
    "        K = self.key(padding_input).view(num_relation, padding_input.size(1), self.num_heads, self.output_dim).permute(0, 2, 1, 3)                             # (num_relations, num_nodes, num_heads, out_features)\n",
    "        Q = Q.reshape(num_relation * self.num_heads, padding_input.size(1), self.output_dim)                                                  # (num_relations*num_heads, num_nodes, out_features)\n",
    "        K = K.reshape(num_relation * self.num_heads, padding_input.size(1), self.output_dim) \n",
    "        \n",
    "        attn = self.LocalAttention(\n",
    "            dim = self.output_dim,                   # dimension of each head (you need to pass this in for relative positional encoding)\n",
    "            window_size = self.window_size,          # window size. 512 is optimal, but 256 or 128 yields good enough results\n",
    "            look_backward = 1,                  # each window looks at the window before\n",
    "            look_forward = 1,                   # for non-auto-regressive case, will default to 1, so each window looks at the window before and after it\n",
    "            pad_start_position = pad_start_position\n",
    "        ) \n",
    "        \n",
    "        attn = attn(Q, K, mask = mask).view(num_relation, self.num_heads, -1, self.window_size, 3*self.window_size).mean(dim=1)  \n",
    "        score = self.gumbel_softmax_top_k(attn, self.k, hard=True)\n",
    "        \n",
    "        result_tensor = self.displace_tensor_blocks_to_rectangle(score, self.window_size)\n",
    "        result_tensor = result_tensor[:, :, 10:-10]\n",
    "        indice = [pad_start_position[i] for i in range(len(pad_start_position)) if i % 2 == 0]\n",
    "        indices = []\n",
    "\n",
    "        for num in indice:\n",
    "            next_multiple_of_10 = ((num + 9) // 10) * 10  # 计算向上取10的倍数\n",
    "            sequence = range(num, next_multiple_of_10)  # 生成序列\n",
    "            indices.extend(sequence)  # 直接将序列中的元素添加到结果列表中\n",
    "        all_indices = list(range(result_tensor.size(1)))\n",
    "        remaining_indices = [idx for idx in all_indices if idx not in indices]\n",
    "        \n",
    "        result_tensor = result_tensor[:, remaining_indices, :]\n",
    "        result_tensor = result_tensor[:, :, remaining_indices]\n",
    "        \n",
    "        return result_tensor.permute(1, 0, 2).contiguous().view(result_tensor.size(1), result_tensor.size(0)*result_tensor.size(2))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间: 0.005996 秒\n",
      "torch.Size([600, 4200])\n"
     ]
    }
   ],
   "source": [
    "input_dim = relational_output.shape[-1]\n",
    "output_dim = 64\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "k = 3\n",
    "num_nodes = relational_output.size(0)\n",
    "\n",
    "model = Rewirescorelayer(input_dim, output_dim, num_heads, window_size, k)\n",
    "start = time.time()\n",
    "attn_output = model(graph, relational_output)\n",
    "end = time.time()\n",
    "print(f\"运行时间: {end - start:.6f} 秒\")\n",
    "print(attn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewireGearnet(nn.Module):\n",
    "    gradient_checkpoint = False\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(RewireGearnet, self).__init__()\n",
    "        self.num_relation = num_relation\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.linear = nn.Linear(num_relation * input_dim, output_dim)\n",
    "        self.self_loop = nn.Linear(input_dim, output_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(output_dim) if batch_norm else None\n",
    "        self.activation = getattr(F, activation) if activation else None\n",
    "        self.edge_linear = nn.Linear(edge_input_dim, output_dim) if edge_input_dim else None\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "\n",
    "        device = input.device  # Ensure device consistency\n",
    "        update = torch.mm(new_edge_list.t(), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = graph.edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(\n",
    "                edge_input * edge_weight, node_out, dim=0,\n",
    "                dim_size=graph.num_node * graph.num_relation\n",
    "            )\n",
    "            update += edge_update\n",
    "            \n",
    "        return update.view(input.size(0), self.num_relation * self.input_dim).to(device)\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm.to(device)\n",
    "        \n",
    "        output = self.linear(update) + self.self_loop(input)\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        \n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self.message_and_aggregate, graph, input)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph, input, new_edge_list)\n",
    "        output = self.combine(input, update)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gearnetlayer(layers.RelationalGraphConv):\n",
    "\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, num_relation, edge_input_dim=None, batch_norm=False, activation=\"relu\"):\n",
    "        super(gearnetlayer, self).__init__(input_dim, output_dim, num_relation, edge_input_dim,\n",
    "                                                           batch_norm, activation)\n",
    "\n",
    "    def message_and_aggregate(self, graph, input, new_edge_list):\n",
    "        assert graph.num_relation == self.num_relation\n",
    "\n",
    "        device = input.device  # Ensure device consistency\n",
    "        update = torch.mm(new_edge_list.t(), input.to(device))\n",
    "        \n",
    "        if self.edge_linear:\n",
    "            edge_input = graph.edge_feature.float().to(device)\n",
    "            edge_input = self.edge_linear(edge_input)\n",
    "            edge_weight = graph.edge_weight.unsqueeze(-1).to(device)\n",
    "            edge_update = scatter_add(\n",
    "                edge_input * edge_weight, node_out, dim=0,\n",
    "                dim_size=graph.num_node * graph.num_relation\n",
    "            )\n",
    "            update += edge_update\n",
    "            \n",
    "        return update.view(input.size(0), self.num_relation * self.input_dim).to(device)\n",
    "    \n",
    "    def combine(self, input, update):\n",
    "        device = input.device\n",
    "        self.linear.to(device)  # Ensure the linear layers are on the correct device\n",
    "        self.self_loop.to(device)\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm.to(device)\n",
    "        \n",
    "        output = self.linear(update) + self.self_loop(input)\n",
    "        if self.batch_norm:\n",
    "            output = self.batch_norm(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        \n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self.message_and_aggregate, graph, input)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph, input, new_edge_list)\n",
    "        output = self.combine(input, update)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rewireGearNetstack(nn.Module):\n",
    "    \n",
    "    def __init__(self, dims, num_relation, edge_input_dim=None, batch_norm=True, activation=\"relu\"):\n",
    "        super(rewireGearNetstack, self).__init__()\n",
    "        self.num_layers = len(dims) - 1\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(RewireGearnet(dims[0], dims[1], num_relation, edge_input_dim, batch_norm, activation))\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers.append(gearnetlayer(dims[i+1], dims[i + 2], num_relation, edge_input_dim, batch_norm, activation))\n",
    "            \n",
    " \n",
    "            \n",
    "    def forward(self, graph, input, new_edge_list=None):\n",
    "        device = input.device\n",
    "        x = input\n",
    "        for layer in self.layers:\n",
    "            x = layer(graph.to(device), x, new_edge_list)         \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 512])\n",
      "tensor([[0.9666, 0.0000, 0.6164,  ..., 0.2444, 0.8194, 1.3916],\n",
      "        [0.0902, 0.9526, 0.5950,  ..., 0.5453, 0.0078, 2.0728],\n",
      "        [0.0000, 0.3094, 1.7195,  ..., 1.0820, 0.5567, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.6042, 0.0000,  ..., 0.0000, 0.0000, 0.2695],\n",
      "        [0.0764, 0.0000, 0.5411,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7347, 0.0338,  ..., 0.9431, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dims = [21, 512, 512]\n",
    "num_relations = graph.num_relation\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "new_node_feature = rewireGearNetstack(dims, num_relations)(graph, graph.node_feature.to(device).float(), attn_output).to(device)\n",
    "\n",
    "print(new_node_feature.shape)\n",
    "print(new_node_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个装饰器来计时\n",
    "def time_layer(layer, layer_name):\n",
    "    def timed_layer(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        output = layer(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f'{layer_name}: {end_time - start_time:.6f} seconds')\n",
    "        return output\n",
    "    return timed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMGearnet(nn.Module, core.Configurable):\n",
    "\n",
    "    def __init__(self, relation_dims, score_in_dim, score_out_dim, diffusion_dims, num_relation, num_heads, window_size, k, edge_input_dim=None, num_angle_bin=None,\n",
    "                 short_cut=False, batch_norm=False, activation=\"relu\", concat_hidden=True, readout=\"sum\"):\n",
    "        super(DGMGearnet, self).__init__()\n",
    "\n",
    "        #if not isinstance(hidden_dims, Sequence):\n",
    "            #hidden_dims = [hidden_dims]\n",
    "        self.relation_dims = relation_dims\n",
    "        self.score_in_dim = score_in_dim\n",
    "        self.score_out_dim = score_out_dim\n",
    "        self.diffusion_dims = diffusion_dims    \n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.k = k\n",
    "        #self.edge_dims = [edge_input_dim] + self.dims[:-1]\n",
    "        self.num_relation = num_relation\n",
    "        self.num_angle_bin = num_angle_bin\n",
    "        self.short_cut = short_cut\n",
    "        self.concat_hidden = concat_hidden\n",
    "        self.batch_norm = batch_norm\n",
    "        self.ouput_dim = self.diffusion_dims[-1][-1]*len(self.diffusion_dims) if concat_hidden else self.diffusion_dims[-1][-1]\n",
    "        print(\"output_dim\", self.ouput_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.score_layers = nn.ModuleList()\n",
    "        for i in range(len(self.relation_dims)):\n",
    "            if i == 0:\n",
    "                self.score_layers.append(relationalGraphStack(self.relation_dims[i], num_relation, \n",
    "                                                        edge_input_dim=None, batch_norm=True, activation=\"relu\")) \n",
    "\n",
    "            else:\n",
    "                self.score_layers.append(relationalGraphStack(self.relation_dims[i], num_relation, \n",
    "                                                        edge_input_dim=None, batch_norm=True, activation=\"relu\")) \n",
    "                    \n",
    "            self.score_layers.append(Rewirescorelayer(self.score_in_dim, self.score_out_dim, self.num_heads, self.window_size, \n",
    "                                                        self.k, temperature=0.5))\n",
    "            \n",
    "\n",
    "            self.layers.append(rewireGearNetstack(self.diffusion_dims[i], num_relation,\n",
    "                                                        edge_input_dim=None, batch_norm=True, activation=\"relu\"))\n",
    "        \n",
    "        if num_angle_bin:\n",
    "            self.spatial_line_graph = layers.SpatialLineGraph(num_angle_bin)\n",
    "            self.edge_layers = nn.ModuleList()\n",
    "            for i in range(len(self.edge_dims) - 1):\n",
    "                self.edge_layers.append(layers.GeometricRelationalGraphConv(\n",
    "                    self.edge_dims[i], self.edge_dims[i + 1], num_angle_bin, None, batch_norm, activation))\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norms = nn.ModuleList()\n",
    "            for i in range(len(self.diffusion_dims) ):\n",
    "                self.batch_norms.append(nn.BatchNorm1d(self.diffusion_dims[i][-1]))\n",
    "\n",
    "        if readout == \"sum\":\n",
    "            self.readout = layers.SumReadout()\n",
    "        elif readout == \"mean\":\n",
    "            self.readout = layers.MeanReadout()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown readout `%s`\" % readout)\n",
    "\n",
    "    def forward(self, graph, input, edge_list=None, all_loss=None, metric=None):\n",
    "        node_in, node_out, relation = graph.edge_list.t().to(device)\n",
    "        node_out = node_out * self.num_relation + relation\n",
    "        adjacency = torch.sparse_coo_tensor(\n",
    "            torch.stack([node_in, node_out]),\n",
    "            graph.edge_weight.to(device),\n",
    "            (graph.num_node, graph.num_node * graph.num_relation),\n",
    "            device=device\n",
    "        )\n",
    "        adjacency = adjacency.to_dense()\n",
    "        \n",
    "        hiddens = []\n",
    "        layer_input = input\n",
    "        score_layer_input = input\n",
    "        \n",
    "        if self.num_angle_bin:\n",
    "            line_graph = self.spatial_line_graph(graph)\n",
    "            edge_input = line_graph.node_feature.float()\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            \n",
    "            \n",
    "            relational_output = time_layer(self.score_layers[2*i], 'relational output')(graph, score_layer_input, edge_list)\n",
    "            new_edge_list = time_layer(self.score_layers[2*i+1], 'new edge list')(graph, relational_output)\n",
    "            new_edge_list = torch.max(adjacency, new_edge_list)\n",
    "            hidden = time_layer(self.layers[i], 'hidden')(graph, layer_input, new_edge_list)\n",
    "            \n",
    "            if self.short_cut and hidden.shape == layer_input.shape:\n",
    "                hidden = hidden + layer_input\n",
    "            \n",
    "            if self.num_angle_bin:\n",
    "                edge_hidden = self.edge_layers[i](line_graph, edge_input)\n",
    "                edge_weight = graph.edge_weight.unsqueeze(-1)\n",
    "                if new_edge_list is None:\n",
    "                    node_out = graph.edge_list[:, 1] * self.num_relation + graph.edge_list[:, 2]\n",
    "                else:\n",
    "                    node_out = new_edge_list[:, 1] * self.num_relation + new_edge_list[:, 2]\n",
    "                \n",
    "                    update = scatter_add(edge_hidden * edge_weight, node_out, dim=0,\n",
    "                                        dim_size=graph.num_node * self.num_relation)\n",
    "                update = update.view(graph.num_node, self.num_relation * edge_hidden.shape[1])\n",
    "                update = self.layers[i].linear(update)\n",
    "                update = self.layers[i].activation(update)\n",
    "                hidden = hidden + update\n",
    "                edge_input = edge_hidden\n",
    "                \n",
    "            if self.batch_norm:\n",
    "                hidden = self.batch_norms[i](hidden)\n",
    "                \n",
    "            hiddens.append(hidden)\n",
    "            score_layer_input = torch.cat([hidden, relational_output.view(hidden.size(0), self.num_relation*hidden.size(-1))], dim=-1)\n",
    "            layer_input = hidden\n",
    "            print(score_layer_input.shape)\n",
    "            edge_list = new_edge_list\n",
    "\n",
    "        if self.concat_hidden:\n",
    "            node_feature = torch.cat(hiddens, dim=-1)\n",
    "        else:\n",
    "            node_feature = hiddens[-1]\n",
    "        graph_feature = self.readout(graph, node_feature)\n",
    "\n",
    "        return {\n",
    "            \"graph_feature\": graph_feature,\n",
    "            \"node_feature\": node_feature\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dim 1024\n",
      "relational output: 0.004743 seconds\n",
      "new edge list: 0.004357 seconds\n",
      "hidden: 0.001370 seconds\n",
      "torch.Size([600, 4096])\n",
      "relational output: 0.006383 seconds\n",
      "new edge list: 0.003749 seconds\n",
      "hidden: 0.001331 seconds\n",
      "torch.Size([600, 4096])\n"
     ]
    }
   ],
   "source": [
    "relation_dims = [[21, 128, 512, 512], [4096, 1024, 512, 512]]\n",
    "score_in_dim = 512\n",
    "score_out_dim = 64\n",
    "diffusion_dims = [[21, 512, 512, 512], [512, 512, 512, 512]]   \n",
    "num_relations = graph.num_relation\n",
    "num_heads = 8\n",
    "window_size = 10\n",
    "k = 5\n",
    "\n",
    "output = DGMGearnet(relation_dims, score_in_dim, score_out_dim, diffusion_dims, num_relations, num_heads, window_size, k, batch_norm=True).to(device)(graph.to(device), graph.node_feature.to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6396,  0.3717, -0.1841,  ..., -0.6632, -0.6266,  0.3609],\n",
      "        [-0.6695, -0.6594, -0.6908,  ...,  0.0965, -0.6266,  0.2488],\n",
      "        [ 0.3171, -0.6594, -0.6908,  ...,  0.1406, -0.6266,  3.9892],\n",
      "        ...,\n",
      "        [-0.6695, -0.6594,  0.1060,  ..., -0.6632, -0.6266,  0.6253],\n",
      "        [-0.6695, -0.6594, -0.6908,  ..., -0.2858,  0.6364, -0.6156],\n",
      "        [-0.6695, -0.6594, -0.5598,  ...,  0.0772, -0.6266, -0.2836]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>)\n",
      "torch.Size([600, 1024])\n",
      "\n",
      "\n",
      "tensor([[-34.3780,   2.6419,  21.6274,  ..., -26.3327,   0.0387,   7.7938],\n",
      "        [ 34.3782,  -2.6417, -21.6273,  ...,  26.3327,  -0.0388,  -7.7937]],\n",
      "       device='cuda:0', grad_fn=<ScatterAddBackward>)\n",
      "torch.Size([2, 1024])\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(output[\"node_feature\"])\n",
    "\n",
    "print(output[\"node_feature\"].shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(output[\"graph_feature\"])\n",
    "print(output[\"graph_feature\"].shape)\n",
    "a = diffusion_dims[-1][-1]*len(diffusion_dims)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n",
      "tensor([[ 2.6553,  0.6771, -0.7213,  ..., -0.6440, -0.3518,  0.5691],\n",
      "        [ 1.1149,  0.7099, -0.7213,  ..., -0.6440, -0.5663,  1.2097],\n",
      "        [-0.3453, -0.1319, -0.7213,  ..., -0.6440, -0.5663,  0.8508],\n",
      "        ...,\n",
      "        [-0.7127, -0.6336,  1.3473,  ...,  0.1884,  0.2556,  0.4028],\n",
      "        [-0.7127, -0.6336, -0.6713,  ...,  0.1304,  0.0762,  0.0320],\n",
      "        [ 1.1920,  0.0447,  0.5976,  ..., -0.1654, -0.5416, -0.2907]],\n",
      "       grad_fn=<CatBackward>)\n",
      "torch.Size([600, 3072])\n",
      "\n",
      "\n",
      "tensor([[ -17.7990,  -14.1211,   29.1059,  ..., -113.1195,  -75.7146,\n",
      "          100.5911],\n",
      "        [  17.7990,   14.1209,  -29.1059,  ...,  113.1196,   75.7146,\n",
      "         -100.5909]], grad_fn=<ScatterAddBackward>)\n",
      "torch.Size([2, 3072])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 21\n",
    "hidden_dims = [512, 512, 512, 512, 512, 512]\n",
    "\n",
    "\n",
    "gearnet = R.search(\"models.GearNet\")(input_dim, hidden_dims, num_relations, batch_norm=True, concat_hidden=True, readout=\"sum\")\n",
    "print(gearnet.output_dim)\n",
    "\n",
    "gearnet_output = gearnet(graph, graph.node_feature.float())\n",
    "print(gearnet_output[\"node_feature\"])\n",
    "print(gearnet_output[\"node_feature\"].shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(gearnet_output[\"graph_feature\"])\n",
    "print(gearnet_output[\"graph_feature\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 90,  82,   1],\n",
      "        [ 85,  78,   1],\n",
      "        [ 83,  78,   1],\n",
      "        ...,\n",
      "        [ 75, 107,   0],\n",
      "        [ 50, 108,   0],\n",
      "        [ 51, 108,   0]])\n",
      "[(90, 82), (85, 78), (83, 78), (84, 79), (73, 79), (85, 79), (85, 80), (87, 80), (106, 81), (91, 81), (90, 81), (76, 81), (105, 81), (91, 82), (106, 82), (84, 78), (92, 82), (76, 82), (90, 83), (89, 83), (91, 83), (89, 84), (79, 84), (90, 84), (79, 85), (80, 85), (78, 85), (79, 86), (96, 86), (80, 86), (70, 75), (75, 69), (92, 69), (75, 70), (37, 70), (172, 71), (171, 71), (175, 71), (93, 72), (94, 72), (92, 72), (92, 73), (78, 73), (40, 74), (69, 74), (97, 87), (44, 75), (69, 75), (92, 75), (43, 75), (40, 75), (92, 76), (82, 76), (91, 76), (43, 76), (43, 77), (40, 77), (92, 77), (73, 78), (102, 95), (72, 92), (69, 93), (72, 93), (115, 93), (68, 93), (112, 93), (73, 93), (89, 94), (72, 94), (73, 94), (84, 94), (89, 95), (90, 95), (115, 95), (106, 92), (100, 95), (89, 96), (88, 96), (90, 96), (102, 96), (88, 97), (102, 97), (89, 97), (90, 97), (88, 98), (89, 98), (87, 98), (88, 99), (89, 99), (82, 90), (96, 87), (80, 87), (96, 88), (97, 88), (98, 88), (83, 88), (96, 89), (94, 89), (95, 89), (84, 89), (83, 89), (97, 89), (83, 90), (95, 90), (93, 69), (84, 90), (96, 90), (106, 90), (106, 91), (82, 91), (107, 91), (83, 91), (111, 91), (76, 92), (69, 92), (73, 92), (75, 92), (107, 92), (75, 42), (178, 35), (23, 36), (175, 36), (175, 37), (70, 37), (178, 38), (179, 38), (34, 39), (74, 39), (74, 40), (70, 40), (75, 40), (179, 41), (75, 41), (175, 35), (47, 42), (75, 43), (76, 43), (77, 43), (75, 44), (66, 44), (40, 45), (75, 45), (75, 46), (41, 46), (107, 47), (108, 47), (184, 48), (53, 48), (14, 30), (17, 26), (32, 26), (32, 27), (20, 27), (17, 27), (21, 27), (17, 28), (20, 28), (15, 28), (16, 28), (14, 29), (15, 29), (13, 29), (16, 29), (54, 49), (174, 30), (13, 30), (15, 30), (14, 31), (174, 31), (20, 32), (27, 32), (175, 33), (15, 33), (174, 33), (14, 33), (175, 34), (178, 34), (180, 63), (55, 60), (2, 60), (4, 60), (3, 60), (1, 61), (55, 61), (2, 61), (56, 61), (55, 62), (52, 62), (184, 62), (51, 62), (176, 63), (184, 63), (1, 60), (179, 63), (176, 64), (173, 64), (172, 64), (51, 65), (116, 65), (176, 66), (179, 66), (176, 67), (175, 67), (172, 67), (172, 68), (93, 68), (61, 55), (184, 49), (109, 50), (108, 50), (112, 50), (62, 51), (65, 51), (62, 52), (184, 52), (62, 53), (48, 53), (113, 54), (112, 54), (60, 55), (62, 55), (118, 99), (1, 56), (0, 56), (0, 57), (1, 57), (52, 57), (1, 58), (116, 58), (117, 58), (0, 58), (1, 59), (0, 59), (3, 59), (2, 59), (130, 155), (125, 151), (159, 151), (159, 152), (145, 152), (162, 152), (163, 152), (159, 153), (141, 153), (145, 153), (144, 153), (159, 154), (129, 154), (130, 154), (141, 155), (126, 151), (160, 155), (141, 156), (137, 156), (133, 156), (140, 156), (141, 157), (137, 157), (153, 158), (166, 158), (153, 159), (154, 159), (141, 159), (144, 159), (166, 160), (163, 147), (123, 142), (122, 142), (126, 142), (169, 143), (119, 143), (123, 143), (169, 144), (168, 144), (160, 144), (150, 145), (122, 145), (119, 146), (122, 146), (118, 146), (167, 160), (168, 147), (164, 147), (169, 147), (163, 148), (162, 148), (144, 149), (122, 149), (145, 150), (122, 150), (126, 150), (125, 150), (142, 150), (145, 151), (33, 174), (140, 169), (160, 169), (14, 170), (12, 170), (13, 170), (14, 171), (15, 171), (33, 171), (67, 172), (68, 172), (7, 173), (12, 173), (14, 174), (30, 174), (143, 169), (67, 175), (34, 175), (33, 175), (37, 175), (67, 176), (63, 176), (64, 176), (8, 177), (7, 177), (34, 178), (37, 178), (63, 179), (184, 179), (63, 180), (18, 165), (168, 160), (144, 160), (169, 160), (166, 161), (167, 161), (152, 161), (152, 162), (148, 162), (167, 162), (148, 163), (168, 163), (147, 163), (147, 164), (148, 164), (153, 141), (19, 165), (16, 165), (161, 166), (160, 166), (16, 166), (18, 166), (160, 167), (16, 167), (15, 167), (144, 168), (163, 168), (160, 168), (144, 169), (103, 110), (90, 106), (81, 106), (92, 107), (91, 107), (112, 107), (82, 107), (76, 107), (50, 108), (47, 108), (50, 109), (54, 109), (104, 109), (51, 109), (104, 110), (92, 106), (105, 110), (104, 111), (103, 111), (106, 111), (93, 112), (54, 112), (54, 113), (58, 113), (103, 114), (102, 114), (93, 115), (95, 115), (65, 116), (58, 116), (110, 103), (95, 100), (118, 100), (114, 100), (96, 101), (95, 101), (90, 101), (114, 101), (97, 102), (95, 102), (90, 102), (111, 102), (96, 102), (114, 102), (111, 103), (58, 117), (114, 103), (95, 103), (110, 104), (111, 104), (109, 104), (114, 104), (111, 105), (110, 105), (91, 105), (82, 105), (91, 106), (82, 106), (111, 106), (5, 136), (126, 131), (138, 131), (127, 132), (138, 132), (137, 132), (138, 133), (127, 133), (156, 133), (5, 134), (139, 134), (5, 135), (4, 135), (127, 135), (6, 135), (125, 130), (6, 136), (10, 136), (9, 136), (10, 137), (156, 137), (127, 138), (126, 138), (123, 139), (5, 139), (4, 139), (169, 140), (10, 140), (126, 141), (138, 124), (112, 117), (146, 118), (95, 118), (146, 119), (142, 119), (142, 120), (60, 120), (142, 121), (116, 121), (142, 122), (145, 122), (142, 123), (139, 123), (142, 124), (20, 26), (150, 125), (142, 125), (138, 126), (142, 126), (138, 127), (135, 127), (123, 128), (138, 128), (124, 129), (154, 129), (150, 129), (138, 130), (155, 130), (136, 7), (135, 4), (136, 4), (59, 4), (123, 4), (136, 5), (135, 5), (139, 5), (138, 5), (60, 5), (136, 6), (61, 6), (139, 6), (173, 7), (177, 7), (60, 4), (176, 7), (64, 7), (177, 8), (180, 8), (173, 8), (176, 8), (174, 8), (136, 9), (137, 9), (177, 9), (136, 10), (137, 10), (140, 10), (173, 10), (61, 1), (59, 0), (56, 0), (60, 0), (57, 0), (58, 0), (55, 0), (61, 0), (59, 1), (60, 1), (170, 11), (56, 1), (55, 1), (58, 1), (62, 1), (60, 2), (59, 2), (61, 2), (56, 2), (59, 3), (60, 3), (61, 3), (58, 3), (139, 4), (26, 21), (29, 16), (165, 16), (28, 17), (27, 17), (29, 17), (22, 17), (165, 18), (166, 18), (28, 18), (165, 19), (167, 19), (28, 20), (25, 20), (15, 20), (167, 16), (27, 21), (17, 22), (16, 22), (36, 23), (18, 23), (32, 23), (19, 24), (36, 24), (32, 24), (20, 25), (32, 25), (17, 25), (21, 26), (30, 14), (173, 11), (137, 11), (174, 11), (170, 12), (174, 12), (173, 12), (30, 12), (171, 12), (169, 12), (170, 13), (29, 13), (30, 13), (171, 13), (174, 13), (29, 14), (171, 14), (170, 14), (174, 14), (33, 14), (171, 15), (29, 15), (33, 15), (28, 15), (167, 15), (20, 15), (30, 15), (28, 16), (62, 180), (62, 184), (176, 181), (63, 181), (8, 181), (41, 182), (63, 182), (177, 182), (38, 182), (63, 183), (62, 183), (48, 183), (178, 183), (52, 183), (63, 184), (179, 184), (52, 184), (48, 184), (61, 184)]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "num_nodes = graph.num_nodes\n",
    "edge_list = graph.edge_list\n",
    "print(edge_list)\n",
    "# 1. 过滤行，只保留每行最后一个数为1的行\n",
    "filtered_rows = edge_list[edge_list[:, 2] == 1]\n",
    "\n",
    "# 2. 去掉最后一列，只保留前两列\n",
    "filtered_rows = filtered_rows[:, :2]\n",
    "\n",
    "# 3. 将结果转换为一对对的形式\n",
    "edge_list = [tuple(pair) for pair in filtered_rows.tolist()]\n",
    "print(edge_list)\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edge_list)\n",
    "plt.figure(figsize=(10, 10))  # 设置画布大小\n",
    "nx.draw(G, with_labels=True, node_size=70, node_color='lightblue', font_size=6, font_color='black', edge_color='black')\n",
    "plt.title('Graph Visualization')\n",
    "plt.savefig('tu.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1295])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "\n",
      "Equal: True\n",
      "\n",
      "Equal: True\n"
     ]
    }
   ],
   "source": [
    "node_in, node_out, relation = graph.edge_list.t()\n",
    "node_out = node_out * graph.num_relation + relation\n",
    "\n",
    "degree_out = scatter_add(graph.edge_weight, node_out, dim_size=graph.num_node * graph.num_relation)\n",
    "print(degree_out.shape)\n",
    "adjacency = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), graph.edge_weight,\n",
    "                                    (graph.num_node, graph.num_node * graph.num_relation))\n",
    "\n",
    "adjacency = adjacency.to_dense()\n",
    "\n",
    "adjacency2 = adjacency.t().view(graph.num_node, graph.num_relation, graph.num_node).permute(1, 0, 2).reshape(graph.num_relation*graph.num_node , graph.num_node).t()\n",
    "\n",
    "adjacency3 = adjacency2.view(graph.num_node, graph.num_relation, graph.num_node).permute(1, 0, 2).reshape(graph.num_relation*graph.num_node , graph.num_node)\n",
    "print(adjacency3)\n",
    "\n",
    "#inverse\n",
    "adjacency4 = adjacency3.view(graph.num_relation, graph.num_node, graph.num_node).permute(1, 0, 2).reshape(graph.num_node, graph.num_relation*graph.num_node)\n",
    "print(\"\\nEqual:\", torch.equal(adjacency2, adjacency4))\n",
    "\n",
    "adjacency5 = adjacency4.t().view(graph.num_relation, graph.num_node, graph.num_node).permute(1, 0, 2).reshape(graph.num_node*graph.num_relation, graph.num_node).t()\n",
    "print(\"\\nEqual:\", torch.equal(adjacency5, adjacency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Equal: True\n"
     ]
    }
   ],
   "source": [
    "degree_out = scatter_add(graph.edge_weight, node_out, dim_size=graph.num_node * graph.num_relation)\n",
    "edge_weight = graph.edge_weight / degree_out[node_out]\n",
    "adjacency_with_degree = utils.sparse_coo_tensor(torch.stack([node_in, node_out]), edge_weight,\n",
    "                                            (graph.num_node, graph.num_node * graph.num_relation)).to_dense()\n",
    "\n",
    "\n",
    "\n",
    "new_edge_list = adjacency2.t().view(graph.num_relation, graph.num_node, graph.num_node).permute(1, 0, 2).reshape(graph.num_node*graph.num_relation, graph.num_node).t()\n",
    "row, col = new_edge_list.nonzero(as_tuple=True)\n",
    "new_edge_weight = torch.ones_like(col, dtype=torch.float32)\n",
    "degree_out2 = scatter_add(new_edge_weight, col, dim_size=graph.num_node * graph.num_relation)\n",
    "edge_weight2 = new_edge_weight / degree_out2[col]\n",
    "adjacency_with_degree2 = utils.sparse_coo_tensor(torch.stack([row, col]), edge_weight2,\n",
    "                                            (graph.num_node, graph.num_node * graph.num_relation)).to_dense()\n",
    "\n",
    "print(\"\\nEqual:\", torch.equal(adjacency_with_degree2, adjacency_with_degree))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gearnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
